{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí∞ Cost Optimization & Scaling: Run ML for Pennies\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module5_deployment/03_cost_optimization_scaling.ipynb)\n",
    "\n",
    "## Hey friend! Let's talk money üí∏\n",
    "\n",
    "Here's what nobody tells you about ML in production:\n",
    "\n",
    "**A badly deployed model can cost $10,000/month.**  \n",
    "**The same model, optimized, costs $100/month.**\n",
    "\n",
    "That's a **100x difference**. Your boss will LOVE you for knowing this!\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "Today we're covering the business side of ML:\n",
    "\n",
    "1. **Cost Analysis** - Calculate actual expenses (AWS, GCP, Azure)\n",
    "2. **Serverless Deployment** - Pay only for what you use\n",
    "3. **Auto-Scaling** - Handle traffic spikes efficiently\n",
    "4. **Spot Instances** - Save 70% with preemptible VMs\n",
    "5. **A/B Testing** - Roll out changes safely\n",
    "6. **Cost Monitoring** - Track every penny\n",
    "\n",
    "### üíº Why This Matters\n",
    "\n",
    "**For your career:**\n",
    "- Companies lose millions on inefficient ML deployments\n",
    "- Knowing cost optimization makes you INVALUABLE\n",
    "- This is senior-level knowledge most ML engineers don't have\n",
    "\n",
    "**For interviews:**\n",
    "- \"How would you reduce ML infrastructure costs?\"\n",
    "- \"Design a scalable ML system for 1M users\"\n",
    "- \"What's your approach to A/B testing models?\"\n",
    "\n",
    "Let's save some money! üí∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding ML Costs üìä\n",
    "\n",
    "### The Reality Check\n",
    "\n",
    "Let's calculate the **actual cost** of running an ML model in production.\n",
    "\n",
    "#### Scenario: Sentiment Analysis API\n",
    "- **Traffic**: 5 million requests/day\n",
    "- **Model**: DistilBERT (66M parameters)\n",
    "- **Latency requirement**: <100ms p95\n",
    "- **Availability**: 24/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "class MLCostCalculator:\n",
    "    \"\"\"\n",
    "    Calculate ML infrastructure costs across different strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pricing (per hour) - As of 2024\n",
    "    PRICING = {\n",
    "        \"aws\": {\n",
    "            \"t3.large\": 0.0832,      # 2 vCPU, 8GB RAM (CPU)\n",
    "            \"c5.2xlarge\": 0.34,       # 8 vCPU, 16GB RAM (CPU optimized)\n",
    "            \"g4dn.xlarge\": 0.526,     # 4 vCPU, 16GB, 1 GPU\n",
    "            \"g5.xlarge\": 1.006,       # 4 vCPU, 16GB, 1 A10G GPU\n",
    "            \"lambda_per_1m\": 0.20,    # Per 1M requests (1GB-sec)\n",
    "        },\n",
    "        \"gcp\": {\n",
    "            \"n1-standard-2\": 0.095,   # 2 vCPU, 7.5GB RAM\n",
    "            \"n1-standard-8\": 0.38,    # 8 vCPU, 30GB RAM\n",
    "            \"n1-gpu-t4\": 0.35,        # T4 GPU per hour\n",
    "            \"cloud_run_per_1m\": 0.40, # Per 1M requests\n",
    "        },\n",
    "        \"azure\": {\n",
    "            \"d2s_v3\": 0.096,          # 2 vCPU, 8GB RAM\n",
    "            \"d8s_v3\": 0.384,          # 8 vCPU, 32GB RAM\n",
    "            \"nc6\": 0.90,              # 1 K80 GPU\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, daily_requests: int):\n",
    "        self.daily_requests = daily_requests\n",
    "        self.monthly_requests = daily_requests * 30\n",
    "    \n",
    "    def calculate_always_on_cost(\n",
    "        self,\n",
    "        instance_type: str,\n",
    "        provider: str = \"aws\",\n",
    "        num_instances: int = 1\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate cost for always-on instances.\n",
    "        \"\"\"\n",
    "        hourly_cost = self.PRICING[provider][instance_type]\n",
    "        monthly_hours = 24 * 30\n",
    "        \n",
    "        monthly_cost = hourly_cost * monthly_hours * num_instances\n",
    "        \n",
    "        return {\n",
    "            \"monthly_cost\": monthly_cost,\n",
    "            \"cost_per_1m_requests\": monthly_cost / (self.monthly_requests / 1_000_000),\n",
    "            \"instance_type\": instance_type,\n",
    "            \"num_instances\": num_instances\n",
    "        }\n",
    "    \n",
    "    def calculate_serverless_cost(\n",
    "        self,\n",
    "        avg_duration_ms: float,\n",
    "        memory_mb: int = 1024,\n",
    "        provider: str = \"aws\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate serverless cost (Lambda, Cloud Run, etc.)\n",
    "        \"\"\"\n",
    "        # Convert to GB-seconds\n",
    "        gb_seconds_per_request = (memory_mb / 1024) * (avg_duration_ms / 1000)\n",
    "        monthly_gb_seconds = gb_seconds_per_request * self.monthly_requests\n",
    "        \n",
    "        # AWS Lambda pricing: $0.0000166667 per GB-second\n",
    "        compute_cost = monthly_gb_seconds * 0.0000166667\n",
    "        \n",
    "        # Request cost: $0.20 per 1M requests\n",
    "        request_cost = (self.monthly_requests / 1_000_000) * 0.20\n",
    "        \n",
    "        total_cost = compute_cost + request_cost\n",
    "        \n",
    "        return {\n",
    "            \"monthly_cost\": total_cost,\n",
    "            \"cost_per_1m_requests\": total_cost / (self.monthly_requests / 1_000_000),\n",
    "            \"compute_cost\": compute_cost,\n",
    "            \"request_cost\": request_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_spot_instance_cost(\n",
    "        self,\n",
    "        instance_type: str,\n",
    "        provider: str = \"aws\",\n",
    "        discount: float = 0.7  # Spot instances are ~70% cheaper\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate cost using spot/preemptible instances.\n",
    "        \"\"\"\n",
    "        hourly_cost = self.PRICING[provider][instance_type] * (1 - discount)\n",
    "        monthly_hours = 24 * 30\n",
    "        monthly_cost = hourly_cost * monthly_hours\n",
    "        \n",
    "        return {\n",
    "            \"monthly_cost\": monthly_cost,\n",
    "            \"cost_per_1m_requests\": monthly_cost / (self.monthly_requests / 1_000_000),\n",
    "            \"discount\": f\"{discount*100:.0f}%\"\n",
    "        }\n",
    "\n",
    "# Create calculator for 5M requests/day\n",
    "calc = MLCostCalculator(daily_requests=5_000_000)\n",
    "\n",
    "print(\"‚úÖ Cost Calculator ready!\")\n",
    "print(f\"\\nAnalyzing costs for:\")\n",
    "print(f\"  - {calc.daily_requests:,} requests/day\")\n",
    "print(f\"  - {calc.monthly_requests:,} requests/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Comparison: Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: GPU Instance (the expensive way)\n",
    "gpu_cost = calc.calculate_always_on_cost(\"g4dn.xlarge\", \"aws\", num_instances=1)\n",
    "\n",
    "# Strategy 2: CPU Instance (optimized model)\n",
    "cpu_cost = calc.calculate_always_on_cost(\"c5.2xlarge\", \"aws\", num_instances=1)\n",
    "\n",
    "# Strategy 3: Serverless (Lambda)\n",
    "serverless_cost = calc.calculate_serverless_cost(\n",
    "    avg_duration_ms=50,  # Optimized model\n",
    "    memory_mb=1024\n",
    ")\n",
    "\n",
    "# Strategy 4: Spot Instance (for batch processing)\n",
    "spot_cost = calc.calculate_spot_instance_cost(\"c5.2xlarge\", \"aws\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\"Strategy\": \"GPU Always-On\", **gpu_cost},\n",
    "    {\"Strategy\": \"CPU Always-On\", **cpu_cost},\n",
    "    {\"Strategy\": \"Serverless (Lambda)\", **serverless_cost},\n",
    "    {\"Strategy\": \"Spot Instance\", **spot_cost},\n",
    "])\n",
    "\n",
    "print(\"üí∞ COST COMPARISON (Monthly)\\n\")\n",
    "print(comparison[['Strategy', 'monthly_cost', 'cost_per_1m_requests']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Insights:\")\n",
    "print(f\"  - GPU vs CPU: {gpu_cost['monthly_cost'] / cpu_cost['monthly_cost']:.1f}x more expensive\")\n",
    "print(f\"  - CPU vs Serverless: {cpu_cost['monthly_cost'] / serverless_cost['monthly_cost']:.1f}x more expensive\")\n",
    "print(f\"  - Regular vs Spot: {cpu_cost['monthly_cost'] / spot_cost['monthly_cost']:.1f}x more expensive\")\n",
    "print(f\"\\nüéØ Best for this traffic: Serverless (saves ${cpu_cost['monthly_cost'] - serverless_cost['monthly_cost']:.2f}/month)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Serverless Deployment üöÄ\n",
    "\n",
    "### Why Serverless?\n",
    "\n",
    "**Traditional Deployment:**\n",
    "- Server runs 24/7 (even at 3 AM with zero traffic)\n",
    "- You pay for idle time\n",
    "- Must handle scaling manually\n",
    "\n",
    "**Serverless:**\n",
    "- Pay only when handling requests\n",
    "- Auto-scales from 0 to millions\n",
    "- Zero server management\n",
    "\n",
    "### When to Use Serverless:\n",
    "- ‚úÖ Variable traffic patterns\n",
    "- ‚úÖ Small to medium traffic (<10M requests/day)\n",
    "- ‚úÖ Can tolerate cold starts (100-500ms)\n",
    "- ‚ùå Constant high traffic (always-on is cheaper)\n",
    "- ‚ùå Require <10ms latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AWS Lambda handler for ML inference\n",
    "serverless_code = '''\n",
    "# lambda_handler.py\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Global variable - loaded once per container (warm start)\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model on cold start.\"\"\"\n",
    "    global model\n",
    "    if model is None:\n",
    "        print(\"Cold start - loading model...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Load from S3 or use containerized model\n",
    "        model = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"./model\",  # Pre-downloaded in Docker image\n",
    "            device=-1  # CPU inference\n",
    "        )\n",
    "        \n",
    "        print(f\"Model loaded in {time.time() - start:.2f}s\")\n",
    "    return model\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda handler for ML inference.\n",
    "    \n",
    "    Cost optimization strategies:\n",
    "    1. Use global variable for warm starts\n",
    "    2. Optimize memory (1GB vs 10GB = 10x cost)\n",
    "    3. Batch requests when possible\n",
    "    4. Use ARM architecture (Graviton2 = 20% cheaper)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model (fast on warm start)\n",
    "        model = load_model()\n",
    "        \n",
    "        # Parse input\n",
    "        body = json.loads(event['body'])\n",
    "        text = body.get('text', '')\n",
    "        \n",
    "        if not text:\n",
    "            return {\n",
    "                'statusCode': 400,\n",
    "                'body': json.dumps({'error': 'No text provided'})\n",
    "            }\n",
    "        \n",
    "        # Inference\n",
    "        start = time.time()\n",
    "        result = model(text)[0]\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {\n",
    "                'Content-Type': 'application/json',\n",
    "                'X-Inference-Time': str(latency)\n",
    "            },\n",
    "            'body': json.dumps({\n",
    "                'prediction': result['label'],\n",
    "                'confidence': result['score'],\n",
    "                'latency_ms': latency * 1000\n",
    "            })\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "\n",
    "# Dockerfile for Lambda\n",
    "\"\"\"\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "\n",
    "# Copy model (pre-downloaded)\n",
    "COPY model/ ${LAMBDA_TASK_ROOT}/model/\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt --target ${LAMBDA_TASK_ROOT}\n",
    "\n",
    "# Copy handler\n",
    "COPY lambda_handler.py ${LAMBDA_TASK_ROOT}\n",
    "\n",
    "CMD [\"lambda_handler.lambda_handler\"]\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "print(\"üìÑ Serverless Lambda Handler:\")\n",
    "print(serverless_code)\n",
    "\n",
    "print(\"\\nüí° Deployment:\")\n",
    "print(\"\"\"\n",
    "1. Build Docker image:\n",
    "   docker build -t ml-inference .\n",
    "\n",
    "2. Push to ECR:\n",
    "   aws ecr create-repository --repository-name ml-inference\n",
    "   docker tag ml-inference:latest <account>.dkr.ecr.us-east-1.amazonaws.com/ml-inference\n",
    "   docker push <account>.dkr.ecr.us-east-1.amazonaws.com/ml-inference\n",
    "\n",
    "3. Create Lambda:\n",
    "   aws lambda create-function \\\n",
    "     --function-name ml-inference \\\n",
    "     --package-type Image \\\n",
    "     --code ImageUri=<account>.dkr.ecr.us-east-1.amazonaws.com/ml-inference \\\n",
    "     --role <lambda-role-arn> \\\n",
    "     --timeout 30 \\\n",
    "     --memory-size 1024\n",
    "\n",
    "4. Add API Gateway and you're done!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Auto-Scaling Strategy üìà\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "Traffic isn't constant:\n",
    "- **9 AM - 5 PM**: 1000 requests/sec\n",
    "- **Night time**: 50 requests/sec\n",
    "- **Black Friday**: 5000 requests/sec\n",
    "\n",
    "You need a system that scales automatically!\n",
    "\n",
    "### Auto-Scaling Strategies:\n",
    "\n",
    "1. **Horizontal Scaling**: Add/remove servers based on load\n",
    "2. **Vertical Scaling**: Increase/decrease server size\n",
    "3. **Predictive Scaling**: Scale before traffic arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubernetes Horizontal Pod Autoscaler (HPA) configuration\n",
    "k8s_autoscaling = '''\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-inference\n",
    "spec:\n",
    "  replicas: 2  # Start with 2 pods\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-inference\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-inference\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: inference\n",
    "        image: your-ml-model:latest\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"500m\"\n",
    "            memory: \"1Gi\"\n",
    "          limits:\n",
    "            cpu: \"1000m\"\n",
    "            memory: \"2Gi\"\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "\n",
    "---\n",
    "# hpa.yaml - Auto-scaling configuration\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: ml-inference-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: ml-inference\n",
    "  minReplicas: 2        # Always run at least 2 (high availability)\n",
    "  maxReplicas: 20       # Scale up to 20 during traffic spikes\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70  # Scale when CPU > 70%\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80  # Scale when memory > 80%\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60   # Wait 60s before scaling up\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100                      # Double the pods\n",
    "        periodSeconds: 60\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5min before scaling down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50                       # Remove half the pods\n",
    "        periodSeconds: 60\n",
    "'''\n",
    "\n",
    "print(\"üìÑ Kubernetes Auto-Scaling Configuration:\")\n",
    "print(k8s_autoscaling)\n",
    "\n",
    "print(\"\\nüí° How it works:\")\n",
    "print(\"\"\"\n",
    "1. Start with 2 pods (for high availability)\n",
    "2. When CPU > 70%, add more pods\n",
    "3. Scale up aggressively (double capacity in 60s)\n",
    "4. Scale down conservatively (wait 5min, then halve)\n",
    "5. Never go below 2 or above 20 pods\n",
    "\n",
    "Cost impact:\n",
    "- Low traffic (2 pods): $100/month\n",
    "- High traffic (20 pods): $1000/month\n",
    "- Auto-scaling saves ~60% vs always running 20 pods\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: A/B Testing & Gradual Rollouts üß™\n",
    "\n",
    "### Why A/B Test Models?\n",
    "\n",
    "Your new model might:\n",
    "- Have bugs\n",
    "- Be slower than expected\n",
    "- Perform worse on real data\n",
    "\n",
    "**Never replace a model 100% instantly!**\n",
    "\n",
    "### Rollout Strategy:\n",
    "\n",
    "1. **Canary (5%)**: Send 5% traffic to new model\n",
    "2. **Monitor**: Watch metrics for 24 hours\n",
    "3. **Expand (25%)**: If good, increase to 25%\n",
    "4. **Expand (50%)**: Still good? Go to 50%\n",
    "5. **Full (100%)**: Finally, switch everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B testing infrastructure\n",
    "ab_testing_code = '''\n",
    "# ab_test_router.py\n",
    "\n",
    "import random\n",
    "import hashlib\n",
    "from typing import Dict, Optional\n",
    "import logging\n",
    "\n",
    "class ABTestRouter:\n",
    "    \"\"\"\n",
    "    Route traffic between model versions for A/B testing.\n",
    "    \n",
    "    Features:\n",
    "    - Consistent routing per user (same user = same model)\n",
    "    - Configurable traffic split\n",
    "    - Metric tracking\n",
    "    - Easy rollback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_a, model_b, b_percentage: int = 5):\n",
    "        self.model_a = model_a  # Current production model\n",
    "        self.model_b = model_b  # New model being tested\n",
    "        self.b_percentage = b_percentage\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = {\n",
    "            \"a\": {\"requests\": 0, \"errors\": 0, \"total_latency\": 0.0},\n",
    "            \"b\": {\"requests\": 0, \"errors\": 0, \"total_latency\": 0.0}\n",
    "        }\n",
    "    \n",
    "    def should_use_model_b(self, user_id: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Decide which model to use.\n",
    "        \n",
    "        If user_id provided: Consistent routing (same user always gets same model)\n",
    "        If no user_id: Random routing\n",
    "        \"\"\"\n",
    "        if user_id:\n",
    "            # Hash user_id to get consistent routing\n",
    "            hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n",
    "            return (hash_value % 100) < self.b_percentage\n",
    "        else:\n",
    "            # Random routing\n",
    "            return random.randint(0, 99) < self.b_percentage\n",
    "    \n",
    "    def predict(self, text: str, user_id: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Make prediction, routing to appropriate model.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Decide which model to use\n",
    "        use_b = self.should_use_model_b(user_id)\n",
    "        model = self.model_b if use_b else self.model_a\n",
    "        variant = \"b\" if use_b else \"a\"\n",
    "        \n",
    "        # Track request\n",
    "        self.metrics[variant][\"requests\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Make prediction\n",
    "            start = time.time()\n",
    "            result = model(text)\n",
    "            latency = time.time() - start\n",
    "            \n",
    "            # Track latency\n",
    "            self.metrics[variant][\"total_latency\"] += latency\n",
    "            \n",
    "            return {\n",
    "                \"prediction\": result,\n",
    "                \"model_version\": variant,\n",
    "                \"latency_ms\": latency * 1000\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Track error\n",
    "            self.metrics[variant][\"errors\"] += 1\n",
    "            logging.error(f\"Model {variant} error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get A/B test metrics for comparison.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for variant in [\"a\", \"b\"]:\n",
    "            m = self.metrics[variant]\n",
    "            requests = m[\"requests\"]\n",
    "            \n",
    "            if requests > 0:\n",
    "                results[f\"model_{variant}\"] = {\n",
    "                    \"requests\": requests,\n",
    "                    \"error_rate\": m[\"errors\"] / requests,\n",
    "                    \"avg_latency_ms\": (m[\"total_latency\"] / requests) * 1000\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def should_rollback(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if we should rollback model B.\n",
    "        \n",
    "        Rollback if:\n",
    "        - Error rate > 2x model A\n",
    "        - Latency > 1.5x model A\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "        \n",
    "        if \"model_b\" not in metrics or metrics[\"model_b\"][\"requests\"] < 100:\n",
    "            return False  # Not enough data\n",
    "        \n",
    "        a = metrics[\"model_a\"]\n",
    "        b = metrics[\"model_b\"]\n",
    "        \n",
    "        # Check error rate\n",
    "        if b[\"error_rate\"] > a[\"error_rate\"] * 2:\n",
    "            logging.warning(f\"Model B error rate too high: {b['error_rate']:.2%}\")\n",
    "            return True\n",
    "        \n",
    "        # Check latency\n",
    "        if b[\"avg_latency_ms\"] > a[\"avg_latency_ms\"] * 1.5:\n",
    "            logging.warning(f\"Model B too slow: {b['avg_latency_ms']:.0f}ms\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Usage:\n",
    "router = ABTestRouter(model_a=old_model, model_b=new_model, b_percentage=5)\n",
    "\n",
    "# Route traffic\n",
    "result = router.predict(\"Great product!\", user_id=\"user123\")\n",
    "\n",
    "# Check metrics after 24 hours\n",
    "if router.should_rollback():\n",
    "    print(\"‚ö†Ô∏è Rolling back model B!\")\n",
    "else:\n",
    "    print(\"‚úÖ Model B looks good, increase traffic!\")\n",
    "'''\n",
    "\n",
    "print(\"üìÑ A/B Testing Router:\")\n",
    "print(ab_testing_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Cost Monitoring & Alerts üìä\n",
    "\n",
    "### The Golden Rule:\n",
    "\n",
    "**\"You can't optimize what you don't measure.\"**\n",
    "\n",
    "Always track:\n",
    "1. **Infrastructure cost** (servers, storage, network)\n",
    "2. **Cost per request** (your efficiency metric)\n",
    "3. **Cost per prediction** (business metric)\n",
    "4. **Cost trends** (are you getting more efficient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost monitoring system\n",
    "cost_monitoring = '''\n",
    "# cost_monitor.py\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "\n",
    "class AWSCostMonitor:\n",
    "    \"\"\"\n",
    "    Monitor AWS costs and send alerts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, budget_monthly: float = 500):\n",
    "        self.ce_client = boto3.client('ce')  # Cost Explorer\n",
    "        self.sns_client = boto3.client('sns')\n",
    "        self.budget_monthly = budget_monthly\n",
    "    \n",
    "    def get_current_month_cost(self) -> float:\n",
    "        \"\"\"\n",
    "        Get total cost for current month.\n",
    "        \"\"\"\n",
    "        # Get first day of month\n",
    "        today = datetime.now()\n",
    "        start = today.replace(day=1).strftime('%Y-%m-%d')\n",
    "        end = today.strftime('%Y-%m-%d')\n",
    "        \n",
    "        response = self.ce_client.get_cost_and_usage(\n",
    "            TimePeriod={'Start': start, 'End': end},\n",
    "            Granularity='MONTHLY',\n",
    "            Metrics=['UnblendedCost']\n",
    "        )\n",
    "        \n",
    "        cost = float(\n",
    "            response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount']\n",
    "        )\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def get_service_breakdown(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get cost breakdown by service.\n",
    "        \"\"\"\n",
    "        today = datetime.now()\n",
    "        start = (today - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "        end = today.strftime('%Y-%m-%d')\n",
    "        \n",
    "        response = self.ce_client.get_cost_and_usage(\n",
    "            TimePeriod={'Start': start, 'End': end},\n",
    "            Granularity='DAILY',\n",
    "            Metrics=['UnblendedCost'],\n",
    "            GroupBy=[{'Type': 'SERVICE', 'Key': 'SERVICE'}]\n",
    "        )\n",
    "        \n",
    "        # Aggregate by service\n",
    "        costs = {}\n",
    "        for result in response['ResultsByTime']:\n",
    "            for group in result['Groups']:\n",
    "                service = group['Keys'][0]\n",
    "                cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "                costs[service] = costs.get(service, 0) + cost\n",
    "        \n",
    "        return costs\n",
    "    \n",
    "    def check_budget_alert(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if we're over budget.\n",
    "        \"\"\"\n",
    "        current_cost = self.get_current_month_cost()\n",
    "        \n",
    "        # Get days in month\n",
    "        today = datetime.now()\n",
    "        days_in_month = (today.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1)\n",
    "        days_in_month = days_in_month.day\n",
    "        \n",
    "        # Projected cost\n",
    "        projected = (current_cost / today.day) * days_in_month\n",
    "        \n",
    "        if projected > self.budget_monthly:\n",
    "            self.send_alert(\n",
    "                f\"‚ö†Ô∏è COST ALERT: Projected ${projected:.2f} (budget: ${self.budget_monthly})\"\n",
    "            )\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def send_alert(self, message: str):\n",
    "        \"\"\"\n",
    "        Send alert via SNS.\n",
    "        \"\"\"\n",
    "        self.sns_client.publish(\n",
    "            TopicArn='arn:aws:sns:us-east-1:123456789:cost-alerts',\n",
    "            Subject='AWS Cost Alert',\n",
    "            Message=message\n",
    "        )\n",
    "\n",
    "# Daily cost monitoring Lambda\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Run daily to check costs.\"\"\"\n",
    "    monitor = AWSCostMonitor(budget_monthly=500)\n",
    "    \n",
    "    # Check budget\n",
    "    monitor.check_budget_alert()\n",
    "    \n",
    "    # Log breakdown\n",
    "    breakdown = monitor.get_service_breakdown()\n",
    "    print(\"Cost breakdown (last 7 days):\")\n",
    "    for service, cost in sorted(breakdown.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {service}: ${cost:.2f}\")\n",
    "\n",
    "# Schedule with CloudWatch Events (every day at 9 AM)\n",
    "# Rate: cron(0 9 * * ? *)\n",
    "'''\n",
    "\n",
    "print(\"üìÑ Cost Monitoring System:\")\n",
    "print(cost_monitoring)\n",
    "\n",
    "print(\"\\nüí° Best practices:\")\n",
    "print(\"\"\"\n",
    "1. Set up budget alerts (monthly and daily)\n",
    "2. Track cost per request (optimize this metric)\n",
    "3. Monitor cost trends (should decrease over time)\n",
    "4. Review top 5 expensive services weekly\n",
    "5. Use cost anomaly detection (AWS Cost Anomaly Detection)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Resume Bullets (Copy These!)\n",
    "\n",
    "### Option 1: Cost Optimization Focus\n",
    "*\"Reduced ML infrastructure costs by 92% (from $10K to $800/month) through serverless deployment, quantization, and auto-scaling\"*\n",
    "\n",
    "### Option 2: Scaling Focus\n",
    "*\"Designed auto-scaling ML system handling 1M-50M requests/day with 99.9% uptime and <100ms p95 latency\"*\n",
    "\n",
    "### Option 3: A/B Testing Focus\n",
    "*\"Implemented gradual rollout system with automatic rollback, safely deploying 15+ model updates with zero downtime\"*\n",
    "\n",
    "### Option 4: Multi-Cloud Focus\n",
    "*\"Architected cost-optimized ML deployment strategy across AWS, GCP, and Azure, selecting optimal platform per use case\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Interview Prep\n",
    "\n",
    "### Q: \"How would you reduce ML infrastructure costs?\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"I'd start with a cost audit - understand where money is going. Usually, I find:*\n",
    "\n",
    "*1. **Over-provisioned instances**: Running GPU when CPU would work (30x cost difference!)\n",
    "*2. **Always-on servers**: Paying for idle time at 3 AM\n",
    "*3. **Unoptimized models**: Using full precision when quantized would work\n",
    "\n",
    "*My approach:*\n",
    "*- **Model optimization**: Quantization + ONNX reduces instance requirements\n",
    "*- **Right-size instances**: Match instance to actual load (not peak!)\n",
    "*- **Serverless for variable traffic**: Pay only for requests, not idle time\n",
    "*- **Spot instances for batch jobs**: 70% cost savings\n",
    "*- **Auto-scaling**: Scale down during low traffic\n",
    "\n",
    "*On my last project, this reduced costs from $10K to $800/month - 92% savings while improving latency.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"Design a scalable ML system for 10M requests/day.\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"I'd design for:\n",
    "\n",
    "**Architecture:**\n",
    "- **Load balancer** (ALB/NLB) distributing traffic\n",
    "- **Auto-scaling group** (2-20 instances) with optimized model\n",
    "- **Redis cache** for frequent queries (cache hit = 1ms vs 50ms)\n",
    "- **Async processing** for batch requests via SQS\n",
    "\n",
    "**Scaling strategy:**\n",
    "- Start with 2 instances (high availability)\n",
    "- Scale up when CPU > 70% or latency > 100ms\n",
    "- Scale down after 5 min of low load\n",
    "- Use predictive scaling for known traffic patterns\n",
    "\n",
    "**Cost optimization:**\n",
    "- Quantized model on CPU instances (not GPU)\n",
    "- ONNX Runtime for 2-3x speedup\n",
    "- Smart batching (batch size 8-16)\n",
    "- Spot instances for batch processing\n",
    "\n",
    "**Monitoring:**\n",
    "- CloudWatch metrics: latency, error rate, cost\n",
    "- Auto-rollback if error rate > 1%\n",
    "- Cost alerts if projected > budget\n",
    "\n",
    "This handles 10M requests/day for ~$300/month with 99.9% uptime.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"How do you safely roll out a new model?\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"Never replace 100% instantly! I use gradual rollout:*\n",
    "\n",
    "**Phase 1: Canary (5%, 24 hours)**\n",
    "- Route 5% of traffic to new model\n",
    "- Monitor: error rate, latency, user feedback\n",
    "- Automatic rollback if error rate > 2x baseline\n",
    "\n",
    "**Phase 2: Expand (25%, 24 hours)**\n",
    "- If metrics look good, increase to 25%\n",
    "- Compare A/B metrics side-by-side\n",
    "- Look for edge cases or specific user segments affected\n",
    "\n",
    "**Phase 3: Majority (50%, 48 hours)**\n",
    "- Increase to 50%\n",
    "- Final check of all metrics\n",
    "- Ensure cost is as expected\n",
    "\n",
    "**Phase 4: Full (100%)**\n",
    "- Complete migration\n",
    "- Keep old model for 7 days (quick rollback)\n",
    "- Monitor for regression\n",
    "\n",
    "I implement this with a router that uses consistent hashing - same user always gets same model version for fair testing.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ You're Now a Cost Optimization Expert!\n",
    "\n",
    "You've learned:\n",
    "\n",
    "‚úÖ **Cost calculation** - Know the real expenses  \n",
    "‚úÖ **Serverless deployment** - Pay only for requests  \n",
    "‚úÖ **Auto-scaling** - Handle any traffic automatically  \n",
    "‚úÖ **A/B testing** - Roll out changes safely  \n",
    "‚úÖ **Cost monitoring** - Track and optimize continuously  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Always optimize models first** - 10x cost savings just from quantization!\n",
    "2. **Match architecture to traffic** - Serverless for variable, always-on for constant\n",
    "3. **Never skip A/B testing** - Bugs in production = lost money\n",
    "4. **Monitor everything** - You can't optimize what you don't measure\n",
    "5. **Cost per request matters** - Not total cost, but efficiency\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "With these techniques, you can:\n",
    "- Save $10K+/year on infrastructure\n",
    "- Handle 100x traffic without code changes\n",
    "- Deploy updates with confidence\n",
    "- Scale from startup to enterprise\n",
    "\n",
    "**This knowledge makes you invaluable to any company running ML in production!**\n",
    "\n",
    "---\n",
    "\n",
    "*Built with ‚ù§Ô∏è for people who understand business value*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
