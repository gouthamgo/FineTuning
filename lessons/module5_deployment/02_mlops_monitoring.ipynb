{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module5_deployment/02_mlops_monitoring.ipynb)\n",
    "\n",
    "# üìä MLOps & Monitoring: Keep Your Models Healthy\n",
    "\n",
    "**Duration:** 1.5 hours  \n",
    "**Level:** Advanced  \n",
    "**What You'll Learn:** How to monitor, maintain, and improve production ML systems\n",
    "\n",
    "---\n",
    "\n",
    "## This Is What Separates Good from Great ML Engineers!\n",
    "\n",
    "**Real talk:** Deploying a model is only 20% of the job.\n",
    "\n",
    "The other 80% is:\n",
    "- üìä Monitoring performance\n",
    "- üîß Fixing issues before users notice\n",
    "- üìà Continuously improving\n",
    "- üö® Alerting when things go wrong\n",
    "- üîÑ Re-training when needed\n",
    "\n",
    "**Companies LOVE candidates who understand production ML!**\n",
    "\n",
    "Today you'll learn the MLOps practices that make you stand out. Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The 5 Pillars of Production ML\n",
    "\n",
    "### 1. **Monitoring** üìä\n",
    "Track what's happening in production\n",
    "\n",
    "### 2. **Logging** üìù\n",
    "Record everything for debugging\n",
    "\n",
    "### 3. **Alerting** üö®\n",
    "Get notified when things go wrong\n",
    "\n",
    "### 4. **Versioning** üîñ\n",
    "Track models, data, and code\n",
    "\n",
    "### 5. **Continuous Training** üîÑ\n",
    "Keep models fresh with new data\n",
    "\n",
    "Let's implement each one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q prometheus-client psutil pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Pillar 1: Comprehensive Monitoring\n",
    "\n",
    "You need to track 4 types of metrics:\n",
    "\n",
    "1. **Model Performance** - Is accuracy dropping?\n",
    "2. **System Performance** - Is it slow? Crashing?\n",
    "3. **Data Quality** - Is input data changing?\n",
    "4. **Business Metrics** - Is it helping the business?\n",
    "\n",
    "Let's build a complete monitoring system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import Counter, Histogram, Gauge, Summary\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class MLMonitor:\n",
    "    \"\"\"Comprehensive ML model monitoring system\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"ml_model\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # ===== MODEL PERFORMANCE METRICS =====\n",
    "        \n",
    "        # Total predictions made\n",
    "        self.prediction_counter = Counter(\n",
    "            f'{model_name}_predictions_total',\n",
    "            'Total number of predictions',\n",
    "            ['model_version', 'prediction_class']\n",
    "        )\n",
    "        \n",
    "        # Prediction confidence distribution\n",
    "        self.confidence_histogram = Histogram(\n",
    "            f'{model_name}_prediction_confidence',\n",
    "            'Distribution of prediction confidence scores',\n",
    "            buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]\n",
    "        )\n",
    "        \n",
    "        # Current accuracy (rolling window)\n",
    "        self.accuracy_gauge = Gauge(\n",
    "            f'{model_name}_accuracy',\n",
    "            'Current model accuracy'\n",
    "        )\n",
    "        \n",
    "        # ===== SYSTEM PERFORMANCE METRICS =====\n",
    "        \n",
    "        # Latency tracking\n",
    "        self.latency_histogram = Histogram(\n",
    "            f'{model_name}_latency_seconds',\n",
    "            'Model inference latency',\n",
    "            buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "        )\n",
    "        \n",
    "        # Error tracking\n",
    "        self.error_counter = Counter(\n",
    "            f'{model_name}_errors_total',\n",
    "            'Total number of errors',\n",
    "            ['error_type']\n",
    "        )\n",
    "        \n",
    "        # Memory usage\n",
    "        self.memory_gauge = Gauge(\n",
    "            f'{model_name}_memory_mb',\n",
    "            'Memory usage in MB'\n",
    "        )\n",
    "        \n",
    "        # ===== DATA QUALITY METRICS =====\n",
    "        \n",
    "        # Input distribution (detect drift)\n",
    "        self.input_length_summary = Summary(\n",
    "            f'{model_name}_input_length',\n",
    "            'Input text length distribution'\n",
    "        )\n",
    "        \n",
    "        # Out-of-vocabulary rate\n",
    "        self.oov_gauge = Gauge(\n",
    "            f'{model_name}_oov_rate',\n",
    "            'Out-of-vocabulary word rate'\n",
    "        )\n",
    "        \n",
    "        # ===== BUSINESS METRICS =====\n",
    "        \n",
    "        # Escalation rate (for chatbots)\n",
    "        self.escalation_gauge = Gauge(\n",
    "            f'{model_name}_escalation_rate',\n",
    "            'Percentage of predictions escalated to humans'\n",
    "        )\n",
    "        \n",
    "        # Internal state for calculations\n",
    "        self.recent_predictions = deque(maxlen=1000)  # Last 1000 predictions\n",
    "        self.recent_confidences = deque(maxlen=1000)\n",
    "        self.recent_escalations = deque(maxlen=1000)\n",
    "    \n",
    "    def track_prediction(self, prediction, confidence, true_label=None, \n",
    "                        model_version=\"v1.0\", input_length=None, escalated=False):\n",
    "        \"\"\"Track a single prediction with all relevant metrics\"\"\"\n",
    "        \n",
    "        # Model performance\n",
    "        self.prediction_counter.labels(\n",
    "            model_version=model_version,\n",
    "            prediction_class=str(prediction)\n",
    "        ).inc()\n",
    "        \n",
    "        self.confidence_histogram.observe(confidence)\n",
    "        self.recent_confidences.append(confidence)\n",
    "        \n",
    "        # Track accuracy if we have true label\n",
    "        if true_label is not None:\n",
    "            is_correct = (prediction == true_label)\n",
    "            self.recent_predictions.append(is_correct)\n",
    "            \n",
    "            # Update rolling accuracy\n",
    "            if len(self.recent_predictions) > 0:\n",
    "                accuracy = sum(self.recent_predictions) / len(self.recent_predictions)\n",
    "                self.accuracy_gauge.set(accuracy)\n",
    "        \n",
    "        # Data quality\n",
    "        if input_length:\n",
    "            self.input_length_summary.observe(input_length)\n",
    "        \n",
    "        # Business metrics\n",
    "        self.recent_escalations.append(1 if escalated else 0)\n",
    "        if len(self.recent_escalations) > 0:\n",
    "            escalation_rate = sum(self.recent_escalations) / len(self.recent_escalations)\n",
    "            self.escalation_gauge.set(escalation_rate)\n",
    "    \n",
    "    def track_latency(self, latency_seconds):\n",
    "        \"\"\"Track inference latency\"\"\"\n",
    "        self.latency_histogram.observe(latency_seconds)\n",
    "    \n",
    "    def track_error(self, error_type):\n",
    "        \"\"\"Track an error\"\"\"\n",
    "        self.error_counter.labels(error_type=error_type).inc()\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get current monitoring summary\"\"\"\n",
    "        summary = {\n",
    "            'total_predictions': len(self.recent_predictions),\n",
    "            'current_accuracy': sum(self.recent_predictions) / len(self.recent_predictions) if self.recent_predictions else 0,\n",
    "            'avg_confidence': sum(self.recent_confidences) / len(self.recent_confidences) if self.recent_confidences else 0,\n",
    "            'escalation_rate': sum(self.recent_escalations) / len(self.recent_escalations) if self.recent_escalations else 0,\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"üìä ML Monitoring System\\n\" + \"=\"*60)\n",
    "\n",
    "monitor = MLMonitor(model_name=\"support_bot\")\n",
    "\n",
    "# Simulate predictions\n",
    "print(\"\\nSimulating 100 predictions...\\n\")\n",
    "for i in range(100):\n",
    "    # Simulate prediction\n",
    "    prediction = np.random.choice([0, 1])\n",
    "    true_label = prediction if np.random.random() > 0.15 else 1 - prediction  # 85% accuracy\n",
    "    confidence = np.random.uniform(0.6, 0.99)\n",
    "    escalated = confidence < 0.75\n",
    "    \n",
    "    # Track it\n",
    "    start = time.time()\n",
    "    monitor.track_prediction(\n",
    "        prediction=prediction,\n",
    "        confidence=confidence,\n",
    "        true_label=true_label,\n",
    "        input_length=np.random.randint(10, 200),\n",
    "        escalated=escalated\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    monitor.track_latency(latency)\n",
    "\n",
    "# Get summary\n",
    "summary = monitor.get_summary()\n",
    "print(\"üìà MONITORING SUMMARY:\")\n",
    "print(f\"  Total Predictions: {summary['total_predictions']}\")\n",
    "print(f\"  Current Accuracy: {summary['current_accuracy']:.1%}\")\n",
    "print(f\"  Average Confidence: {summary['avg_confidence']:.1%}\")\n",
    "print(f\"  Escalation Rate: {summary['escalation_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n‚úÖ This data would be visible in Prometheus/Grafana dashboards!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Pillar 2: Structured Logging\n",
    "\n",
    "Logs are your time machine - they let you see exactly what happened.\n",
    "\n",
    "**Good logging practices:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class MLLogger:\n",
    "    \"\"\"Structured logging for ML systems\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, log_level=logging.INFO):\n",
    "        self.model_name = model_name\n",
    "        self.logger = logging.getLogger(model_name)\n",
    "        self.logger.setLevel(log_level)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(log_level)\n",
    "        \n",
    "        # JSON formatter for structured logging\n",
    "        class JSONFormatter(logging.Formatter):\n",
    "            def format(self, record):\n",
    "                log_data = {\n",
    "                    'timestamp': datetime.utcnow().isoformat(),\n",
    "                    'level': record.levelname,\n",
    "                    'model': model_name,\n",
    "                    'message': record.getMessage(),\n",
    "                }\n",
    "                \n",
    "                # Add extra fields if present\n",
    "                if hasattr(record, 'extra_data'):\n",
    "                    log_data.update(record.extra_data)\n",
    "                \n",
    "                return json.dumps(log_data)\n",
    "        \n",
    "        console_handler.setFormatter(JSONFormatter())\n",
    "        self.logger.addHandler(console_handler)\n",
    "    \n",
    "    def log_prediction(self, request_id, input_text, prediction, confidence, latency_ms):\n",
    "        \"\"\"Log a prediction with all context\"\"\"\n",
    "        extra_data = {\n",
    "            'request_id': request_id,\n",
    "            'input_length': len(input_text),\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'latency_ms': latency_ms,\n",
    "        }\n",
    "        \n",
    "        # Create log record with extra data\n",
    "        record = self.logger.makeRecord(\n",
    "            self.logger.name,\n",
    "            logging.INFO,\n",
    "            \"\", 0, \"Prediction made\", (), None\n",
    "        )\n",
    "        record.extra_data = extra_data\n",
    "        self.logger.handle(record)\n",
    "    \n",
    "    def log_error(self, request_id, error_type, error_message, stack_trace=None):\n",
    "        \"\"\"Log an error with context\"\"\"\n",
    "        extra_data = {\n",
    "            'request_id': request_id,\n",
    "            'error_type': error_type,\n",
    "            'error_message': str(error_message),\n",
    "        }\n",
    "        \n",
    "        if stack_trace:\n",
    "            extra_data['stack_trace'] = stack_trace\n",
    "        \n",
    "        record = self.logger.makeRecord(\n",
    "            self.logger.name,\n",
    "            logging.ERROR,\n",
    "            \"\", 0, \"Error occurred\", (), None\n",
    "        )\n",
    "        record.extra_data = extra_data\n",
    "        self.logger.handle(record)\n",
    "\n",
    "# Example usage\n",
    "print(\"üìù Structured Logging Example\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "logger = MLLogger(\"support_bot\")\n",
    "\n",
    "# Log a prediction\n",
    "logger.log_prediction(\n",
    "    request_id=\"req_123\",\n",
    "    input_text=\"How do I reset my password?\",\n",
    "    prediction=\"account_help\",\n",
    "    confidence=0.92,\n",
    "    latency_ms=145\n",
    ")\n",
    "\n",
    "# Log an error\n",
    "logger.log_error(\n",
    "    request_id=\"req_124\",\n",
    "    error_type=\"ModelError\",\n",
    "    error_message=\"Model inference failed\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Logs are structured (JSON) for easy parsing and analysis!\")\n",
    "print(\"   These can be sent to CloudWatch, DataDog, or Elasticsearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Pillar 3: Smart Alerting\n",
    "\n",
    "Don't wait for users to complain - detect problems automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class AlertingSystem:\n",
    "    \"\"\"Intelligent alerting for ML systems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alerts = []\n",
    "        self.metrics_history = {\n",
    "            'accuracy': deque(maxlen=1000),\n",
    "            'latency': deque(maxlen=1000),\n",
    "            'confidence': deque(maxlen=1000),\n",
    "            'error_rate': deque(maxlen=1000),\n",
    "        }\n",
    "    \n",
    "    def check_alerts(self, current_metrics, thresholds):\n",
    "        \"\"\"Check if any metrics exceed thresholds\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Accuracy dropped\n",
    "        if current_metrics['accuracy'] < thresholds['min_accuracy']:\n",
    "            alerts.append({\n",
    "                'severity': 'CRITICAL',\n",
    "                'metric': 'accuracy',\n",
    "                'current': current_metrics['accuracy'],\n",
    "                'threshold': thresholds['min_accuracy'],\n",
    "                'message': f\"Accuracy dropped to {current_metrics['accuracy']:.1%} (threshold: {thresholds['min_accuracy']:.1%})\",\n",
    "                'action': 'Investigate data drift or model degradation'\n",
    "            })\n",
    "        \n",
    "        # Latency too high\n",
    "        if current_metrics['latency_p95'] > thresholds['max_latency_p95']:\n",
    "            alerts.append({\n",
    "                'severity': 'WARNING',\n",
    "                'metric': 'latency',\n",
    "                'current': current_metrics['latency_p95'],\n",
    "                'threshold': thresholds['max_latency_p95'],\n",
    "                'message': f\"P95 latency is {current_metrics['latency_p95']}ms (threshold: {thresholds['max_latency_p95']}ms)\",\n",
    "                'action': 'Scale up resources or optimize model'\n",
    "            })\n",
    "        \n",
    "        # Confidence dropping (potential data drift)\n",
    "        if current_metrics['avg_confidence'] < thresholds['min_confidence']:\n",
    "            alerts.append({\n",
    "                'severity': 'WARNING',\n",
    "                'metric': 'confidence',\n",
    "                'current': current_metrics['avg_confidence'],\n",
    "                'threshold': thresholds['min_confidence'],\n",
    "                'message': f\"Average confidence dropped to {current_metrics['avg_confidence']:.1%} (threshold: {thresholds['min_confidence']:.1%})\",\n",
    "                'action': 'Check for distribution shift or new input patterns'\n",
    "            })\n",
    "        \n",
    "        # Error rate too high\n",
    "        if current_metrics['error_rate'] > thresholds['max_error_rate']:\n",
    "            alerts.append({\n",
    "                'severity': 'CRITICAL',\n",
    "                'metric': 'error_rate',\n",
    "                'current': current_metrics['error_rate'],\n",
    "                'threshold': thresholds['max_error_rate'],\n",
    "                'message': f\"Error rate at {current_metrics['error_rate']:.1%} (threshold: {thresholds['max_error_rate']:.1%})\",\n",
    "                'action': 'Check logs immediately - service may be failing'\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def send_alerts(self, alerts):\n",
    "        \"\"\"Send alerts (in production, this would email/Slack/PagerDuty)\"\"\"\n",
    "        for alert in alerts:\n",
    "            severity_emoji = 'üö®' if alert['severity'] == 'CRITICAL' else '‚ö†Ô∏è'\n",
    "            print(f\"\\n{severity_emoji} {alert['severity']} ALERT\")\n",
    "            print(f\"   Metric: {alert['metric']}\")\n",
    "            print(f\"   {alert['message']}\")\n",
    "            print(f\"   Recommended Action: {alert['action']}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"üö® Alerting System Example\\n\" + \"=\"*60)\n",
    "\n",
    "alerting = AlertingSystem()\n",
    "\n",
    "# Define thresholds\n",
    "thresholds = {\n",
    "    'min_accuracy': 0.80,\n",
    "    'max_latency_p95': 500,  # ms\n",
    "    'min_confidence': 0.70,\n",
    "    'max_error_rate': 0.05,  # 5%\n",
    "}\n",
    "\n",
    "# Simulate degraded performance\n",
    "print(\"\\nüìâ Simulating degraded system performance...\\n\")\n",
    "\n",
    "current_metrics = {\n",
    "    'accuracy': 0.75,  # Below threshold!\n",
    "    'latency_p95': 650,  # Too high!\n",
    "    'avg_confidence': 0.65,  # Too low!\n",
    "    'error_rate': 0.02,  # OK\n",
    "}\n",
    "\n",
    "alerts = alerting.check_alerts(current_metrics, thresholds)\n",
    "alerting.send_alerts(alerts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ In production, these alerts would trigger:\")\n",
    "print(\"   ‚Ä¢ Slack/Email notifications\")\n",
    "print(\"   ‚Ä¢ PagerDuty incidents\")\n",
    "print(\"   ‚Ä¢ Automatic rollback to previous model version\")\n",
    "print(\"   ‚Ä¢ Runbook links for debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîñ Pillar 4: Model Versioning\n",
    "\n",
    "You MUST track:\n",
    "- Which model version is in production?\n",
    "- What data was it trained on?\n",
    "- What were the hyperparameters?\n",
    "- When was it deployed?\n",
    "\n",
    "**Best practice: Use MLflow or similar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"Simple model registry for tracking versions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "    \n",
    "    def register_model(self, version, metadata):\n",
    "        \"\"\"Register a new model version\"\"\"\n",
    "        self.models[version] = {\n",
    "            **metadata,\n",
    "            'registered_at': datetime.now().isoformat(),\n",
    "            'status': 'registered'\n",
    "        }\n",
    "        print(f\"‚úÖ Registered model version {version}\")\n",
    "    \n",
    "    def promote_to_production(self, version):\n",
    "        \"\"\"Promote a model to production\"\"\"\n",
    "        if version not in self.models:\n",
    "            raise ValueError(f\"Version {version} not found\")\n",
    "        \n",
    "        # Demote current production model\n",
    "        for v, data in self.models.items():\n",
    "            if data['status'] == 'production':\n",
    "                data['status'] = 'archived'\n",
    "                data['archived_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Promote new model\n",
    "        self.models[version]['status'] = 'production'\n",
    "        self.models[version]['deployed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        print(f\"üöÄ Promoted version {version} to production\")\n",
    "    \n",
    "    def get_production_model(self):\n",
    "        \"\"\"Get currently deployed model\"\"\"\n",
    "        for version, data in self.models.items():\n",
    "            if data['status'] == 'production':\n",
    "                return version, data\n",
    "        return None, None\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all registered models\"\"\"\n",
    "        return self.models\n",
    "\n",
    "# Example usage\n",
    "print(\"üîñ Model Registry Example\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register version 1.0\n",
    "registry.register_model(\n",
    "    version=\"1.0.0\",\n",
    "    metadata={\n",
    "        'model_type': 'DistilBERT',\n",
    "        'training_data': 'customer_qa_v1.csv',\n",
    "        'num_examples': 10000,\n",
    "        'accuracy': 0.87,\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': 3e-5,\n",
    "            'batch_size': 16,\n",
    "            'epochs': 3,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Deploy to production\n",
    "registry.promote_to_production(\"1.0.0\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Register version 1.1 with improvements\n",
    "registry.register_model(\n",
    "    version=\"1.1.0\",\n",
    "    metadata={\n",
    "        'model_type': 'DistilBERT',\n",
    "        'training_data': 'customer_qa_v2.csv',\n",
    "        'num_examples': 15000,  # More data\n",
    "        'accuracy': 0.91,  # Better!\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': 2e-5,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 5,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check production model\n",
    "prod_version, prod_data = registry.get_production_model()\n",
    "print(f\"\\nüìä Current Production Model: v{prod_version}\")\n",
    "print(f\"   Accuracy: {prod_data['accuracy']:.1%}\")\n",
    "print(f\"   Deployed: {prod_data['deployed_at']}\")\n",
    "\n",
    "print(\"\\nüí° Benefits of model registry:\")\n",
    "print(\"   ‚Ä¢ Easy rollback if new model fails\")\n",
    "print(\"   ‚Ä¢ Track what changed between versions\")\n",
    "print(\"   ‚Ä¢ Reproducible experiments\")\n",
    "print(\"   ‚Ä¢ Compliance and auditing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Pillar 5: Continuous Training Pipeline\n",
    "\n",
    "Models degrade over time. You need a pipeline to:\n",
    "1. Detect when retraining is needed\n",
    "2. Automatically retrain\n",
    "3. Evaluate new model\n",
    "4. Deploy if better\n",
    "\n",
    "**Simple CI/CD for ML:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTrainingPipeline:\n",
    "    \"\"\"Automated model retraining pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor, registry):\n",
    "        self.monitor = monitor\n",
    "        self.registry = registry\n",
    "    \n",
    "    def should_retrain(self, current_metrics, thresholds):\n",
    "        \"\"\"Decide if model needs retraining\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        # Accuracy dropped significantly\n",
    "        if current_metrics['accuracy'] < thresholds['min_accuracy']:\n",
    "            reasons.append(f\"Accuracy below threshold: {current_metrics['accuracy']:.1%} < {thresholds['min_accuracy']:.1%}\")\n",
    "        \n",
    "        # Confidence dropping (data drift)\n",
    "        if current_metrics['avg_confidence'] < thresholds['min_confidence']:\n",
    "            reasons.append(f\"Confidence dropping: {current_metrics['avg_confidence']:.1%}\")\n",
    "        \n",
    "        # Enough new data accumulated\n",
    "        if current_metrics.get('new_examples', 0) > thresholds['retrain_data_threshold']:\n",
    "            reasons.append(f\"New data available: {current_metrics['new_examples']} examples\")\n",
    "        \n",
    "        # Time-based (retrain every N days)\n",
    "        prod_version, prod_data = self.registry.get_production_model()\n",
    "        if prod_data:\n",
    "            deployed_at = datetime.fromisoformat(prod_data['deployed_at'])\n",
    "            days_since_deploy = (datetime.now() - deployed_at).days\n",
    "            if days_since_deploy > thresholds['max_days_since_retrain']:\n",
    "                reasons.append(f\"Model is {days_since_deploy} days old\")\n",
    "        \n",
    "        return len(reasons) > 0, reasons\n",
    "    \n",
    "    def retrain_model(self):\n",
    "        \"\"\"Retrain model (simplified)\"\"\"\n",
    "        print(\"üîÑ Starting retraining...\")\n",
    "        print(\"   1. Fetching latest data...\")\n",
    "        print(\"   2. Preprocessing...\")\n",
    "        print(\"   3. Training model...\")\n",
    "        print(\"   4. Evaluating on test set...\")\n",
    "        \n",
    "        # Simulate training\n",
    "        new_accuracy = np.random.uniform(0.88, 0.93)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': new_accuracy,\n",
    "            'model_path': f'/models/model_{datetime.now().strftime(\"%Y%m%d\")}',\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self, current_metrics, thresholds):\n",
    "        \"\"\"Run the full continuous training pipeline\"\"\"\n",
    "        print(\"üîÑ CONTINUOUS TRAINING PIPELINE\\n\" + \"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Check if retraining needed\n",
    "        should_retrain, reasons = self.should_retrain(current_metrics, thresholds)\n",
    "        \n",
    "        if not should_retrain:\n",
    "            print(\"‚úÖ Model performance is good. No retraining needed.\")\n",
    "            return\n",
    "        \n",
    "        print(\"‚ö†Ô∏è Retraining triggered!\")\n",
    "        print(\"\\nReasons:\")\n",
    "        for reason in reasons:\n",
    "            print(f\"   ‚Ä¢ {reason}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Retrain\n",
    "        new_model = self.retrain_model()\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Evaluate\n",
    "        print(\"üìä Model Comparison:\")\n",
    "        prod_version, prod_data = self.registry.get_production_model()\n",
    "        print(f\"   Current (v{prod_version}): {prod_data['accuracy']:.1%}\")\n",
    "        print(f\"   New Model: {new_model['accuracy']:.1%}\")\n",
    "        \n",
    "        # Step 4: Deploy if better\n",
    "        if new_model['accuracy'] > prod_data['accuracy']:\n",
    "            new_version = self._increment_version(prod_version)\n",
    "            \n",
    "            self.registry.register_model(\n",
    "                version=new_version,\n",
    "                metadata={\n",
    "                    'model_type': 'DistilBERT',\n",
    "                    'accuracy': new_model['accuracy'],\n",
    "                    'retrained': True,\n",
    "                    'previous_version': prod_version,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.registry.promote_to_production(new_version)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Deployed new model v{new_version}!\")\n",
    "            print(f\"   Improvement: {new_model['accuracy'] - prod_data['accuracy']:.2%}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è New model not better. Keeping current version.\")\n",
    "    \n",
    "    def _increment_version(self, version):\n",
    "        \"\"\"Increment semantic version\"\"\"\n",
    "        parts = version.split('.')\n",
    "        parts[-1] = str(int(parts[-1]) + 1)\n",
    "        return '.'.join(parts)\n",
    "\n",
    "# Example usage\n",
    "pipeline = ContinuousTrainingPipeline(monitor, registry)\n",
    "\n",
    "# Simulate degraded metrics that trigger retraining\n",
    "degraded_metrics = {\n",
    "    'accuracy': 0.78,  # Dropped!\n",
    "    'avg_confidence': 0.68,\n",
    "    'new_examples': 5000,\n",
    "}\n",
    "\n",
    "retrain_thresholds = {\n",
    "    'min_accuracy': 0.80,\n",
    "    'min_confidence': 0.70,\n",
    "    'retrain_data_threshold': 3000,\n",
    "    'max_days_since_retrain': 30,\n",
    "}\n",
    "\n",
    "pipeline.run_pipeline(degraded_metrics, retrain_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Complete MLOps Checklist\n",
    "\n",
    "Use this checklist for every production ML system:\n",
    "\n",
    "### Pre-Deployment ‚úÖ\n",
    "- [ ] Model versioning set up\n",
    "- [ ] Training data versioned\n",
    "- [ ] Hyperparameters documented\n",
    "- [ ] Evaluation metrics defined\n",
    "- [ ] Performance baseline established\n",
    "\n",
    "### Deployment ‚úÖ\n",
    "- [ ] Health check endpoint\n",
    "- [ ] Metrics endpoint\n",
    "- [ ] Structured logging configured\n",
    "- [ ] Error tracking enabled\n",
    "- [ ] Load testing completed\n",
    "- [ ] Rollback plan documented\n",
    "\n",
    "### Post-Deployment ‚úÖ\n",
    "- [ ] Dashboards created (Grafana/DataDog)\n",
    "- [ ] Alerts configured (PagerDuty/Slack)\n",
    "- [ ] On-call rotation set up\n",
    "- [ ] Incident runbooks written\n",
    "- [ ] Performance reviewed weekly\n",
    "- [ ] Retraining schedule defined\n",
    "\n",
    "### Monitoring Metrics ‚úÖ\n",
    "- [ ] Accuracy/F1 score\n",
    "- [ ] Latency (p50, p95, p99)\n",
    "- [ ] Throughput (requests/sec)\n",
    "- [ ] Error rate\n",
    "- [ ] Confidence distribution\n",
    "- [ ] Input data distribution\n",
    "- [ ] Resource usage (CPU/memory)\n",
    "- [ ] Cost per prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interview Answers: MLOps Questions\n",
    "\n",
    "### Q: \"How do you monitor models in production?\"\n",
    "\n",
    "**Your Answer:**\n",
    "> \"I monitor four categories of metrics:\n",
    ">\n",
    "> 1. **Model Performance**: Accuracy, F1, confidence scores using rolling windows to detect degradation\n",
    "> 2. **System Performance**: Latency (p95/p99), throughput, error rates using Prometheus\n",
    "> 3. **Data Quality**: Input distributions, OOV rates to detect drift\n",
    "> 4. **Business Metrics**: Escalation rate, cost per prediction, user satisfaction\n",
    ">\n",
    "> I use Prometheus for metrics collection, Grafana for dashboards, and set up alerts in PagerDuty when accuracy drops below 80% or latency exceeds 500ms. In my support bot project, this caught a 15% accuracy drop within 2 hours due to new product features not in training data.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"How do you handle model drift?\"\n",
    "\n",
    "**Your Answer:**\n",
    "> \"I use a multi-stage approach:\n",
    ">\n",
    "> 1. **Detection**: Monitor confidence scores, input distributions, and per-class accuracy. If avg confidence drops 10% or input distribution shifts significantly (KL divergence), I investigate.\n",
    ">\n",
    "> 2. **Validation**: Compare recent performance (last 7 days) to baseline. If accuracy drops below threshold, trigger retraining.\n",
    ">\n",
    "> 3. **Retraining**: Automated pipeline collects new labeled data, retrains model, evaluates on holdout set.\n",
    ">\n",
    "> 4. **Deployment**: If new model is 2%+ better, deploy via blue-green deployment. Monitor for 24 hours before full rollout.\n",
    ">\n",
    "> I also schedule monthly retraining regardless of drift to incorporate new examples.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"Describe your ML deployment pipeline\"\n",
    "\n",
    "**Your Answer:**\n",
    "> \"My pipeline has 6 stages:\n",
    ">\n",
    "> 1. **Training**: Train model, track with MLflow (hyperparams, metrics, artifacts)\n",
    "> 2. **Validation**: Test on holdout set, check for regression vs current prod model\n",
    "> 3. **Staging**: Deploy to staging environment, run integration tests\n",
    "> 4. **Canary**: Route 5% traffic to new model, monitor for 2 hours\n",
    "> 5. **Gradual Rollout**: If metrics good, increase to 25%, 50%, 100% over 24 hours\n",
    "> 6. **Monitor**: Track performance, ready to rollback if issues\n",
    ">\n",
    "> Everything is automated via GitHub Actions and Docker. Rollback takes <2 minutes using previous container version.\"\n",
    "\n",
    "---\n",
    "\n",
    "**These answers show you actually understand production ML!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ You're Now an MLOps Pro!\n",
    "\n",
    "You learned:\n",
    "- ‚úÖ Comprehensive monitoring (4 types of metrics)\n",
    "- ‚úÖ Structured logging for debugging\n",
    "- ‚úÖ Smart alerting systems\n",
    "- ‚úÖ Model versioning and registry\n",
    "- ‚úÖ Continuous training pipelines\n",
    "\n",
    "**This knowledge makes you stand out!**\n",
    "\n",
    "90% of ML engineers can train models.  \n",
    "Only 10% can maintain them in production.\n",
    "\n",
    "You're now in that 10%! üí™\n",
    "\n",
    "---\n",
    "\n",
    "**Put on your resume:**\n",
    "- \"Implemented comprehensive monitoring system tracking 15+ production metrics\"\n",
    "- \"Built automated retraining pipeline reducing model deployment time by 75%\"\n",
    "- \"Set up alerting system detecting model degradation within 2 hours\"\n",
    "\n",
    "**Now go build production ML systems! You've got this! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
