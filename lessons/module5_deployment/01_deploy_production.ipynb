{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module5_deployment/01_deploy_production.ipynb)\n",
    "\n",
    "# üöÄ Deploy Your Model to Production\n",
    "\n",
    "**Duration:** 2 hours  \n",
    "**Level:** Intermediate  \n",
    "**What You'll Learn:** How to actually deploy models so people can use them\n",
    "\n",
    "---\n",
    "\n",
    "## This Is What Separates Hobbyists from Professionals!\n",
    "\n",
    "Real talk: **Training a model in a notebook means NOTHING if you can't deploy it.**\n",
    "\n",
    "Companies don't care about your Colab notebooks. They care if you can:\n",
    "- ‚úÖ Deploy a model as an API\n",
    "- ‚úÖ Handle real user traffic\n",
    "- ‚úÖ Monitor performance\n",
    "- ‚úÖ Update models without downtime\n",
    "\n",
    "**This lesson teaches you ALL of that.**\n",
    "\n",
    "We'll deploy the same model THREE different ways:\n",
    "1. **HuggingFace Spaces** (Easiest, free)\n",
    "2. **FastAPI + Docker** (Production-grade)\n",
    "3. **AWS Lambda** (Serverless, scalable)\n",
    "\n",
    "By the end, you'll have a deployed model with a public URL you can share with recruiters! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Deployment Option 1: HuggingFace Spaces (Start Here!)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Completely free\n",
    "- ‚úÖ No server management\n",
    "- ‚úÖ Automatic HTTPS\n",
    "- ‚úÖ Public URL instantly\n",
    "- ‚úÖ Perfect for portfolio\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Limited to Gradio/Streamlit\n",
    "- ‚ùå Slower than dedicated servers\n",
    "- ‚ùå Can't customize infrastructure\n",
    "\n",
    "**Best for:** Demos, portfolio projects, MVP\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Create Your Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py - This is what you'll deploy!\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned model\n",
    "# In production, you'd load from HuggingFace Hub or local path\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "def predict(text):\n",
    "    \"\"\"Get sentiment prediction\"\"\"\n",
    "    result = classifier(text)[0]\n",
    "    \n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    return f\"**Prediction:** {label}\\n**Confidence:** {score:.2%}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Textbox(label=\"Enter text to analyze\", placeholder=\"Type something...\"),\n",
    "    outputs=gr.Markdown(label=\"Result\"),\n",
    "    title=\"Sentiment Analysis Model\",\n",
    "    description=\"Fine-tuned DistilBERT for sentiment classification. Part of my ML portfolio.\",\n",
    "    examples=[\n",
    "        [\"This product is amazing! I love it!\"],\n",
    "        [\"Terrible experience. Very disappointed.\"],\n",
    "        [\"It's okay, nothing special.\"],\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Deploy to HuggingFace Spaces\n",
    "\n",
    "```bash\n",
    "# 1. Create a new Space on huggingface.co/spaces\n",
    "# 2. Choose \"Gradio\" as the SDK\n",
    "# 3. Clone the repo:\n",
    "git clone https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME\n",
    "cd YOUR_SPACE_NAME\n",
    "\n",
    "# 4. Create these files:\n",
    "```\n",
    "\n",
    "**app.py** (your Gradio code from above)\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "gradio\n",
    "transformers\n",
    "torch\n",
    "```\n",
    "\n",
    "**README.md:**\n",
    "```markdown\n",
    "---\n",
    "title: Sentiment Analysis\n",
    "emoji: üé≠\n",
    "colorFrom: blue\n",
    "colorTo: green\n",
    "sdk: gradio\n",
    "sdk_version: 4.0.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# Sentiment Analysis Model\n",
    "\n",
    "Fine-tuned DistilBERT for sentiment classification.\n",
    "\n",
    "Built by [Your Name] as part of ML portfolio.\n",
    "```\n",
    "\n",
    "```bash\n",
    "# 5. Push to HuggingFace:\n",
    "git add .\n",
    "git commit -m \"Deploy sentiment analysis model\"\n",
    "git push\n",
    "```\n",
    "\n",
    "**That's it!** Your model is now live at:\n",
    "`https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME`\n",
    "\n",
    "üéâ **Put this link on your resume and LinkedIn!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Deployment Option 2: FastAPI + Docker (Production-Grade)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Full control\n",
    "- ‚úÖ REST API (industry standard)\n",
    "- ‚úÖ Can deploy anywhere (AWS, GCP, Azure)\n",
    "- ‚úÖ Scalable\n",
    "- ‚úÖ What companies actually use\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Requires more setup\n",
    "- ‚ùå Need to manage servers\n",
    "- ‚ùå Costs money (unless free tier)\n",
    "\n",
    "**Best for:** Production apps, companies, professional projects\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Create FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py - Production-grade FastAPI app\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Sentiment Analysis API\",\n",
    "    description=\"Production-ready sentiment analysis using fine-tuned transformers\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load model on startup\n",
    "model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load model when server starts\"\"\"\n",
    "    global model\n",
    "    logger.info(\"Loading model...\")\n",
    "    model = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully!\")\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str\n",
    "    \n",
    "class PredictionResponse(BaseModel):\n",
    "    label: str\n",
    "    confidence: float\n",
    "    processing_time_ms: float\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Check if service is healthy\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None\n",
    "    }\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Get sentiment prediction\"\"\"\n",
    "    \n",
    "    if not model:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    if not request.text or len(request.text.strip()) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"Text cannot be empty\")\n",
    "    \n",
    "    # Log request\n",
    "    logger.info(f\"Prediction request: {request.text[:50]}...\")\n",
    "    \n",
    "    # Time the prediction\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = model(request.text)[0]\n",
    "        processing_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        response = PredictionResponse(\n",
    "            label=result['label'],\n",
    "            confidence=result['score'],\n",
    "            processing_time_ms=processing_time\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Prediction: {response.label} ({response.confidence:.2f}) in {processing_time:.2f}ms\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Prediction failed\")\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.post(\"/predict/batch\")\n",
    "async def predict_batch(texts: list[str]):\n",
    "    \"\"\"Batch prediction for multiple texts\"\"\"\n",
    "    \n",
    "    if not model:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    if len(texts) > 100:\n",
    "        raise HTTPException(status_code=400, detail=\"Max 100 texts per batch\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = model(texts)\n",
    "    processing_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": results,\n",
    "        \"count\": len(results),\n",
    "        \"processing_time_ms\": processing_time\n",
    "    }\n",
    "\n",
    "# Metrics endpoint (for monitoring)\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    \"\"\"Get service metrics\"\"\"\n",
    "    # In production, you'd track real metrics\n",
    "    return {\n",
    "        \"total_requests\": \"See logs\",\n",
    "        \"average_latency_ms\": \"See logs\",\n",
    "        \"model_version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY main.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "transformers==4.35.0\n",
    "torch==2.1.0\n",
    "pydantic==2.5.0\n",
    "```\n",
    "\n",
    "### Step 3: Build and Run\n",
    "\n",
    "```bash\n",
    "# Build Docker image\n",
    "docker build -t sentiment-api .\n",
    "\n",
    "# Run container\n",
    "docker run -p 8000:8000 sentiment-api\n",
    "```\n",
    "\n",
    "### Step 4: Test Your API\n",
    "\n",
    "```bash\n",
    "# Health check\n",
    "curl http://localhost:8000/health\n",
    "\n",
    "# Single prediction\n",
    "curl -X POST \"http://localhost:8000/predict\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"text\": \"This is amazing!\"}'\n",
    "\n",
    "# Batch prediction\n",
    "curl -X POST \"http://localhost:8000/predict/batch\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '[\"Great product!\", \"Terrible service\"]'\n",
    "```\n",
    "\n",
    "### Step 5: Deploy to Cloud\n",
    "\n",
    "**AWS (ECS):**\n",
    "```bash\n",
    "# Push to ECR\n",
    "aws ecr create-repository --repository-name sentiment-api\n",
    "docker tag sentiment-api:latest YOUR_ECR_URL/sentiment-api:latest\n",
    "docker push YOUR_ECR_URL/sentiment-api:latest\n",
    "\n",
    "# Deploy to ECS (use AWS Console or Terraform)\n",
    "```\n",
    "\n",
    "**Google Cloud Run:**\n",
    "```bash\n",
    "# Build and deploy in one command!\n",
    "gcloud run deploy sentiment-api \\\n",
    "  --source . \\\n",
    "  --platform managed \\\n",
    "  --region us-central1 \\\n",
    "  --allow-unauthenticated\n",
    "```\n",
    "\n",
    "**Azure Container Instances:**\n",
    "```bash\n",
    "# Push to ACR\n",
    "az acr build --registry myregistry --image sentiment-api .\n",
    "\n",
    "# Deploy\n",
    "az container create \\\n",
    "  --resource-group myResourceGroup \\\n",
    "  --name sentiment-api \\\n",
    "  --image myregistry.azurecr.io/sentiment-api:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Deployment Option 3: AWS Lambda (Serverless)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Scales automatically\n",
    "- ‚úÖ Pay per request (cheap!)\n",
    "- ‚úÖ Zero server management\n",
    "- ‚úÖ High availability built-in\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Cold start latency\n",
    "- ‚ùå Size limits (10GB)\n",
    "- ‚ùå 15-minute timeout\n",
    "\n",
    "**Best for:** Variable traffic, cost optimization, microservices\n",
    "\n",
    "---\n",
    "\n",
    "### Lambda Handler Code\n",
    "\n",
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "import boto3\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model (done once per container)\n",
    "model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"AWS Lambda handler\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse request\n",
    "        body = json.loads(event['body'])\n",
    "        text = body['text']\n",
    "        \n",
    "        # Predict\n",
    "        result = model(text)[0]\n",
    "        \n",
    "        # Return response\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*'  # CORS\n",
    "            },\n",
    "            'body': json.dumps({\n",
    "                'label': result['label'],\n",
    "                'confidence': result['score']\n",
    "            })\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "```\n",
    "\n",
    "### Deploy to Lambda\n",
    "\n",
    "```bash\n",
    "# 1. Create deployment package\n",
    "pip install -t package transformers torch\n",
    "cd package\n",
    "zip -r ../deployment-package.zip .\n",
    "cd ..\n",
    "zip -g deployment-package.zip lambda_function.py\n",
    "\n",
    "# 2. Create Lambda function\n",
    "aws lambda create-function \\\n",
    "  --function-name sentiment-analysis \\\n",
    "  --runtime python3.9 \\\n",
    "  --role YOUR_LAMBDA_ROLE_ARN \\\n",
    "  --handler lambda_function.lambda_handler \\\n",
    "  --zip-file fileb://deployment-package.zip \\\n",
    "  --memory-size 3008 \\\n",
    "  --timeout 60\n",
    "\n",
    "# 3. Create API Gateway endpoint\n",
    "# (Use AWS Console - easier for beginners)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comparison Table\n",
    "\n",
    "| Feature | HuggingFace Spaces | FastAPI + Docker | AWS Lambda |\n",
    "|---------|-------------------|------------------|------------|\n",
    "| **Cost** | Free | $5-50/mo | $0.20 per 1M requests |\n",
    "| **Setup Time** | 5 minutes | 30 minutes | 1 hour |\n",
    "| **Scalability** | Limited | Manual | Automatic |\n",
    "| **Latency** | 2-3s | <500ms | 1-2s (cold), <200ms (warm) |\n",
    "| **Control** | Low | High | Medium |\n",
    "| **Best For** | Demos | Production | Variable traffic |\n",
    "| **Resume Impact** | Good | Excellent | Excellent |\n",
    "\n",
    "**My recommendation:**\n",
    "1. **Start with HuggingFace Spaces** - Get something live fast\n",
    "2. **Learn FastAPI + Docker** - What companies actually use\n",
    "3. **Try Lambda** - Shows you know cloud/serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Production Best Practices\n",
    "\n",
    "### 1. **Add Authentication**\n",
    "```python\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "\n",
    "security = HTTPBearer()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(\n",
    "    request: PredictionRequest,\n",
    "    credentials: HTTPAuthorizationCredentials = Depends(security)\n",
    "):\n",
    "    # Verify API key\n",
    "    if credentials.credentials != \"your-secret-key\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    # ... rest of code\n",
    "```\n",
    "\n",
    "### 2. **Add Rate Limiting**\n",
    "```python\n",
    "from slowapi import Limiter\n",
    "from slowapi.util import get_remote_address\n",
    "\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "@limiter.limit(\"100/minute\")  # Max 100 requests per minute\n",
    "async def predict(request: Request, data: PredictionRequest):\n",
    "    # ... code\n",
    "```\n",
    "\n",
    "### 3. **Add Monitoring**\n",
    "```python\n",
    "import prometheus_client\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Metrics\n",
    "prediction_counter = Counter('predictions_total', 'Total predictions')\n",
    "prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: PredictionRequest):\n",
    "    with prediction_latency.time():\n",
    "        result = model(request.text)\n",
    "    prediction_counter.inc()\n",
    "    return result\n",
    "```\n",
    "\n",
    "### 4. **Add Caching**\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def predict_cached(text: str):\n",
    "    return model(text)\n",
    "```\n",
    "\n",
    "### 5. **Error Handling**\n",
    "```python\n",
    "@app.exception_handler(Exception)\n",
    "async def global_exception_handler(request: Request, exc: Exception):\n",
    "    logger.error(f\"Unhandled error: {exc}\")\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"error\": \"Internal server error\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "### 6. **Model Versioning**\n",
    "```python\n",
    "@app.post(\"/v1/predict\")  # Version in URL\n",
    "async def predict_v1(request: PredictionRequest):\n",
    "    return model_v1(request.text)\n",
    "\n",
    "@app.post(\"/v2/predict\")  # New version doesn't break old clients\n",
    "async def predict_v2(request: PredictionRequest):\n",
    "    return model_v2(request.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Resume Bullets\n",
    "\n",
    "**After completing this lesson, you can say:**\n",
    "\n",
    "- \"Deployed ML models to production using FastAPI, Docker, and AWS Lambda\"\n",
    "- \"Built RESTful API serving 1000+ requests/day with <200ms latency\"\n",
    "- \"Implemented monitoring, rate limiting, and authentication for production ML services\"\n",
    "- \"Reduced infrastructure costs by 60% using serverless deployment (AWS Lambda)\"\n",
    "- \"Created public demo showcasing fine-tuned models (link: [your HF Space])\"\n",
    "\n",
    "**Interview answers:**\n",
    "\n",
    "Q: \"How do you deploy ML models?\"\n",
    "A: \"I've deployed models three different ways: HuggingFace Spaces for quick demos, FastAPI + Docker for production environments, and AWS Lambda for cost-effective serverless deployment. For my last project, I chose [X] because [business reason].\"\n",
    "\n",
    "Q: \"How do you monitor models in production?\"\n",
    "A: \"I track latency, throughput, and error rates using Prometheus. I also log all predictions with confidence scores to detect model drift. If confidence drops below threshold, I alert the team.\"\n",
    "\n",
    "Q: \"How do you handle scaling?\"\n",
    "A: \"For Docker deployments, I use container orchestration (ECS/Kubernetes) with auto-scaling based on CPU/memory. For Lambda, scaling is automatic. I also implement caching and batch processing to reduce load.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You now know how to deploy ML models like a professional!\n",
    "\n",
    "**Action items:**\n",
    "1. ‚úÖ Deploy one model to HuggingFace Spaces (do this now!)\n",
    "2. ‚úÖ Build a FastAPI wrapper locally\n",
    "3. ‚úÖ Test with Docker\n",
    "4. ‚úÖ Deploy to cloud (free tier)\n",
    "5. ‚úÖ Add URL to LinkedIn/resume\n",
    "\n",
    "Companies love seeing **live deployed models**. This immediately sets you apart from 90% of candidates who only have notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** MLOps & Monitoring - Keep your models healthy in production! üìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
