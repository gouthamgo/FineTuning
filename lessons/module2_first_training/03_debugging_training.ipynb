{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module2_first_training/03_debugging_training.ipynb)\n",
    "\n",
    "# üîß Debugging Like a Pro (When Things Go Wrong)\n",
    "\n",
    "**Duration:** 1 hour  \n",
    "**Level:** Intermediate  \n",
    "**Prerequisites:** Module 2, Lessons 1-2\n",
    "\n",
    "---\n",
    "\n",
    "## Hey Friend, Let's Fix Some Bugs! üêõ\n",
    "\n",
    "Okay, real talk time:\n",
    "\n",
    "**Your model WILL fail. Multiple times. And that's totally normal!**\n",
    "\n",
    "I've been doing this for years and I STILL run into errors every single day. The difference between a beginner and a pro isn't that pros don't get errors - it's that **pros know how to fix them fast**.\n",
    "\n",
    "Think of this lesson as your **\"Oh crap, it's broken!\"** survival guide.\n",
    "\n",
    "We're going to cover:\n",
    "1. The most common errors (and how to fix them)\n",
    "2. How to debug training issues\n",
    "3. When your model \"works\" but gives bad results\n",
    "4. Memory errors (the #1 frustration!)\n",
    "\n",
    "Let's become debugging ninjas! ü•∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® The Top 10 Errors (And How to Fix Them)\n",
    "\n",
    "Let me save you HOURS of frustration. Here are the errors everyone hits:\n",
    "\n",
    "### 1. **\"CUDA Out of Memory\" üí•**\n",
    "\n",
    "**What it looks like:**\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB\n",
    "```\n",
    "\n",
    "**What it means:**\n",
    "Your GPU ran out of RAM. Like trying to fit 10 pounds of potatoes in a 5-pound bag.\n",
    "\n",
    "**How to fix:**\n",
    "```python\n",
    "# Option 1: Smaller batch size\n",
    "per_device_train_batch_size=8  # Instead of 16 or 32\n",
    "\n",
    "# Option 2: Shorter sequences\n",
    "max_length=128  # Instead of 512\n",
    "\n",
    "# Option 3: Gradient accumulation (simulate larger batches)\n",
    "gradient_accumulation_steps=4\n",
    "per_device_train_batch_size=4  # 4 * 4 = effective batch of 16\n",
    "\n",
    "# Option 4: Enable mixed precision\n",
    "fp16=True  # Uses less memory\n",
    "\n",
    "# Option 5: Smaller model\n",
    "# Use 'distilbert' instead of 'bert-base'\n",
    "# Use 'bert-base' instead of 'bert-large'\n",
    "```\n",
    "\n",
    "**Pro tip:** Start small (batch_size=4), then gradually increase until you hit the limit!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **\"Size Mismatch\" Error ‚ö†Ô∏è**\n",
    "\n",
    "**What it looks like:**\n",
    "```\n",
    "RuntimeError: The size of tensor a (128) must match the size of tensor b (512)\n",
    "```\n",
    "\n",
    "**What it means:**\n",
    "Your input is the wrong size for the model.\n",
    "\n",
    "**How to fix:**\n",
    "```python\n",
    "# Make sure you're using the SAME tokenizer as your model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ensure padding and truncation\n",
    "tokenizer(text, padding='max_length', truncation=True, max_length=512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **\"Token Type IDs Error\" üé≠**\n",
    "\n",
    "**What it looks like:**\n",
    "```\n",
    "TypeError: forward() got an unexpected keyword argument 'token_type_ids'\n",
    "```\n",
    "\n",
    "**What it means:**\n",
    "Some models (like RoBERTa) don't use token_type_ids, but the tokenizer creates them.\n",
    "\n",
    "**How to fix:**\n",
    "```python\n",
    "# Remove them from your tokenized data\n",
    "tokenized = tokenizer(text, truncation=True, padding=True)\n",
    "tokenized.pop('token_type_ids', None)  # Remove if present\n",
    "\n",
    "# Or use this in your dataset map function\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "    result.pop('token_type_ids', None)\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **\"Loss is NaN\" üìà**\n",
    "\n",
    "**What it looks like:**\n",
    "```\n",
    "loss: nan\n",
    "```\n",
    "\n",
    "**What it means:**\n",
    "Your training exploded! The numbers got too big and became \"Not a Number\".\n",
    "\n",
    "**How to fix:**\n",
    "```python\n",
    "# Lower your learning rate\n",
    "learning_rate=1e-5  # Instead of 1e-4 or higher\n",
    "\n",
    "# Add gradient clipping\n",
    "max_grad_norm=1.0  # Prevents gradient explosion\n",
    "\n",
    "# Enable fp16 mixed precision\n",
    "fp16=True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **\"No module named 'transformers'\" üì¶**\n",
    "\n",
    "**What it means:**\n",
    "You forgot to install the library!\n",
    "\n",
    "**How to fix:**\n",
    "```python\n",
    "!pip install transformers datasets torch accelerate\n",
    "```\n",
    "\n",
    "**In Colab?** Run this at the TOP of your notebook, BEFORE importing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Debugging Training Issues\n",
    "\n",
    "Your code runs... but something's wrong. Let's diagnose:\n",
    "\n",
    "### Issue: \"Loss isn't decreasing\" üìâ\n",
    "\n",
    "Let's create a diagnostic script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers torch accelerate evaluate matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load small dataset for testing\n",
    "dataset = load_dataset('imdb')\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(100))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(50))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "train_data = small_train.map(tokenize, batched=True)\n",
    "test_data = small_test.map(tokenize, batched=True)\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"‚úÖ Data ready for debugging!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Test #1: Is Your Model Learning Anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ DIAGNOSTIC TEST: Can the model overfit on a tiny dataset?\\n\")\n",
    "print(\"If a model can't overfit on 10 examples, something is VERY wrong.\\n\")\n",
    "\n",
    "# Take ONLY 10 examples\n",
    "tiny_data = small_train.select(range(10)).map(tokenize, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./test_overfit',\n",
    "    num_train_epochs=20,  # Many epochs on tiny data\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tiny_data,\n",
    "    eval_dataset=tiny_data,  # Eval on same data!\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTS:\\n\")\n",
    "print(f\"Final loss: {result.training_loss:.4f}\")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Accuracy on training data: {eval_result['eval_accuracy']:.3f}\")\n",
    "\n",
    "if eval_result['eval_accuracy'] > 0.95:\n",
    "    print(\"\\n‚úÖ GOOD! Model CAN learn (it overfitted on tiny data)\")\n",
    "    print(\"   ‚Üí Your model setup is working\")\n",
    "    print(\"   ‚Üí Problem is probably with hyperparameters or data\")\n",
    "else:\n",
    "    print(\"\\n‚ùå BAD! Model CANNOT learn even on 10 examples!\")\n",
    "    print(\"   ‚Üí Check your model architecture\")\n",
    "    print(\"   ‚Üí Check your data labels\")\n",
    "    print(\"   ‚Üí Check your loss function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Test #2: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot training history\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"Plot loss over time\"\"\"\n",
    "    logs = trainer.state.log_history\n",
    "    \n",
    "    train_loss = [log['loss'] for log in logs if 'loss' in log]\n",
    "    eval_loss = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss', marker='o')\n",
    "    if eval_loss:\n",
    "        plt.plot(eval_loss, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Step', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('üìâ Training Progress', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot (if available)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    eval_acc = [log['eval_accuracy'] for log in logs if 'eval_accuracy' in log]\n",
    "    if eval_acc:\n",
    "        plt.plot(eval_acc, label='Accuracy', marker='o', color='green')\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('üéØ Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "        plt.ylim([0, 1])\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Diagnosis\n",
    "    print(\"\\nüîç WHAT DO THESE CHARTS TELL US?\\n\")\n",
    "    \n",
    "    if len(train_loss) > 1:\n",
    "        if train_loss[-1] < train_loss[0] * 0.5:\n",
    "            print(\"‚úÖ Training loss is decreasing nicely!\")\n",
    "        elif train_loss[-1] > train_loss[0]:\n",
    "            print(\"‚ùå Training loss is INCREASING! That's bad!\")\n",
    "            print(\"   ‚Üí Try lower learning rate\")\n",
    "            print(\"   ‚Üí Check your data for errors\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Training loss barely changing\")\n",
    "            print(\"   ‚Üí Try higher learning rate\")\n",
    "            print(\"   ‚Üí Train more epochs\")\n",
    "    \n",
    "    if len(eval_loss) > 1 and len(train_loss) > 1:\n",
    "        if eval_loss[-1] > train_loss[-1] * 1.5:\n",
    "            print(\"\\n‚ö†Ô∏è Eval loss >> Train loss = OVERFITTING!\")\n",
    "            print(\"   ‚Üí Use more training data\")\n",
    "            print(\"   ‚Üí Add regularization (weight_decay)\")\n",
    "            print(\"   ‚Üí Train fewer epochs\")\n",
    "\n",
    "plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Issue: \"Model Always Predicts The Same Thing\"\n",
    "\n",
    "Your model just predicts \"positive\" for EVERYTHING. What's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_predictions(trainer, dataset, num_examples=20):\n",
    "    \"\"\"Check if model is actually making different predictions\"\"\"\n",
    "    \n",
    "    predictions = trainer.predict(dataset.select(range(num_examples)))\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    print(\"üîç PREDICTION ANALYSIS\\n\" + \"=\"*60)\n",
    "    print(f\"\\nChecking {num_examples} examples...\\n\")\n",
    "    \n",
    "    # Count predictions\n",
    "    unique, counts = np.unique(pred_labels, return_counts=True)\n",
    "    \n",
    "    print(\"Predicted labels:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Class {label}: {count}/{num_examples} ({count/num_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Check if model is stuck\n",
    "    if len(unique) == 1:\n",
    "        print(\"\\n‚ùå PROBLEM: Model predicts ONLY class\", unique[0])\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  1. Class imbalance in training data\")\n",
    "        print(\"  2. Learning rate too high (model collapsed)\")\n",
    "        print(\"  3. Wrong loss function\")\n",
    "        print(\"  4. Model not trained enough\")\n",
    "        print(\"\\nTry:\")\n",
    "        print(\"  ‚Üí Check data balance (should be ~50/50)\")\n",
    "        print(\"  ‚Üí Lower learning rate to 1e-5\")\n",
    "        print(\"  ‚Üí Train more epochs\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Model makes different predictions (that's good!)\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nüìã Sample predictions:\\n\")\n",
    "    for i in range(min(5, num_examples)):\n",
    "        correct = \"‚úÖ\" if pred_labels[i] == true_labels[i] else \"‚ùå\"\n",
    "        print(f\"{correct} True: {true_labels[i]}, Predicted: {pred_labels[i]}\")\n",
    "\n",
    "diagnose_predictions(trainer, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Memory Management Tips\n",
    "\n",
    "**The #1 beginner frustration: Running out of memory**\n",
    "\n",
    "Here's how to handle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY-EFFICIENT TRAINING CONFIG\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "memory_efficient_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    \n",
    "    # 1. Smallest batch size that still works\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    \n",
    "    # 2. Simulate larger batch with gradient accumulation\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 4 * 4 = 16\n",
    "    \n",
    "    # 3. Enable mixed precision (uses less memory)\n",
    "    fp16=True,  # For NVIDIA GPUs\n",
    "    \n",
    "    # 4. Clear cache between batches\n",
    "    gradient_checkpointing=True,  # Saves memory during backprop\n",
    "    \n",
    "    # 5. Don't keep all checkpoints\n",
    "    save_total_limit=2,  # Only keep 2 best models\n",
    "    \n",
    "    # Standard settings\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy='epoch',\n",
    ")\n",
    "\n",
    "print(\"üí° These settings should work on most free-tier GPUs!\")\n",
    "print(\"\\nIf you STILL run out of memory:\")\n",
    "print(\"  1. Reduce max_length when tokenizing\")\n",
    "print(\"  2. Use a smaller model (distilbert instead of bert)\")\n",
    "print(\"  3. Reduce batch_size to 2 or even 1\")\n",
    "print(\"  4. Train on CPU (slow but works!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Your Debugging Checklist\n",
    "\n",
    "When something goes wrong, go through this list:\n",
    "\n",
    "### ‚úÖ Before Training:\n",
    "- [ ] Data loaded correctly?\n",
    "- [ ] Labels are correct (0, 1, 2... not text)?\n",
    "- [ ] Tokenizer matches the model?\n",
    "- [ ] Data is balanced (or you're aware it's not)?\n",
    "- [ ] Train/test split done properly?\n",
    "\n",
    "### ‚úÖ During Training:\n",
    "- [ ] Loss is decreasing?\n",
    "- [ ] No NaN in loss?\n",
    "- [ ] Training speed reasonable?\n",
    "- [ ] Memory usage okay?\n",
    "- [ ] Validation metrics improving?\n",
    "\n",
    "### ‚úÖ After Training:\n",
    "- [ ] Train accuracy > random guessing?\n",
    "- [ ] Test accuracy reasonable?\n",
    "- [ ] Model makes varied predictions?\n",
    "- [ ] No severe overfitting?\n",
    "- [ ] Results make sense for your task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöë Emergency Quick Fixes\n",
    "\n",
    "**Training exploded (NaN loss)?**\n",
    "```python\n",
    "learning_rate=1e-5  # Lower!\n",
    "max_grad_norm=1.0   # Clip gradients\n",
    "```\n",
    "\n",
    "**Out of memory?**\n",
    "```python\n",
    "per_device_train_batch_size=4  # Smaller!\n",
    "fp16=True  # Mixed precision\n",
    "max_length=128  # Shorter sequences\n",
    "```\n",
    "\n",
    "**Not learning?**\n",
    "```python\n",
    "learning_rate=5e-5  # Higher!\n",
    "num_train_epochs=5  # More epochs\n",
    "# Check your data!\n",
    "```\n",
    "\n",
    "**Overfitting?**\n",
    "```python\n",
    "weight_decay=0.01  # Add regularization\n",
    "num_train_epochs=2  # Fewer epochs\n",
    "# Get more data if possible\n",
    "```\n",
    "\n",
    "**Too slow?**\n",
    "```python\n",
    "per_device_train_batch_size=32  # Larger!\n",
    "fp16=True  # Faster\n",
    "# Use smaller dataset for testing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Final Wisdom\n",
    "\n",
    "Here's what I wish someone told me when I started:\n",
    "\n",
    "1. **Errors are NORMAL** - You'll see hundreds. Don't panic!\n",
    "\n",
    "2. **Start small** - Get it working on 100 examples before trying 100,000\n",
    "\n",
    "3. **One change at a time** - Change learning rate OR batch size, not both!\n",
    "\n",
    "4. **Google is your friend** - 99% of errors, someone else hit first\n",
    "\n",
    "5. **Check your data FIRST** - Most \"model problems\" are actually data problems\n",
    "\n",
    "6. **Visualize everything** - Plot loss, plot predictions, plot distributions\n",
    "\n",
    "7. **Save your work** - Nothing worse than losing a good model\n",
    "\n",
    "8. **Ask for help** - Community is super friendly!\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You're Now a Debugging Ninja!\n",
    "\n",
    "You learned:\n",
    "- ‚úÖ Common errors and fixes\n",
    "- ‚úÖ How to diagnose training issues\n",
    "- ‚úÖ Memory management\n",
    "- ‚úÖ Quick debugging techniques\n",
    "\n",
    "**Remember:** Every expert was once a beginner who didn't give up!\n",
    "\n",
    "You got this! üí™\n",
    "\n",
    "---\n",
    "\n",
    "**Next up:** Module 3 - Advanced Techniques! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resources\n",
    "\n",
    "**When you're stuck:**\n",
    "- [HuggingFace Forums](https://discuss.huggingface.co/) - Super active community\n",
    "- [Stack Overflow](https://stackoverflow.com/questions/tagged/huggingface-transformers) - For specific errors\n",
    "- [HuggingFace Docs](https://huggingface.co/docs/transformers) - Official documentation\n",
    "\n",
    "**For debugging:**\n",
    "- Google the EXACT error message\n",
    "- Check GitHub Issues for the library\n",
    "- Ask in our community Discord!\n",
    "\n",
    "Don't struggle alone - we're here to help! ü§ù"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
