{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module2_first_training/02_hyperparameter_tuning.ipynb)\n",
    "\n",
    "# ğŸ›ï¸ Making Your Model Better (Hyperparameter Tuning)\n",
    "\n",
    "**Duration:** 1.5 hours  \n",
    "**Level:** Intermediate  \n",
    "**Prerequisites:** Module 2, Lesson 1\n",
    "\n",
    "---\n",
    "\n",
    "## Hey! So Your Model Works... But Could It Be Better? ğŸ¤”\n",
    "\n",
    "Okay, so you trained your first model in the last lesson. Awesome!\n",
    "\n",
    "But here's the thing: That was like cooking a recipe for the first time with the default settings. It worked, but...\n",
    "\n",
    "**What if you could make it EVEN BETTER?**\n",
    "\n",
    "That's what we're doing today. We're going to learn about **hyperparameters** - the secret knobs and dials that control how your model learns.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "ğŸ® **Training a model = Playing a video game**\n",
    "- Easy mode vs Hard mode\n",
    "- Fast combat vs Slow & strategic\n",
    "- Auto-save frequently vs Save manually\n",
    "\n",
    "Different settings = different results!\n",
    "\n",
    "Let's learn how to tune these settings like a pro! ğŸ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ What Even Are Hyperparameters?\n",
    "\n",
    "Hyperparameters are settings you choose BEFORE training starts. They control:\n",
    "- How fast the model learns\n",
    "- How long it trains\n",
    "- How much data it sees at once\n",
    "- And more!\n",
    "\n",
    "Here are the most important ones:\n",
    "\n",
    "### 1. **Learning Rate** ğŸƒ\n",
    "**How fast should the model learn?**\n",
    "\n",
    "- Too high? â†’ Model learns wrong things (overshoots)\n",
    "- Too low? â†’ Takes forever to learn\n",
    "- Just right? â†’ Learns efficiently!\n",
    "\n",
    "**Analogy:** Like walking to a destination\n",
    "- Too fast = you run past it\n",
    "- Too slow = takes forever\n",
    "- Just right = you get there efficiently\n",
    "\n",
    "**Typical values:** 1e-5 to 5e-5 for fine-tuning\n",
    "\n",
    "### 2. **Batch Size** ğŸ“¦\n",
    "**How many examples to show at once?**\n",
    "\n",
    "- Bigger batch = faster training, more memory\n",
    "- Smaller batch = slower, less memory, sometimes better results\n",
    "\n",
    "**Analogy:** Like reading books\n",
    "- Batch size 1 = Read one page, think, read next page\n",
    "- Batch size 32 = Read 32 pages, then think about all of them\n",
    "\n",
    "**Typical values:** 8, 16, 32 (limited by GPU memory)\n",
    "\n",
    "### 3. **Number of Epochs** ğŸ”„\n",
    "**How many times to see all the data?**\n",
    "\n",
    "- Too few = underfitting (doesn't learn enough)\n",
    "- Too many = overfitting (memorizes instead of learning)\n",
    "- Just right = learns the patterns!\n",
    "\n",
    "**Analogy:** Studying for an exam\n",
    "- 1 epoch = Read the book once\n",
    "- 10 epochs = Read it 10 times\n",
    "- Too many = You memorize exact words but don't understand\n",
    "\n",
    "**Typical values:** 3-5 for fine-tuning\n",
    "\n",
    "### 4. **Weight Decay** âš–ï¸\n",
    "**How much to prevent overfitting?**\n",
    "\n",
    "Think of it as a regularization penalty that keeps the model from getting too complex.\n",
    "\n",
    "**Typical values:** 0.01\n",
    "\n",
    "### 5. **Warmup Steps** ğŸŒ…\n",
    "**Should we start slow?**\n",
    "\n",
    "Instead of jumping to full learning rate immediately, gradually increase it at the start.\n",
    "\n",
    "**Analogy:** Like warming up before exercise - you don't sprint immediately!\n",
    "\n",
    "**Typical values:** 10% of total training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Let's Experiment!\n",
    "\n",
    "The best way to understand this? Try different settings and see what happens!\n",
    "\n",
    "First, let's set everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers torch accelerate evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load a smaller dataset for quick experiments\n",
    "print(\"ğŸ“¥ Loading dataset...\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Use only 1000 examples for quick training\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(f\"âœ… Training on {len(small_train)} examples\")\n",
    "print(f\"âœ… Testing on {len(small_test)} examples\")\n",
    "\n",
    "# Load tokenizer and tokenize\n",
    "print(\"\\nğŸ“ Tokenizing...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = small_train.map(tokenize_function, batched=True)\n",
    "test_dataset = small_test.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"âœ… Ready to train!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Experiment 1: Learning Rate\n",
    "\n",
    "Let's try three different learning rates and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# We'll test 3 learning rates\n",
    "learning_rates = [1e-5, 3e-5, 1e-4]\n",
    "results = {}\n",
    "\n",
    "print(\"ğŸ§ª Testing different learning rates...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nğŸ¯ Training with learning rate = {lr}\")\n",
    "    \n",
    "    # Load fresh model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', \n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_lr_{lr}',\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    results[lr] = {\n",
    "        'accuracy': eval_result['eval_accuracy'],\n",
    "        'loss': eval_result['eval_loss'],\n",
    "        'train_time': train_result.metrics['train_runtime']\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Accuracy: {eval_result['eval_accuracy']:.3f}\")\n",
    "    print(f\"   âœ… Loss: {eval_result['eval_loss']:.3f}\")\n",
    "    print(f\"   â±ï¸ Time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nğŸ“Š RESULTS SUMMARY:\\n\")\n",
    "for lr, result in results.items():\n",
    "    print(f\"Learning Rate {lr:>8}: Accuracy = {result['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Let's Visualize the Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot accuracies\n",
    "lrs = list(results.keys())\n",
    "accuracies = [results[lr]['accuracy'] for lr in lrs]\n",
    "losses = [results[lr]['loss'] for lr in lrs]\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].bar(range(len(lrs)), accuracies, color=['#51cf66', '#ffd43b', '#ff6b6b'])\n",
    "axes[0].set_xticks(range(len(lrs)))\n",
    "axes[0].set_xticklabels([f'{lr}' for lr in lrs])\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('ğŸ¯ Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
    "for i, acc in enumerate(accuracies):\n",
    "    axes[0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Loss plot\n",
    "axes[1].bar(range(len(lrs)), losses, color=['#51cf66', '#ffd43b', '#ff6b6b'])\n",
    "axes[1].set_xticks(range(len(lrs)))\n",
    "axes[1].set_xticklabels([f'{lr}' for lr in lrs])\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('ğŸ“‰ Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "for i, loss in enumerate(losses):\n",
    "    axes[1].text(i, loss, f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best\n",
    "best_lr = max(results.keys(), key=lambda lr: results[lr]['accuracy'])\n",
    "print(f\"\\nğŸ† WINNER: Learning Rate = {best_lr}\")\n",
    "print(f\"   Best Accuracy: {results[best_lr]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ What Did We Learn?\n",
    "\n",
    "Look at those results! Here's what probably happened:\n",
    "\n",
    "- **1e-5 (small):** Might work but learns slowly\n",
    "- **3e-5 (medium):** Usually the sweet spot! âœ¨\n",
    "- **1e-4 (large):** Might be too aggressive, could overshoot\n",
    "\n",
    "This is why **3e-5 is the default** in most tutorials - it's proven to work well!\n",
    "\n",
    "But here's the secret: **The best learning rate depends on your data and model!**\n",
    "\n",
    "Always experiment! ğŸ§ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Experiment 2: Batch Size\n",
    "\n",
    "Now let's try different batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8, 16, 32]\n",
    "batch_results = {}\n",
    "\n",
    "print(\"ğŸ§ª Testing different batch sizes...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nğŸ“¦ Training with batch size = {batch_size}\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', \n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_bs_{batch_size}',\n",
    "        learning_rate=3e-5,  # Use the best one from before!\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=2,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    batch_results[batch_size] = {\n",
    "        'accuracy': eval_result['eval_accuracy'],\n",
    "        'train_time': train_result.metrics['train_runtime']\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Accuracy: {eval_result['eval_accuracy']:.3f}\")\n",
    "    print(f\"   â±ï¸ Time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nğŸ“Š BATCH SIZE COMPARISON:\\n\")\n",
    "for bs, result in batch_results.items():\n",
    "    print(f\"Batch Size {bs:>2}: Accuracy = {result['accuracy']:.3f}, Time = {result['train_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Batch Size Insights\n",
    "\n",
    "What you'll probably notice:\n",
    "\n",
    "**Larger batch sizes:**\n",
    "- âœ… Train FASTER (fewer updates needed)\n",
    "- âŒ Use MORE memory\n",
    "- âŒ Sometimes slightly lower accuracy\n",
    "\n",
    "**Smaller batch sizes:**\n",
    "- âœ… Use LESS memory (great for big models)\n",
    "- âœ… Sometimes better accuracy (more frequent updates)\n",
    "- âŒ Train SLOWER\n",
    "\n",
    "**Pro tip:** If you're running out of GPU memory, reduce batch size!\n",
    "\n",
    "**Most common:** 16 or 32 for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ The Ultimate Setup\n",
    "\n",
    "Based on what we learned, here's a great starting point for most fine-tuning tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE RECOMMENDED SETUP ğŸŒŸ\n",
    "\n",
    "optimal_args = TrainingArguments(\n",
    "    output_dir='./results_optimal',\n",
    "    \n",
    "    # Learning\n",
    "    learning_rate=3e-5,              # Sweet spot for most tasks\n",
    "    num_train_epochs=3,               # Usually enough to learn well\n",
    "    \n",
    "    # Batch sizes\n",
    "    per_device_train_batch_size=16,   # Good balance\n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay=0.01,                # Prevent overfitting\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    warmup_steps=100,                 # Start slow, ramp up\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy='epoch',      # Check after each epoch\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,     # Keep the best version\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Other useful stuff\n",
    "    save_total_limit=2,              # Only keep 2 best checkpoints (save space)\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(\"âœ¨ This is a solid starting point for 80% of fine-tuning tasks!\")\n",
    "print(\"\\nFeel free to adjust based on:\")\n",
    "print(\"  - Your GPU memory (lower batch_size if needed)\")\n",
    "print(\"  - Your dataset size (more epochs for smaller datasets)\")\n",
    "print(\"  - Your results (experiment with learning_rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ Advanced Tips\n",
    "\n",
    "### 1. **Learning Rate Scheduling**\n",
    "Instead of constant learning rate, decrease it over time:\n",
    "```python\n",
    "lr_scheduler_type='cosine'  # Gradually decrease\n",
    "```\n",
    "\n",
    "### 2. **Gradient Accumulation**\n",
    "Simulate larger batches without using more memory:\n",
    "```python\n",
    "gradient_accumulation_steps=4  # 4 small batches = 1 big batch\n",
    "```\n",
    "\n",
    "### 3. **Mixed Precision Training**\n",
    "Train faster with less memory:\n",
    "```python\n",
    "fp16=True  # If you have a modern GPU\n",
    "```\n",
    "\n",
    "### 4. **Early Stopping**\n",
    "Stop if not improving:\n",
    "```python\n",
    "load_best_model_at_end=True\n",
    "metric_for_best_model='eval_loss'\n",
    "early_stopping_patience=3  # Stop if no improvement for 3 evals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Your Hyperparameter Cheat Sheet\n",
    "\n",
    "| Hyperparameter | What it Does | Typical Range | Start With |\n",
    "|---------------|--------------|---------------|------------|\n",
    "| **learning_rate** | How fast to learn | 1e-5 to 5e-5 | 3e-5 |\n",
    "| **batch_size** | Examples per update | 8 to 32 | 16 |\n",
    "| **num_epochs** | Training loops | 2 to 5 | 3 |\n",
    "| **weight_decay** | Regularization | 0 to 0.1 | 0.01 |\n",
    "| **warmup_steps** | Warmup period | 0 to 1000 | 500 |\n",
    "| **max_length** | Text truncation | 128 to 512 | 256 |\n",
    "\n",
    "### ğŸš¦ When to Adjust:\n",
    "\n",
    "**Model not learning?** (Loss not decreasing)\n",
    "- âœ… Increase learning rate\n",
    "- âœ… Train more epochs\n",
    "- âœ… Check your data!\n",
    "\n",
    "**Model overfitting?** (Train accuracy >> Test accuracy)\n",
    "- âœ… Decrease learning rate\n",
    "- âœ… Increase weight decay\n",
    "- âœ… Use fewer epochs\n",
    "- âœ… Get more training data\n",
    "\n",
    "**Running out of memory?**\n",
    "- âœ… Decrease batch size\n",
    "- âœ… Decrease max_length\n",
    "- âœ… Use gradient_accumulation_steps\n",
    "- âœ… Enable fp16\n",
    "\n",
    "**Training too slow?**\n",
    "- âœ… Increase batch size\n",
    "- âœ… Enable fp16\n",
    "- âœ… Use a smaller model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ You're Now a Tuning Pro!\n",
    "\n",
    "You just learned:\n",
    "- âœ… What hyperparameters are\n",
    "- âœ… How to experiment with them\n",
    "- âœ… Which ones matter most\n",
    "- âœ… How to debug common issues\n",
    "\n",
    "**Remember:** The \"perfect\" hyperparameters don't exist!\n",
    "\n",
    "Every dataset is different. Every task is unique. The key is:\n",
    "\n",
    "1. **Start with good defaults** (3e-5 learning rate, 16 batch size, 3 epochs)\n",
    "2. **Monitor your metrics** (Is it learning? Overfitting?)\n",
    "3. **Experiment systematically** (Change ONE thing at a time)\n",
    "4. **Trust the data** (Let results guide you)\n",
    "\n",
    "You got this! ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**Next lesson:** Debugging Like a Pro - What to do when things go wrong! ğŸ”§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ Practice Exercise\n",
    "\n",
    "Your homework (yes, fun homework!):\n",
    "\n",
    "1. **Pick a dataset** (emotion, sst2, or any text classification dataset)\n",
    "2. **Train with 3 different learning rates**\n",
    "3. **Compare the results**\n",
    "4. **Share what you found!**\n",
    "\n",
    "Bonus points:\n",
    "- Try different batch sizes\n",
    "- Enable fp16 and measure speedup\n",
    "- Use gradient accumulation\n",
    "\n",
    "Post your experiments in the community - we love seeing your progress! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
