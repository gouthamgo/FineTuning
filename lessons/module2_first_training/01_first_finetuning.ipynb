{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# üéØ Lesson 2.1: Your First Fine-Tuning!\n",
    "\n",
    "**Duration:** 2 hours  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Module 1 completed\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. **Actually fine-tune a model** (for real!)\n",
    "2. Prepare a dataset for training\n",
    "3. Configure training parameters\n",
    "4. Watch your model learn in real-time\n",
    "5. Evaluate your trained model\n",
    "6. Save and use your model\n",
    "\n",
    "**This is the BIG one - you're about to become a fine-tuner! üöÄ**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-well-do",
   "metadata": {},
   "source": [
    "## üìã What We'll Build\n",
    "\n",
    "**Project:** Movie Review Sentiment Classifier\n",
    "\n",
    "**Task:** Classify movie reviews as positive or negative\n",
    "\n",
    "**Model:** DistilBERT (small, fast, beginner-friendly)\n",
    "\n",
    "**Dataset:** IMDB movie reviews (small subset for speed)\n",
    "\n",
    "**Why this project?**\n",
    "- Simple binary classification (2 classes)\n",
    "- Fast to train (5-10 minutes)\n",
    "- Easy to understand results\n",
    "- Real-world applicable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets torch evaluate scikit-learn\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported!\")\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-data",
   "metadata": {},
   "source": [
    "## üìä Step 1: Load and Explore Data\n",
    "\n",
    "Let's load a small subset of IMDB reviews for quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Let's use a smaller subset for faster training\n",
    "# In real projects, use more data!\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"\\nDataset structure: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some examples\n",
    "print(\"Sample reviews:\\n\")\n",
    "for i in range(3):\n",
    "    review = train_dataset[i]\n",
    "    label = \"POSITIVE\" if review['label'] == 1 else \"NEGATIVE\"\n",
    "    text = review['text'][:200] + \"...\"  # First 200 characters\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "train_labels = [example['label'] for example in train_dataset]\n",
    "positive_count = sum(train_labels)\n",
    "negative_count = len(train_labels) - positive_count\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"Positive: {positive_count} ({positive_count/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"Negative: {negative_count} ({negative_count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "if abs(positive_count - negative_count) < len(train_labels) * 0.2:\n",
    "    print(\"\\n‚úÖ Dataset is balanced! Good for training.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Dataset is imbalanced. Consider balancing techniques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-tokenize",
   "metadata": {},
   "source": [
    "## üî§ Step 2: Tokenize the Data\n",
    "\n",
    "Convert text to numbers that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Loaded tokenizer: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text data.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256  # Limit sequence length for speed\n",
    "    )\n",
    "\n",
    "# Apply tokenization to entire dataset\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"\\nTokenized features: {tokenized_train.features.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-tokenized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what tokenized data looks like\n",
    "example = tokenized_train[0]\n",
    "\n",
    "print(\"Example tokenized data:\\n\")\n",
    "print(f\"Original text: {train_dataset[0]['text'][:100]}...\\n\")\n",
    "print(f\"Input IDs (first 20): {example['input_ids'][:20]}\")\n",
    "print(f\"Attention mask (first 20): {example['attention_mask'][:20]}\")\n",
    "print(f\"Label: {example['label']}\")\n",
    "print(f\"\\nTotal tokens in this example: {len(example['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-model",
   "metadata": {},
   "source": [
    "## üß† Step 3: Load the Model\n",
    "\n",
    "Load a pre-trained model and prepare it for our task (binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification (positive/negative)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Number of labels: {model.num_labels}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.num_attention_heads}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-metrics",
   "metadata": {},
   "source": [
    "## üìä Step 4: Define Evaluation Metrics\n",
    "\n",
    "How will we measure if our model is learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")\n",
    "print(\"\\nMetrics we'll track:\")\n",
    "print(\"  - Accuracy: Overall correctness\")\n",
    "print(\"  - Precision: Of positive predictions, how many are correct?\")\n",
    "print(\"  - Recall: Of actual positives, how many did we find?\")\n",
    "print(\"  - F1: Harmonic mean of precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-training",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Configure Training\n",
    "\n",
    "Set up how we want to train the model.\n",
    "\n",
    "### Understanding Training Arguments:\n",
    "\n",
    "- **epochs**: How many times to see entire dataset (3 is good for small datasets)\n",
    "- **batch_size**: How many examples to process at once (8 or 16 for beginners)\n",
    "- **learning_rate**: How fast to learn (2e-5 is a safe default)\n",
    "- **weight_decay**: Regularization to prevent overfitting (0.01 is standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Where to save checkpoints\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size for training\n",
    "    per_device_eval_batch_size=16,      # Batch size for evaluation\n",
    "    learning_rate=2e-5,                  # Learning rate\n",
    "    weight_decay=0.01,                   # Weight decay for regularization\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy=\"epoch\",              # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",               # Save after each epoch\n",
    "    logging_steps=50,                    # Log every 50 steps\n",
    "    \n",
    "    # Performance\n",
    "    load_best_model_at_end=True,        # Load best model when done\n",
    "    metric_for_best_model=\"accuracy\",   # Use accuracy to determine best model\n",
    "    \n",
    "    # Optional: Reduce output\n",
    "    report_to=\"none\",                    # Don't report to wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total training steps: {len(tokenized_train) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-train",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Train the Model!\n",
    "\n",
    "**This is it! We're about to fine-tune your first model!**\n",
    "\n",
    "Watch the loss decrease - that means your model is learning! üìâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created and ready!\")\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "print(\"This will take 5-10 minutes. Watch the loss go down!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL!\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal training loss: {training_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {training_result.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-evaluate",
   "metadata": {},
   "source": [
    "## üìä Step 7: Evaluate the Model\n",
    "\n",
    "How well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {eval_results['eval_accuracy']:.2%}\")\n",
    "print(f\"Precision: {eval_results['eval_precision']:.2%}\")\n",
    "print(f\"Recall:    {eval_results['eval_recall']:.2%}\")\n",
    "print(f\"F1 Score:  {eval_results['eval_f1']:.2%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "if eval_results['eval_accuracy'] >= 0.85:\n",
    "    print(\"\\nüåü EXCELLENT! Your model is performing very well!\")\n",
    "elif eval_results['eval_accuracy'] >= 0.75:\n",
    "    print(\"\\n‚úÖ GOOD! Your model is working well. Consider more training data or epochs for improvement.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è The model is learning but could improve. Try more data or different hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-test",
   "metadata": {},
   "source": [
    "## üß™ Step 8: Test Your Model!\n",
    "\n",
    "Let's try it on real reviews (including your own!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"Predict sentiment of a given text.\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    \n",
    "    # Move to same device as model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    label = \"POSITIVE\" if prediction == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Test examples\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely amazing! Best film I've seen this year!\",\n",
    "    \"Terrible movie. Complete waste of time and money.\",\n",
    "    \"It was okay, nothing special but not terrible either.\",\n",
    "    \"I loved every minute of it! The acting was superb and the plot was gripping.\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\"\n",
    "]\n",
    "\n",
    "print(\"Testing your fine-tuned model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for review in test_reviews:\n",
    "    label, confidence = predict_sentiment(review)\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"‚Üí Prediction: {label} (confidence: {confidence:.2%})\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "your-reviews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ YOUR TURN: Test with your own reviews!\n",
    "\n",
    "your_reviews = [\n",
    "    \"Write your own movie review here!\",\n",
    "    # Add more reviews...\n",
    "]\n",
    "\n",
    "print(\"Your custom reviews:\\n\")\n",
    "for review in your_reviews:\n",
    "    label, confidence = predict_sentiment(review)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"‚Üí {label} ({confidence:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-save",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Save Your Model\n",
    "\n",
    "Save your fine-tuned model so you can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "save_directory = \"./my_first_finetuned_model\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"‚úÖ Model and tokenizer saved to: {save_directory}\")\n",
    "print(\"\\nYou can now load this model anytime with:\")\n",
    "print(f'model = AutoModelForSequenceClassification.from_pretrained(\"{save_directory}\")')\n",
    "print(f'tokenizer = AutoTokenizer.from_pretrained(\"{save_directory}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"Testing if we can load the saved model...\\n\")\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Quick test\n",
    "test_text = \"This is a great movie!\"\n",
    "inputs = loaded_tokenizer(test_text, return_tensors=\"pt\")\n",
    "outputs = loaded_model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "print(f\"\\nQuick test: '{test_text}'\")\n",
    "print(f\"Prediction: {'POSITIVE' if prediction == 1 else 'NEGATIVE'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congratulations",
   "metadata": {},
   "source": [
    "## üéâ CONGRATULATIONS!\n",
    "\n",
    "You just fine-tuned your first transformer model!\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "‚úÖ Loaded and explored a dataset  \n",
    "‚úÖ Tokenized text data  \n",
    "‚úÖ Configured training parameters  \n",
    "‚úÖ **Fine-tuned a model from scratch**  \n",
    "‚úÖ Evaluated model performance  \n",
    "‚úÖ Made predictions on new data  \n",
    "‚úÖ Saved your model for future use  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Data Preparation**: Load ‚Üí Tokenize ‚Üí Batching\n",
    "2. **Training Loop**: Model sees data multiple times (epochs)\n",
    "3. **Loss Decreasing = Learning**: Lower loss = better performance\n",
    "4. **Evaluation Metrics**: Accuracy, precision, recall, F1\n",
    "5. **Overfitting Prevention**: Validation set, early stopping\n",
    "\n",
    "### The Fine-Tuning Pipeline:\n",
    "\n",
    "```\n",
    "Data ‚Üí Tokenize ‚Üí Model ‚Üí Training Args ‚Üí Trainer ‚Üí Train ‚Üí Evaluate ‚Üí Save\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Challenge Yourself:\n",
    "\n",
    "1. **Experiment with hyperparameters:**\n",
    "   - Try different learning rates (1e-5, 3e-5, 5e-5)\n",
    "   - Adjust batch sizes (4, 8, 16)\n",
    "   - More epochs (5, 10)\n",
    "\n",
    "2. **Use more data:**\n",
    "   - Increase from 1000 to 5000 training examples\n",
    "   - See how accuracy improves!\n",
    "\n",
    "3. **Try different models:**\n",
    "   - `bert-base-uncased`\n",
    "   - `roberta-base`\n",
    "   - `albert-base-v2`\n",
    "\n",
    "4. **Upload to HuggingFace Hub:**\n",
    "   - Share your model with the world!\n",
    "   - `model.push_to_hub(\"my-awesome-sentiment-model\")`\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Reflection Questions\n",
    "\n",
    "Can you answer these?\n",
    "\n",
    "1. What does the loss value tell you about training?\n",
    "2. Why do we need a separate test set?\n",
    "3. What's the difference between accuracy and F1 score?\n",
    "4. How would you improve the model's performance?\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Lesson\n",
    "\n",
    "**Lesson 2.2: Evaluating Your Model**\n",
    "- Deep dive into metrics\n",
    "- Confusion matrices\n",
    "- Error analysis\n",
    "- When is your model good enough?\n",
    "\n",
    "---\n",
    "\n",
    "**Progress:** üü¢üü¢üü¢üü¢üîò (Lesson 4 of 15)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Achievement Unlocked!\n",
    "\n",
    "**üèÜ First Fine-Tuning Complete!**\n",
    "\n",
    "You're no longer a beginner - you're a fine-tuner! Keep going! üí™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
