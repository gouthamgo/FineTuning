{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module1_foundations/03_understanding_data.ipynb)\n",
    "\n",
    "# üìä Understanding Your Data (The Secret Sauce)\n",
    "\n",
    "**Duration:** 1 hour  \n",
    "**Level:** Beginner  \n",
    "**Prerequisites:** Module 1, Lessons 1-2\n",
    "\n",
    "---\n",
    "\n",
    "## Hey Friend! Let's Talk About Data üç≥\n",
    "\n",
    "Okay, so here's the thing about AI that nobody tells beginners:\n",
    "\n",
    "**Your data matters WAYYYY more than your model.**\n",
    "\n",
    "Seriously. I'm not kidding. Let me explain with an example:\n",
    "\n",
    "Imagine you want to become a great chef. You have two options:\n",
    "\n",
    "**Option A:** World's best kitchen + Rotten vegetables  \n",
    "**Option B:** Okay kitchen + Fresh, quality ingredients\n",
    "\n",
    "Which one makes better food? **Option B, every single time!**\n",
    "\n",
    "That's EXACTLY how AI works:\n",
    "- Your **model** = the kitchen\n",
    "- Your **data** = the ingredients\n",
    "\n",
    "Bad data = bad AI. Period.\n",
    "\n",
    "So in this lesson, we're going to learn how to:\n",
    "1. Look at your data (actually SEE what's in there)\n",
    "2. Clean your data (remove the rotten stuff)\n",
    "3. Prepare your data (chop it up the right way)\n",
    "4. Split your data (for training and testing)\n",
    "\n",
    "Let's do this! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install What We Need\n",
    "\n",
    "Same as before - we need the Hugging Face tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Some Real Data\n",
    "\n",
    "Let's use the IMDB movie reviews dataset. It's perfect for learning because:\n",
    "- It's free\n",
    "- It's real data (actual movie reviews)\n",
    "- It's labeled (we know which reviews are positive/negative)\n",
    "- It's messy (just like real-world data!)\n",
    "\n",
    "Let me show you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up pretty plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"üì• Loading IMDB dataset...\")\n",
    "dataset = load_dataset('imdb')\n",
    "print(\"‚úÖ Done!\\n\")\n",
    "\n",
    "# Let's see what we got\n",
    "print(\"Here's what the dataset looks like:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î What Are We Looking At?\n",
    "\n",
    "You should see something like:\n",
    "```\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['text', 'label'],\n",
    "        num_rows: 25000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['text', 'label'],\n",
    "        num_rows: 25000\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "Let me break this down:\n",
    "\n",
    "- **train:** 25,000 reviews for training our model\n",
    "- **test:** 25,000 reviews for testing how well it learned\n",
    "- **text:** The actual movie review (the words)\n",
    "- **label:** 0 = negative review, 1 = positive review\n",
    "\n",
    "Cool, right? Now let's actually LOOK at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some actual reviews!\n",
    "train_data = dataset['train']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ EXAMPLE 1: A POSITIVE Review (label = 1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find a positive review\n",
    "for i in range(10):\n",
    "    if train_data[i]['label'] == 1:\n",
    "        print(f\"\\n{train_data[i]['text'][:500]}...\")  # First 500 characters\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ EXAMPLE 2: A NEGATIVE Review (label = 0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find a negative review\n",
    "for i in range(10):\n",
    "    if train_data[i]['label'] == 0:\n",
    "        print(f\"\\n{train_data[i]['text'][:500]}...\")  # First 500 characters\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Explore Your Data (Like a Detective!)\n",
    "\n",
    "Before we do ANYTHING with data, we need to understand it. Here's what we want to know:\n",
    "\n",
    "1. **How much data do we have?** (More = better, usually)\n",
    "2. **Is it balanced?** (Equal positive/negative reviews?)\n",
    "3. **How long are the texts?** (Super important!)\n",
    "4. **Any weird stuff?** (Missing data, HTML tags, etc.)\n",
    "\n",
    "Let's investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier analysis\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "print(\"üìà DATASET STATISTICS\\n\" + \"=\"*50)\n",
    "print(f\"\\nüì¶ Total examples: {len(df):,}\")\n",
    "print(f\"\\nüòä Positive reviews: {(df['label'] == 1).sum():,}\")\n",
    "print(f\"üòû Negative reviews: {(df['label'] == 0).sum():,}\")\n",
    "print(f\"\\n‚öñÔ∏è Balance: {(df['label'] == 1).sum() / len(df) * 100:.1f}% positive\")\n",
    "\n",
    "# Add text length as a new column\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(f\"\\nüìù Average review length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"üìù Average word count: {df['word_count'].mean():.0f} words\")\n",
    "print(f\"\\nüìè Shortest review: {df['text_length'].min()} characters\")\n",
    "print(f\"üìè Longest review: {df['text_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Let's Visualize This!\n",
    "\n",
    "Numbers are cool, but pictures are better. Let's make some charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Chart 1: Positive vs Negative\n",
    "label_counts = df['label'].value_counts()\n",
    "axes[0, 0].bar(['Negative', 'Positive'], label_counts.values, color=['#ff6b6b', '#51cf66'])\n",
    "axes[0, 0].set_title('üé¨ Review Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0, 0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Chart 2: Text length distribution\n",
    "axes[0, 1].hist(df['text_length'], bins=50, color='#4dabf7', edgecolor='black')\n",
    "axes[0, 1].set_title('üìè Review Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Characters')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].axvline(df['text_length'].mean(), color='red', linestyle='--', label='Average')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Chart 3: Word count distribution\n",
    "axes[1, 0].hist(df['word_count'], bins=50, color='#ff922b', edgecolor='black')\n",
    "axes[1, 0].set_title('üìù Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].axvline(df['word_count'].mean(), color='red', linestyle='--', label='Average')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Chart 4: Length comparison by sentiment\n",
    "df.boxplot(column='word_count', by='label', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('üìä Word Count by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Label (0=Negative, 1=Positive)')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What do these charts tell us?\")\n",
    "print(\"   1. The data is perfectly balanced (50/50 positive/negative)\")\n",
    "print(\"   2. Most reviews are between 100-400 words\")\n",
    "print(\"   3. Some reviews are SUPER long (outliers)\")\n",
    "print(\"   4. Positive and negative reviews have similar lengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 4: Clean Your Data\n",
    "\n",
    "Real-world data is MESSY. Like, really messy. Let's look for problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Looking for data quality issues...\\n\")\n",
    "\n",
    "# Check for missing data\n",
    "print(\"1. Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"‚úÖ No missing data! Nice.\\n\")\n",
    "\n",
    "# Check for HTML tags (common in web-scraped data)\n",
    "html_count = df['text'].str.contains('<br', regex=False).sum()\n",
    "print(f\"2. Reviews with HTML tags: {html_count:,}\")\n",
    "if html_count > 0:\n",
    "    print(\"‚ö†Ô∏è We found HTML! Let's look at an example:\")\n",
    "    for text in df['text']:\n",
    "        if '<br' in text:\n",
    "            print(f\"\\n{text[:300]}...\")\n",
    "            break\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['text']).sum()\n",
    "print(f\"\\n3. Duplicate reviews: {duplicates}\")\n",
    "\n",
    "# Check for very short reviews (might be useless)\n",
    "very_short = (df['word_count'] < 10).sum()\n",
    "print(f\"\\n4. Very short reviews (<10 words): {very_short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßº Let's Clean That HTML!\n",
    "\n",
    "Those `<br />` tags are HTML line breaks. They don't help our model learn. Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags and extra whitespace\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Let's test it!\n",
    "print(\"üß™ Testing our cleaning function:\\n\")\n",
    "print(\"BEFORE:\")\n",
    "dirty_text = \"This movie was great!<br /><br />I loved it so much.\"\n",
    "print(dirty_text)\n",
    "\n",
    "print(\"\\nAFTER:\")\n",
    "clean = clean_text(dirty_text)\n",
    "print(clean)\n",
    "\n",
    "print(\"\\n‚úÖ Perfect! Now let's clean the whole dataset...\")\n",
    "\n",
    "# Clean all texts\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"‚úÖ Done! Let's compare:\")\n",
    "print(\"\\nORIGINAL:\")\n",
    "print(df['text'].iloc[0][:200])\n",
    "print(\"\\nCLEANED:\")\n",
    "print(df['clean_text'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 5: Split Your Data Properly\n",
    "\n",
    "This is CRUCIAL. Listen carefully:\n",
    "\n",
    "**You CANNOT test your model on the same data you trained it on!**\n",
    "\n",
    "Why? Imagine studying for an exam:\n",
    "- If you memorize the exact questions, you'll get 100%\n",
    "- But you didn't actually LEARN anything\n",
    "- You just memorized!\n",
    "\n",
    "Same with AI. We need:\n",
    "1. **Training data** - Model learns from this\n",
    "2. **Validation data** - We check progress during training\n",
    "3. **Test data** - Final exam (model never sees this during training!)\n",
    "\n",
    "Typical split: 80% train, 10% validation, 10% test\n",
    "\n",
    "Good news: IMDB dataset already split train/test for us! But let's learn how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's split our training data into train + validation\n",
    "train_df = df.copy()\n",
    "\n",
    "# Split: 90% train, 10% validation\n",
    "train_subset, val_subset = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1,  # 10% for validation\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=train_df['label']  # Keep same positive/negative ratio\n",
    ")\n",
    "\n",
    "print(\"‚úÇÔ∏è Data Split Results:\\n\" + \"=\"*50)\n",
    "print(f\"\\nüìö Training set: {len(train_subset):,} examples\")\n",
    "print(f\"   Positive: {(train_subset['label'] == 1).sum():,} ({(train_subset['label'] == 1).sum()/len(train_subset)*100:.1f}%)\")\n",
    "print(f\"   Negative: {(train_subset['label'] == 0).sum():,} ({(train_subset['label'] == 0).sum()/len(train_subset)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîç Validation set: {len(val_subset):,} examples\")\n",
    "print(f\"   Positive: {(val_subset['label'] == 1).sum():,} ({(val_subset['label'] == 1).sum()/len(val_subset)*100:.1f}%)\")\n",
    "print(f\"   Negative: {(val_subset['label'] == 0).sum():,} ({(val_subset['label'] == 0).sum()/len(val_subset)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Notice how the percentages are the same? That's what 'stratify' does!\")\n",
    "print(\"   It keeps the class balance consistent across splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Prepare Data for Your Model\n",
    "\n",
    "AI models don't understand words. They only understand numbers!\n",
    "\n",
    "So we need to convert:\n",
    "- \"This movie is great!\" ‚Üí [2023, 3742, 2003, 2307, 999]\n",
    "\n",
    "This is called **tokenization**. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer (we'll use BERT's)\n",
    "print(\"üì• Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"‚úÖ Done!\\n\")\n",
    "\n",
    "# Let's tokenize a sample text\n",
    "sample_text = \"This movie is absolutely fantastic! I loved it!\"\n",
    "\n",
    "print(\"üî§ Original text:\")\n",
    "print(f'   \"{sample_text}')\n",
    "\n",
    "# Tokenize it\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(\"\\nüî¢ Tokens (words/subwords):\")\n",
    "print(f\"   {tokens}\")\n",
    "\n",
    "# Convert to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nüî¢ Token IDs (numbers the model uses):\")\n",
    "print(f\"   {token_ids}\")\n",
    "\n",
    "# The easy way (does everything at once)\n",
    "encoded = tokenizer(sample_text, padding=True, truncation=True, return_tensors='pt')\n",
    "print(\"\\nüì¶ Full encoding:\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Lessons About Data\n",
    "\n",
    "Let me summarize what we learned:\n",
    "\n",
    "### 1. **Always Explore Your Data First** üîç\n",
    "   - Look at actual examples\n",
    "   - Check for missing values\n",
    "   - Visualize distributions\n",
    "   - Understand what you're working with\n",
    "\n",
    "### 2. **Clean Your Data** üßπ\n",
    "   - Remove HTML/special characters\n",
    "   - Handle missing values\n",
    "   - Fix inconsistencies\n",
    "   - Remove duplicates\n",
    "\n",
    "### 3. **Check Balance** ‚öñÔ∏è\n",
    "   - Are classes equally represented?\n",
    "   - If not, you might need to balance them\n",
    "   - Use stratification when splitting\n",
    "\n",
    "### 4. **Split Properly** ‚úÇÔ∏è\n",
    "   - Train: Model learns from this (70-80%)\n",
    "   - Validation: Monitor progress (10-15%)\n",
    "   - Test: Final evaluation (10-15%)\n",
    "   - NEVER test on training data!\n",
    "\n",
    "### 5. **Tokenize for Your Model** üî¢\n",
    "   - Models need numbers, not text\n",
    "   - Use the right tokenizer for your model\n",
    "   - Handle padding and truncation\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "You now understand data preparation - the MOST IMPORTANT part of fine-tuning!\n",
    "\n",
    "Remember: **Garbage in = Garbage out**\n",
    "\n",
    "Good data = Good AI. Every. Single. Time.\n",
    "\n",
    "In the next lesson, we'll actually fine-tune a model using this clean, prepared data!\n",
    "\n",
    "Ready? Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Quick Reference\n",
    "\n",
    "**Load dataset:**\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('dataset_name')\n",
    "```\n",
    "\n",
    "**Clean text:**\n",
    "```python\n",
    "import re\n",
    "text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML\n",
    "text = ' '.join(text.split())  # Remove extra spaces\n",
    "```\n",
    "\n",
    "**Split data:**\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(data, test_size=0.1, stratify=data['label'])\n",
    "```\n",
    "\n",
    "**Tokenize:**\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('model_name')\n",
    "encoded = tokenizer(text, padding=True, truncation=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéØ Practice Exercise\n",
    "\n",
    "Try this yourself:\n",
    "\n",
    "1. Load a different dataset (try 'emotion' or 'sst2')\n",
    "2. Explore it like we did here\n",
    "3. Clean any messy text\n",
    "4. Create train/val/test splits\n",
    "5. Tokenize a few examples\n",
    "\n",
    "Post your results in the community! We'd love to see what you found!\n",
    "\n",
    "**Next up:** Module 2, Lesson 1 - Your First Fine-Tuning! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
