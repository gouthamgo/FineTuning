{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# üöÄ Lesson 1.2: Your First Model\n",
    "\n",
    "**Duration:** 1 hour  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Lesson 1.1 completed\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Load your first pre-trained model\n",
    "2. Understand what tokenizers do\n",
    "3. Run inference (predictions) on text\n",
    "4. Understand model inputs and outputs\n",
    "\n",
    "**This is a hands-on lesson - you'll write and run real code!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "\n",
    "First, let's make sure everything is installed. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "# Uncomment the line below if running in Google Colab or fresh environment\n",
    "# !pip install transformers torch\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easiest-way",
   "metadata": {},
   "source": [
    "## üéà The Easiest Way: Using Pipelines\n",
    "\n",
    "HuggingFace provides `pipelines` - super simple ways to use pre-trained models.\n",
    "\n",
    "Let's try sentiment analysis (positive/negative classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# This automatically downloads a pre-trained model\n",
    "print(\"Loading model... (this might take a minute the first time)\")\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Now let's use it!\n",
    "result = classifier(\"I love this tutorial! It's so helpful!\")\n",
    "print(f\"Text: 'I love this tutorial! It's so helpful!'\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Try a negative example\n",
    "result2 = classifier(\"This is terrible and frustrating.\")\n",
    "print(f\"\\nText: 'This is terrible and frustrating.'\")\n",
    "print(f\"Result: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-explanation",
   "metadata": {},
   "source": [
    "### üéâ Congratulations!\n",
    "\n",
    "You just:\n",
    "1. Loaded a pre-trained model (DistilBERT)\n",
    "2. Made predictions on real text\n",
    "3. Got confidence scores\n",
    "\n",
    "**That was easy, right?** But what's happening under the hood? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try-yourself",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ YOUR TURN: Try your own texts!\n",
    "# Replace the text below with anything you want\n",
    "\n",
    "my_texts = [\n",
    "    \"The weather is beautiful today!\",\n",
    "    \"I'm disappointed with the service.\",\n",
    "    \"Just another day at work.\",\n",
    "    # Add your own texts here!\n",
    "]\n",
    "\n",
    "results = classifier(my_texts)\n",
    "\n",
    "for text, result in zip(my_texts, results):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"‚Üí {result['label']}: {result['score']:.2%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## üî§ Understanding Tokenizers\n",
    "\n",
    "Models can't read text directly - they need numbers. **Tokenizers** convert text to numbers.\n",
    "\n",
    "### What's a Token?\n",
    "\n",
    "A token is a piece of text (could be a word, part of a word, or punctuation).\n",
    "\n",
    "Example:\n",
    "```\n",
    "\"I love fine-tuning\" \n",
    "    ‚Üì (tokenization)\n",
    "[\"I\", \"love\", \"fine\", \"-\", \"tuning\"]\n",
    "    ‚Üì (to numbers)\n",
    "[1045, 2293, 2986, 1011, 17372]\n",
    "```\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"I love fine-tuning transformers!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert to IDs (numbers)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# The easy way (does both at once)\n",
    "encoded = tokenizer(text)\n",
    "print(f\"\\nEncoded (automatic): {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-special",
   "metadata": {},
   "source": [
    "### üéØ Special Tokens\n",
    "\n",
    "Notice the `[CLS]` and `[SEP]` tokens? These are special:\n",
    "\n",
    "- **[CLS]** - \"Classification\" token - goes at the start\n",
    "- **[SEP]** - \"Separator\" token - marks the end\n",
    "- **[PAD]** - \"Padding\" token - used to make sequences the same length\n",
    "- **[UNK]** - \"Unknown\" token - for words not in vocabulary\n",
    "\n",
    "Let's explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all special tokens\n",
    "print(\"Special tokens:\")\n",
    "print(f\"CLS token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"SEP token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "\n",
    "# Let's see a complete tokenization with special tokens\n",
    "text = \"Hello world!\"\n",
    "tokens_with_special = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Tokens with special tokens: {tokenizer.convert_ids_to_tokens(tokenizer.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "your-turn-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ YOUR TURN: Tokenize your own text\n",
    "\n",
    "your_text = \"Write anything here and see how it's tokenized!\"\n",
    "\n",
    "tokens = tokenizer.tokenize(your_text)\n",
    "encoded = tokenizer(your_text)\n",
    "\n",
    "print(f\"Your text: {your_text}\")\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"\\nToken IDs: {encoded['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-details",
   "metadata": {},
   "source": [
    "## üß† Loading Models (The Manual Way)\n",
    "\n",
    "Pipelines are easy, but let's understand what's happening step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer separately\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded!\")\n",
    "print(f\"\\nModel has {model.num_labels} labels (classes)\")\n",
    "print(f\"Label mapping: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-manual",
   "metadata": {},
   "source": [
    "### Running Inference Step-by-Step\n",
    "\n",
    "Now let's make predictions manually to understand the full process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare text\n",
    "text = \"This is amazing!\"\n",
    "\n",
    "# Step 2: Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Step 2 - Tokenized inputs:\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "\n",
    "# Step 3: Run through model\n",
    "with torch.no_grad():  # Don't compute gradients (we're not training)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"\\nStep 3 - Model outputs:\")\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Logits (raw scores): {outputs.logits}\")\n",
    "\n",
    "# Step 4: Convert to probabilities\n",
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "print(\"\\nStep 4 - Probabilities:\")\n",
    "print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "# Step 5: Get prediction\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "print(\"\\nStep 5 - Final prediction:\")\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Predicted class: {model.config.id2label[predicted_class]}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understand-process",
   "metadata": {},
   "source": [
    "### üìä What Just Happened?\n",
    "\n",
    "```\n",
    "Text ‚Üí Tokenizer ‚Üí Numbers ‚Üí Model ‚Üí Logits ‚Üí Softmax ‚Üí Probabilities ‚Üí Prediction\n",
    "\n",
    "\"Amazing!\" ‚Üí [101, 6429, 102] ‚Üí Model ‚Üí [-2.1, 3.4] ‚Üí Softmax ‚Üí [0.02, 0.98] ‚Üí POSITIVE (98%)\n",
    "```\n",
    "\n",
    "**Key Terms:**\n",
    "- **Logits**: Raw scores from the model (can be any number)\n",
    "- **Softmax**: Converts logits to probabilities (0-1, sum to 1)\n",
    "- **Argmax**: Picks the class with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Processing Multiple Texts at Once (Batching)\n",
    "\n",
    "texts = [\n",
    "    \"I love this!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay, I guess.\",\n",
    "    \"Absolutely fantastic!\",\n",
    "    \"Worst experience ever.\"\n",
    "]\n",
    "\n",
    "# Tokenize all at once\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Display results\n",
    "print(\"Batch predictions:\\n\")\n",
    "for text, probs in zip(texts, probabilities):\n",
    "    pred_class = torch.argmax(probs).item()\n",
    "    confidence = probs[pred_class].item()\n",
    "    label = model.config.id2label[pred_class]\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"‚Üí {label} ({confidence:.2%} confidence)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exploring-models",
   "metadata": {},
   "source": [
    "## üåü Exploring Other Pre-trained Models\n",
    "\n",
    "Let's try different models for different tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-tasks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"Fine-tuning is the process of adapting a pre-trained model to a specific task. It requires less data and time than training from scratch.\"\n",
    "question = \"What is fine-tuning?\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"Question Answering:\")\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {result['answer']} (confidence: {result['score']:.2%})\\n\")\n",
    "\n",
    "# 2. Named Entity Recognition\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(\"\\nNamed Entity Recognition:\")\n",
    "print(f\"Text: {text}\")\n",
    "print(\"Entities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"  - {entity['word']}: {entity['entity_group']} ({entity['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Text Generation (bonus - this is fun!)\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "prompt = \"Fine-tuning machine learning models is\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Text Generation:\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generated: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercise\n",
    "\n",
    "**Challenge:** Create a function that:\n",
    "1. Takes a list of texts\n",
    "2. Classifies each as positive/negative\n",
    "3. Returns only the positive ones with their scores\n",
    "\n",
    "Try it yourself first, then check the solution below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def filter_positive_texts(texts):\n",
    "    \"\"\"\n",
    "    Filter and return only positive texts with their confidence scores.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (text, score)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_texts = [\n",
    "    \"I love learning about AI!\",\n",
    "    \"This is frustrating and difficult.\",\n",
    "    \"Great tutorial, very helpful!\",\n",
    "    \"I don't understand anything.\",\n",
    "    \"Amazing progress today!\"\n",
    "]\n",
    "\n",
    "# positive_texts = filter_positive_texts(test_texts)\n",
    "# print(positive_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (Run this after trying yourself!)\n",
    "def filter_positive_texts(texts):\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    results = classifier(texts)\n",
    "    \n",
    "    positive_texts = []\n",
    "    for text, result in zip(texts, results):\n",
    "        if result['label'] == 'POSITIVE':\n",
    "            positive_texts.append((text, result['score']))\n",
    "    \n",
    "    return positive_texts\n",
    "\n",
    "# Test\n",
    "positive_texts = filter_positive_texts(test_texts)\n",
    "\n",
    "print(\"Positive texts found:\\n\")\n",
    "for text, score in positive_texts:\n",
    "    print(f\"‚úÖ '{text}' (confidence: {score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "Congratulations! You now know how to:\n",
    "- ‚úÖ Load pre-trained models using pipelines\n",
    "- ‚úÖ Understand tokenization (text ‚Üí numbers)\n",
    "- ‚úÖ Run inference manually (step-by-step)\n",
    "- ‚úÖ Process multiple texts in batches\n",
    "- ‚úÖ Use different model types (sentiment, QA, NER, generation)\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Takeaways\n",
    "\n",
    "1. **Pipelines = Easy Mode** (one line of code)\n",
    "2. **Tokenizers convert text to numbers** that models understand\n",
    "3. **Models output logits** ‚Üí softmax ‚Üí probabilities ‚Üí predictions\n",
    "4. **Batching is more efficient** than processing one at a time\n",
    "5. **Different models for different tasks** (classification, QA, NER, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Self-Check\n",
    "\n",
    "Can you answer these?\n",
    "1. What does a tokenizer do?\n",
    "2. What are \"special tokens\" and why do we need them?\n",
    "3. What's the difference between logits and probabilities?\n",
    "4. What's the advantage of using batches?\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Lesson\n",
    "\n",
    "**Lesson 1.3: Understanding Your Data**\n",
    "- Dataset formats and structure\n",
    "- Data preparation and cleaning\n",
    "- Train/validation/test splits\n",
    "- Data quality checks\n",
    "\n",
    "**Ready to learn about data? Let's go! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "**Progress:** üü¢üü¢üîòüîòüîò (Lesson 2 of 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
