{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# üìö Lesson 1.1: What is Fine-Tuning?\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Basic Python knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand what fine-tuning is and why it matters\n",
    "2. Know the difference between training from scratch vs fine-tuning\n",
    "3. Identify when to use fine-tuning\n",
    "4. See real-world examples of fine-tuning applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is-finetuning",
   "metadata": {},
   "source": [
    "## ü§î What is Fine-Tuning?\n",
    "\n",
    "Imagine you're learning to play piano:\n",
    "- **Training from scratch** = Learning music from absolute zero (what are notes? what is rhythm?)\n",
    "- **Fine-tuning** = You already know piano, now learning jazz specifically\n",
    "\n",
    "### In Machine Learning Terms:\n",
    "\n",
    "**Fine-tuning** is taking a model that's already been trained on a large dataset and adapting it to your specific task with a smaller dataset.\n",
    "\n",
    "```\n",
    "Pre-trained Model (knows language)  +  Your Data (specific task)  =  Fine-tuned Model\n",
    "     BERT/GPT/RoBERTa              +  Customer reviews           =  Review classifier\n",
    "```\n",
    "\n",
    "### Why This is Powerful:\n",
    "\n",
    "1. **Less Data Needed** - Instead of millions of examples, you might need only hundreds\n",
    "2. **Faster Training** - Hours instead of weeks\n",
    "3. **Better Results** - Standing on the shoulders of giants\n",
    "4. **Cost Effective** - Much cheaper than training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-comparison",
   "metadata": {},
   "source": [
    "## üìä Training from Scratch vs Fine-Tuning\n",
    "\n",
    "| Aspect | Training from Scratch | Fine-Tuning |\n",
    "|--------|----------------------|-------------|\n",
    "| **Data Required** | Millions of examples | Hundreds to thousands |\n",
    "| **Time** | Days to weeks | Minutes to hours |\n",
    "| **Cost** | $1,000s in compute | $10s to $100s |\n",
    "| **Expertise** | PhD-level ML knowledge | Intermediate Python |\n",
    "| **Starting Point** | Random weights | Pre-trained model |\n",
    "| **Use Case** | New architecture/language | Specific domain task |\n",
    "\n",
    "### Visual Representation:\n",
    "\n",
    "```\n",
    "TRAINING FROM SCRATCH:\n",
    "Random Weights ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí Final Model\n",
    "|_____________Very Long Path_____________|\n",
    "\n",
    "FINE-TUNING:\n",
    "Pre-trained Model ‚Üí ‚Üí Final Model\n",
    "                |_Short Path_|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-finetune",
   "metadata": {},
   "source": [
    "## üéØ When Should You Fine-Tune?\n",
    "\n",
    "### ‚úÖ Fine-tuning is GREAT for:\n",
    "\n",
    "1. **Domain-Specific Tasks**\n",
    "   - Medical text classification\n",
    "   - Legal document analysis\n",
    "   - Financial sentiment analysis\n",
    "\n",
    "2. **Custom Classifications**\n",
    "   - Classifying your company's support tickets\n",
    "   - Detecting spam in your specific context\n",
    "   - Categorizing your product reviews\n",
    "\n",
    "3. **Limited Data Scenarios**\n",
    "   - You have 500-10,000 labeled examples\n",
    "   - Data is expensive to collect\n",
    "   - Privacy constraints limit data\n",
    "\n",
    "4. **Specific Style/Tone**\n",
    "   - Writing in your brand voice\n",
    "   - Following specific formatting rules\n",
    "   - Domain-specific language\n",
    "\n",
    "### ‚ùå Fine-tuning is NOT needed for:\n",
    "\n",
    "1. **Generic Tasks**\n",
    "   - General sentiment analysis (use off-the-shelf models)\n",
    "   - Basic text summarization\n",
    "   - Simple Q&A\n",
    "\n",
    "2. **Very Small Data**\n",
    "   - Less than 100 examples (try few-shot prompting instead)\n",
    "\n",
    "3. **Rapidly Changing Requirements**\n",
    "   - Prompting is more flexible\n",
    "\n",
    "### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have a specific task?\n",
    "    ‚îú‚îÄ No ‚Üí Use general models with prompting\n",
    "    ‚îî‚îÄ Yes ‚Üí Do you have 100+ labeled examples?\n",
    "        ‚îú‚îÄ No ‚Üí Use few-shot prompting or collect more data\n",
    "        ‚îî‚îÄ Yes ‚Üí Do existing models work well?\n",
    "            ‚îú‚îÄ Yes ‚Üí Stick with existing models\n",
    "            ‚îî‚îÄ No ‚Üí FINE-TUNE! ‚ú®\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-world-examples",
   "metadata": {},
   "source": [
    "## üåç Real-World Examples\n",
    "\n",
    "### Example 1: Customer Support Automation\n",
    "\n",
    "**Problem:** A company receives 1,000 support tickets daily across 5 categories  \n",
    "**Solution:** Fine-tune BERT on 2,000 historical tickets  \n",
    "**Result:** 94% accuracy, saves 20 hours/day of manual routing\n",
    "\n",
    "### Example 2: Medical Record Analysis\n",
    "\n",
    "**Problem:** Extract diagnosis codes from doctor's notes  \n",
    "**Solution:** Fine-tune BioBERT on 5,000 annotated notes  \n",
    "**Result:** 89% accuracy, reduces coding time by 60%\n",
    "\n",
    "### Example 3: Content Moderation\n",
    "\n",
    "**Problem:** Detect toxic comments specific to gaming community  \n",
    "**Solution:** Fine-tune RoBERTa on 10,000 labeled comments  \n",
    "**Result:** 92% accuracy, better than generic models (78%)\n",
    "\n",
    "### Example 4: Legal Document Classification\n",
    "\n",
    "**Problem:** Categorize contracts into 15 types  \n",
    "**Solution:** Fine-tune Legal-BERT on 3,000 contracts  \n",
    "**Result:** 96% accuracy, saves paralegals 15 hours/week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-section",
   "metadata": {},
   "source": [
    "## üß™ Interactive Exercise\n",
    "\n",
    "Let's test your understanding! For each scenario, decide:\n",
    "- Should you fine-tune?\n",
    "- Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiz-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first to set up the quiz\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"You have 5,000 customer reviews and need to classify them as positive/negative for your e-commerce site.\",\n",
    "        \"should_finetune\": True,\n",
    "        \"explanation\": \"YES! You have enough data (5,000 examples) and a specific domain (your products). Fine-tuning will give better results than generic sentiment models.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"You want to summarize any Wikipedia article.\",\n",
    "        \"should_finetune\": False,\n",
    "        \"explanation\": \"NO! This is a generic task. Use an existing summarization model like BART or T5. No fine-tuning needed.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"You have 50 examples of emails you want to categorize.\",\n",
    "        \"should_finetune\": False,\n",
    "        \"explanation\": \"NO! 50 examples is too few for effective fine-tuning. Try few-shot prompting with GPT-3.5/4 instead, or collect more data first.\"\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"You need to extract medical entity names (diseases, medications) from clinical notes specific to your hospital.\",\n",
    "        \"should_finetune\": True,\n",
    "        \"explanation\": \"YES! This is domain-specific (medical), task-specific (NER), and likely has terminology unique to your hospital. Fine-tune a medical model like BioBERT.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Quiz ready! Run the cells below to test your knowledge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiz-q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1\n",
    "print(\"Scenario 1:\")\n",
    "print(scenarios[0][\"scenario\"])\n",
    "print(\"\\nShould you fine-tune? (Think about it, then run next cell for answer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiz-a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "display(Markdown(f\"### Answer:\\n{scenarios[0]['explanation']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-concepts",
   "metadata": {},
   "source": [
    "## üîë Key Concepts to Remember\n",
    "\n",
    "1. **Fine-tuning = Adaptation**\n",
    "   - Start with pre-trained knowledge\n",
    "   - Adapt to your specific task\n",
    "   - Much faster and cheaper than training from scratch\n",
    "\n",
    "2. **The Sweet Spot**\n",
    "   - 500-10,000 labeled examples\n",
    "   - Domain-specific tasks\n",
    "   - When generic models aren't good enough\n",
    "\n",
    "3. **Not Always Necessary**\n",
    "   - Try existing models first\n",
    "   - Consider few-shot prompting for small data\n",
    "   - Fine-tune when you need better performance\n",
    "\n",
    "4. **Real Business Value**\n",
    "   - Automation of manual tasks\n",
    "   - Improved accuracy for critical decisions\n",
    "   - Cost savings from efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "Congratulations! You now understand:\n",
    "- ‚úÖ What fine-tuning is (adapting pre-trained models)\n",
    "- ‚úÖ Why it's powerful (less data, faster, better results)\n",
    "- ‚úÖ When to use it (domain-specific tasks with 100+ examples)\n",
    "- ‚úÖ Real-world applications (customer support, medical, legal, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Self-Check Questions\n",
    "\n",
    "Before moving to the next lesson, make sure you can answer:\n",
    "\n",
    "1. What's the main difference between training from scratch and fine-tuning?\n",
    "2. How much data do you typically need for fine-tuning?\n",
    "3. Name three scenarios where fine-tuning is beneficial\n",
    "4. Name one scenario where fine-tuning is NOT the best approach\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Lesson\n",
    "\n",
    "**Lesson 1.2: Your First Model**\n",
    "- Load a pre-trained model\n",
    "- Run inference (no training yet)\n",
    "- Understand tokenizers\n",
    "- See a model in action!\n",
    "\n",
    "**Ready to write some code? Let's go! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "additional-resources",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "- [HuggingFace Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [Transfer Learning in NLP](https://arxiv.org/abs/1801.06146)\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or stuck?** Open an issue in the repo or check the FAQ!\n",
    "\n",
    "**Progress:** üü¢üîòüîòüîòüîò (Lesson 1 of 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
