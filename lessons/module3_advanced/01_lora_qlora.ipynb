{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module3_advanced/01_lora_qlora.ipynb)\n",
    "\n",
    "# ðŸš€ LoRA & QLoRA: Train HUGE Models on Free GPUs!\n",
    "\n",
    "**Duration:** 1.5 hours  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Module 2 complete\n",
    "\n",
    "---\n",
    "\n",
    "## Hey! Ready to Train Models with BILLIONS of Parameters? ðŸ¤¯\n",
    "\n",
    "Okay, so here's the problem:\n",
    "\n",
    "**Big models = Better results... but they need MASSIVE GPUs!**\n",
    "\n",
    "For example:\n",
    "- GPT-2 (small): 117M parameters â†’ Works on free Colab âœ…\n",
    "- BERT (base): 110M parameters â†’ Works on free Colab âœ…\n",
    "- LLaMA 7B: 7,000M parameters â†’ Needs 28GB GPU ðŸ˜±\n",
    "- LLaMA 13B: 13,000M parameters â†’ Needs 52GB GPU ðŸ’¸ðŸ’¸\n",
    "\n",
    "**But what if I told you there's a HACK?**\n",
    "\n",
    "What if you could train a 7B parameter model on a FREE Colab GPU?\n",
    "\n",
    "**That's LoRA!** (Low-Rank Adaptation)\n",
    "\n",
    "And **QLoRA** makes it even BETTER (uses even less memory)!\n",
    "\n",
    "Let me explain this magic trick... ðŸŽ©âœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” The Problem: Full Fine-Tuning is Expensive\n",
    "\n",
    "When you normally fine-tune a model, you update **ALL the parameters**.\n",
    "\n",
    "Imagine a model with 7 billion parameters:\n",
    "- Each parameter is a number (32-bit float = 4 bytes)\n",
    "- 7 billion Ã— 4 bytes = 28GB just to LOAD the model\n",
    "- During training, you need gradients, optimizer states, etc.\n",
    "- **Total: ~100GB of GPU memory!** ðŸ˜±\n",
    "\n",
    "Free Colab GPU? **15GB**. Yeah, not happening.\n",
    "\n",
    "## ðŸ’¡ The Solution: Only Train a Tiny Bit!\n",
    "\n",
    "Here's the genius idea behind LoRA:\n",
    "\n",
    "**Instead of changing ALL 7 billion parameters, what if we only add and train a few MILLION new parameters?**\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "ðŸ  **Full Fine-Tuning:** Renovate the entire house  \n",
    "ðŸ›‹ï¸ **LoRA:** Just add some new furniture!\n",
    "\n",
    "You keep the original model **frozen** (don't change it), and add small \"adapter\" layers that you train.\n",
    "\n",
    "**Result:**\n",
    "- Original model: 7B parameters (frozen, no gradients needed)\n",
    "- LoRA adapters: 4M parameters (trainable)\n",
    "- Memory needed: **WAY less!**\n",
    "\n",
    "And the crazy part? **It works just as well as full fine-tuning!** ðŸ¤¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Quick Theory (I Promise It's Not Boring!)\n",
    "\n",
    "LoRA works by decomposing weight updates into low-rank matrices.\n",
    "\n",
    "**In English:**\n",
    "\n",
    "Instead of updating a huge weight matrix directly:\n",
    "```\n",
    "W (original) â†’ W + Î”W (updated)\n",
    "```\n",
    "\n",
    "LoRA does:\n",
    "```\n",
    "W (frozen) â†’ W + A Ã— B\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `W` = Original weights (frozen, don't train)\n",
    "- `A` = Small matrix (e.g., 768 Ã— 8)\n",
    "- `B` = Small matrix (e.g., 8 Ã— 768)\n",
    "- `A Ã— B` = The update (only 8Ã—768Ã—2 = 12,288 parameters!)\n",
    "\n",
    "**Example:**\n",
    "- Original layer: 768 Ã— 768 = 589,824 parameters\n",
    "- LoRA version: 768 Ã— 8 + 8 Ã— 768 = 12,288 parameters\n",
    "- **That's 98% fewer parameters!** ðŸŽ‰\n",
    "\n",
    "And that \"8\" is called the **rank** - it's a hyperparameter you can tune!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ QLoRA: LoRA on Steroids\n",
    "\n",
    "QLoRA adds one more trick: **Quantization**\n",
    "\n",
    "**Normal model:**\n",
    "- Each number = 32 bits (float32)\n",
    "\n",
    "**Quantized model:**\n",
    "- Each number = 4 bits (int4)\n",
    "- **8x less memory!**\n",
    "\n",
    "**QLoRA = Quantized LoRA**\n",
    "- Load model in 4-bit precision\n",
    "- Add LoRA adapters\n",
    "- Train only the adapters in full precision\n",
    "\n",
    "**Result:**\n",
    "- 7B model in 4-bit: ~3.5GB (instead of 28GB)\n",
    "- LoRA adapters: ~10MB\n",
    "- **Total: Fits in free Colab!** ðŸŽŠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Let's Actually Do It!\n",
    "\n",
    "Time to train a REAL large model on Colab. We'll use a smaller model for this demo (so it runs fast), but the technique works the same for 7B, 13B, or even 70B models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the LoRA library (PEFT = Parameter-Efficient Fine-Tuning)\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Model in 4-bit (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Quantization config (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",             # Use normalized float 4\n",
    "    bnb_4bit_compute_dtype=torch.float16,   # Compute in fp16\n",
    "    bnb_4bit_use_double_quant=True,        # Double quantization for extra compression\n",
    ")\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # We'll use BERT for demo (works with any model!)\n",
    "\n",
    "print(\"ðŸ“¥ Loading model in 4-bit mode...\")\n",
    "print(\"   (This is the QLoRA magic!)\\n\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically use GPU if available\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"âœ… Model loaded!\")\n",
    "print(f\"\\nðŸ’¾ Check GPU memory:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"   Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add LoRA Adapters\n",
    "\n",
    "Now we add the trainable adapter layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Rank (higher = more parameters but better accuracy)\n",
    "    lora_alpha=16,                    # Scaling factor (usually 2Ã—r)\n",
    "    target_modules=[\"query\", \"value\"], # Which layers to add LoRA to\n",
    "    lora_dropout=0.1,                 # Dropout for regularization\n",
    "    bias=\"none\",                      # Don't train bias parameters\n",
    "    task_type=TaskType.SEQ_CLS,      # Sequence classification task\n",
    ")\n",
    "\n",
    "print(\"ðŸ”§ Adding LoRA adapters...\\n\")\n",
    "\n",
    "# Wrap model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# See the magic!\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nâœ¨ Look at that! We're only training a TINY fraction of the parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Understanding the LoRA Config\n",
    "\n",
    "Let me break down what we just did:\n",
    "\n",
    "### **r (rank) = 8**\n",
    "- This is the bottleneck dimension\n",
    "- Higher = more parameters = better accuracy (but more memory)\n",
    "- Typical values: 4, 8, 16, 32\n",
    "- Start with 8, increase if needed\n",
    "\n",
    "### **lora_alpha = 16**\n",
    "- Scaling factor for the LoRA updates\n",
    "- Rule of thumb: 2 Ã— r\n",
    "- Affects how much the adapters influence the output\n",
    "\n",
    "### **target_modules = [\"query\", \"value\"]**\n",
    "- Which attention layers to add LoRA to\n",
    "- Common choices:\n",
    "  - `[\"query\", \"value\"]` - Minimal (fastest)\n",
    "  - `[\"query\", \"key\", \"value\"]` - Balanced\n",
    "  - `[\"query\", \"key\", \"value\", \"output\"]` - Maximum (slowest)\n",
    "\n",
    "### **lora_dropout = 0.1**\n",
    "- Regularization to prevent overfitting\n",
    "- 0.05 to 0.1 is typical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data (Same as Before!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Load dataset\n",
    "print(\"ðŸ“¥ Loading dataset...\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Use subset for quick demo\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = small_train.map(tokenize_function, batched=True)\n",
    "test_dataset = small_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"âœ… Data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train! (It's FAST!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (optimized for LoRA)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./lora_results',\n",
    "    \n",
    "    # LoRA can use HIGHER learning rates!\n",
    "    learning_rate=1e-4,  # 10x higher than normal fine-tuning!\n",
    "    \n",
    "    # Batch sizes (can be larger with LoRA)\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    # Epochs\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Save space\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting LoRA training...\\n\")\n",
    "print(\"Notice how FAST this is compared to full fine-tuning!\\n\")\n",
    "\n",
    "# Train!\n",
    "result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"\\nâ±ï¸ Time: {result.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"ðŸŽ¯ Final loss: {result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate & Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"ðŸ“Š EVALUATION RESULTS\\n\" + \"=\"*50)\n",
    "print(f\"\\nðŸŽ¯ Accuracy: {eval_results['eval_accuracy']:.3f}\")\n",
    "print(f\"ðŸ“‰ Loss: {eval_results['eval_loss']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Fun Fact:\")\n",
    "print(\"   This accuracy is usually JUST AS GOOD as full fine-tuning!\")\n",
    "print(\"   But we trained 100x fewer parameters! ðŸ¤¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Saving Your LoRA Model\n",
    "\n",
    "Here's the BEST part: LoRA adapters are TINY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save LoRA adapters\n",
    "output_dir = \"./my_lora_model\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to {output_dir}\\n\")\n",
    "\n",
    "# Check file sizes\n",
    "print(\"ðŸ“¦ File sizes:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, file)\n",
    "    if os.path.isfile(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"   {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nðŸ¤¯ The adapter_model.bin is TINY!\")\n",
    "print(\"   Full model would be 400+ MB\")\n",
    "print(\"   LoRA adapters are just a few MB!\")\n",
    "print(\"\\n   You can share this on GitHub, email it, whatever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Loading Your LoRA Model Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# To load later:\n",
    "print(\"ðŸ“¥ How to load your LoRA model:\\n\")\n",
    "print(\"```python\")\n",
    "print(\"# 1. Load base model\")\n",
    "print(\"base_model = AutoModelForSequenceClassification.from_pretrained(\")\n",
    "print(\"    'bert-base-uncased',\")\n",
    "print(\"    quantization_config=bnb_config\")\n",
    "print(\")\")\n",
    "print(\"\")\n",
    "print(\"# 2. Load LoRA adapters\")\n",
    "print(\"model = PeftModel.from_pretrained(base_model, './my_lora_model')\")\n",
    "print(\"\")\n",
    "print(\"# 3. Use it!\")\n",
    "print(\"predictions = model(inputs)\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nâœ¨ That's it! Super simple!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ LoRA vs Full Fine-Tuning Comparison\n",
    "\n",
    "Let me show you the differences:\n",
    "\n",
    "| Aspect | Full Fine-Tuning | LoRA | QLoRA |\n",
    "|--------|------------------|------|-------|\n",
    "| **Trainable Params** | 100% (110M) | ~1% (1M) | ~1% (1M) |\n",
    "| **GPU Memory** | ~40GB | ~15GB | ~8GB |\n",
    "| **Training Speed** | Baseline | 1.5x faster | 1.2x faster |\n",
    "| **Saved Model Size** | 440MB | 4MB | 4MB |\n",
    "| **Accuracy** | 100% | 98-100% | 97-100% |\n",
    "| **Learning Rate** | 3e-5 | 1e-4 (higher!) | 1e-4 |\n",
    "| **Best For** | Small models | Large models | HUGE models |\n",
    "\n",
    "**TL;DR:**\n",
    "- LoRA/QLoRA give you 97-100% of the accuracy\n",
    "- With 1% of the trainable parameters\n",
    "- Using way less memory\n",
    "- Training faster\n",
    "- Producing tiny model files\n",
    "\n",
    "**It's basically magic.** âœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Hyperparameter Tuning for LoRA\n",
    "\n",
    "Here's what to adjust:\n",
    "\n",
    "### **Rank (r)**\n",
    "```python\n",
    "r=4   # Minimal - fastest, smallest, might lose some accuracy\n",
    "r=8   # Sweet spot - balanced\n",
    "r=16  # More capacity - slower, bigger, sometimes better\n",
    "r=32  # Overkill for most tasks\n",
    "```\n",
    "\n",
    "**Rule:** Start with 8. Increase if accuracy is too low.\n",
    "\n",
    "### **Alpha (lora_alpha)**\n",
    "```python\n",
    "lora_alpha = 2 * r  # Common rule of thumb\n",
    "```\n",
    "\n",
    "### **Target Modules**\n",
    "```python\n",
    "# Minimal (fastest)\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Balanced\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "# Maximum (most trainable params)\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "```\n",
    "\n",
    "**Note:** Module names change per model architecture. Check model config!\n",
    "\n",
    "### **Learning Rate**\n",
    "```python\n",
    "# LoRA can handle HIGHER learning rates!\n",
    "learning_rate = 1e-4  # vs 3e-5 for full fine-tuning\n",
    "learning_rate = 2e-4  # Sometimes even this works!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Real-World LoRA Applications\n",
    "\n",
    "People are using LoRA to train:\n",
    "\n",
    "### **1. LLaMA/Mistral Models (7B-70B parameters)**\n",
    "```python\n",
    "# This would need 280GB without LoRA!\n",
    "# With QLoRA? Fits in 24GB!\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-70b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "### **2. Multiple LoRA Adapters for Different Tasks**\n",
    "```python\n",
    "# Load base model once\n",
    "base_model = load_base_model()\n",
    "\n",
    "# Switch between tasks by swapping adapters!\n",
    "sentiment_model = PeftModel.from_pretrained(base_model, './sentiment_lora')\n",
    "summary_model = PeftModel.from_pretrained(base_model, './summary_lora')\n",
    "qa_model = PeftModel.from_pretrained(base_model, './qa_lora')\n",
    "```\n",
    "\n",
    "### **3. Personalized AI Assistants**\n",
    "- Train one LoRA adapter per user\n",
    "- Each adapter is tiny (4-10MB)\n",
    "- Share the base model, swap adapters per user\n",
    "- Suddenly you can serve THOUSANDS of personalized models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "1. **LoRA = Low-Rank Adaptation**\n",
    "   - Only train a tiny fraction of parameters\n",
    "   - Get 98%+ of full fine-tuning performance\n",
    "   - Use way less memory\n",
    "\n",
    "2. **QLoRA = Quantized LoRA**\n",
    "   - Load base model in 4-bit\n",
    "   - Use even LESS memory\n",
    "   - Train huge models on free GPUs!\n",
    "\n",
    "3. **When to Use LoRA:**\n",
    "   - Training models with >1B parameters\n",
    "   - Limited GPU memory\n",
    "   - Need to save/share small model files\n",
    "   - Want faster training\n",
    "   - Multiple tasks with same base model\n",
    "\n",
    "4. **When to Use Full Fine-Tuning:**\n",
    "   - Small models (<500M parameters)\n",
    "   - You have lots of GPU memory\n",
    "   - Need absolute maximum accuracy\n",
    "   - Simple single-task use case\n",
    "\n",
    "5. **LoRA Hyperparameters:**\n",
    "   - `r=8`: Good default rank\n",
    "   - `lora_alpha=16`: Usually 2Ã—r\n",
    "   - `target_modules`: Start with [\"q\", \"v\"]\n",
    "   - `learning_rate=1e-4`: Higher than normal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ You're Now a LoRA Expert!\n",
    "\n",
    "You just learned the technique that's revolutionizing AI fine-tuning!\n",
    "\n",
    "**This is how people are fine-tuning:**\n",
    "- LLaMA 2 (7B-70B)\n",
    "- Mistral (7B)\n",
    "- Falcon (40B)\n",
    "- All the latest open-source LLMs!\n",
    "\n",
    "**On consumer GPUs!** Even free ones!\n",
    "\n",
    "This is a game-changer. You can now compete with big tech companies using just a free Colab notebook.\n",
    "\n",
    "Pretty awesome, right? ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**Next up:** Multi-Task Learning - Train one model for multiple jobs! ðŸŽ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Resources & Further Reading\n",
    "\n",
    "**Papers:**\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "**Libraries:**\n",
    "- [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)\n",
    "- [bitsandbytes (Quantization)](https://github.com/TimDettmers/bitsandbytes)\n",
    "\n",
    "**Community:**\n",
    "- [HuggingFace Forums - PEFT](https://discuss.huggingface.co/c/peft/)\n",
    "- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Lots of LoRA discussions!\n",
    "\n",
    "Now go train some HUGE models! ðŸ’ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
