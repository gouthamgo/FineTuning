{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module3_advanced/03_custom_loss_functions.ipynb)\n",
    "\n",
    "# üé≤ Custom Loss Functions: Get Creative!\n",
    "\n",
    "**Duration:** 1 hour  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Module 2 complete\n",
    "\n",
    "---\n",
    "\n",
    "## When Standard Loss Functions Aren't Enough üéØ\n",
    "\n",
    "Most of the time, `CrossEntropyLoss` works great. But sometimes you need something special:\n",
    "\n",
    "**Situations where custom loss functions shine:**\n",
    "- üìä **Class imbalance** - Some classes way more common than others\n",
    "- üéØ **Custom business metrics** - Optimize for what actually matters\n",
    "- ü§ù **Multi-objective learning** - Balance multiple goals\n",
    "- üí∞ **Cost-sensitive prediction** - Some errors cost more than others\n",
    "- üé™ **Ranking/Ordering tasks** - Not just classification\n",
    "\n",
    "Today we'll build custom loss functions from scratch and see when to use them!\n",
    "\n",
    "Let's get creative! üé®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Understanding Loss Functions\n",
    "\n",
    "### What IS a loss function?\n",
    "\n",
    "It's how we tell the model \"you're doing this wrong, fix it!\"\n",
    "\n",
    "**Simple analogy:**\n",
    "- Model makes prediction\n",
    "- Loss function compares to true answer\n",
    "- Returns a number (higher = worse)\n",
    "- Model adjusts to make that number smaller\n",
    "\n",
    "**Example - CrossEntropyLoss:**\n",
    "```python\n",
    "# True label: class 1 (positive)\n",
    "# Model predicts: [0.2, 0.8] (80% confident it's class 1)\n",
    "# Loss: low (good prediction!)\n",
    "\n",
    "# Model predicts: [0.9, 0.1] (90% confident it's class 0)\n",
    "# Loss: high (bad prediction, needs fixing!)\n",
    "```\n",
    "\n",
    "Now let's build our own! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Custom Loss #1: Weighted Cross-Entropy\n",
    "\n",
    "**Problem:** You have 1000 negative examples and only 100 positive ones.\n",
    "\n",
    "Standard loss treats them equally ‚Üí Model learns to just predict \"negative\" always!\n",
    "\n",
    "**Solution:** Give more weight to the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Custom loss that weights classes differently\"\"\"\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: model logits [batch_size, num_classes]\n",
    "        targets: true labels [batch_size]\n",
    "        \"\"\"\n",
    "        # Move weights to same device as predictions\n",
    "        weights = self.weights.to(predictions.device)\n",
    "        \n",
    "        # Use PyTorch's cross entropy with weights\n",
    "        return F.cross_entropy(predictions, targets, weight=weights)\n",
    "\n",
    "# Example usage\n",
    "print(\"üéØ Weighted Cross-Entropy Example\\n\")\n",
    "\n",
    "# Simulate imbalanced data: 90% class 0, 10% class 1\n",
    "# We want to give class 1 more importance\n",
    "class_weights = [1.0, 9.0]  # Class 1 is 9x more important\n",
    "\n",
    "# Create loss function\n",
    "weighted_loss = WeightedCrossEntropyLoss(class_weights)\n",
    "standard_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test predictions\n",
    "predictions = torch.tensor([[2.0, -1.0], [1.0, 3.0]])  # 2 examples\n",
    "targets = torch.tensor([0, 1])  # True labels\n",
    "\n",
    "print(f\"Standard loss: {standard_loss(predictions, targets):.4f}\")\n",
    "print(f\"Weighted loss: {weighted_loss(predictions, targets):.4f}\")\n",
    "print(\"\\nüí° Weighted loss is higher because we care more about class 1 errors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Custom Loss #2: Focal Loss\n",
    "\n",
    "**Problem:** Model is too confident on easy examples, ignores hard ones\n",
    "\n",
    "**Solution:** Focal Loss - reduces loss for well-classified examples, focuses on hard cases\n",
    "\n",
    "**When to use:** Imbalanced classes + want model to focus on hard examples\n",
    "\n",
    "This is what **Facebook AI Research used for object detection!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss - focuses on hard examples\n",
    "    \n",
    "    Paper: https://arxiv.org/abs/1708.02002\n",
    "    Used by: Facebook AI Research for object detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weighting factor\n",
    "        self.gamma = gamma  # Focusing parameter (higher = more focus on hard examples)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Get probabilities\n",
    "        probs = F.softmax(predictions, dim=-1)\n",
    "        \n",
    "        # Get probability of correct class\n",
    "        batch_size = targets.size(0)\n",
    "        correct_class_probs = probs[range(batch_size), targets]\n",
    "        \n",
    "        # Focal weight: (1 - p)^gamma\n",
    "        # If p is high (easy example), weight is low\n",
    "        # If p is low (hard example), weight is high\n",
    "        focal_weight = (1 - correct_class_probs) ** self.gamma\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce_loss = F.cross_entropy(predictions, targets, reduction='none')\n",
    "        \n",
    "        # Apply focal weight\n",
    "        focal_loss = self.alpha * focal_weight * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Demonstrate the difference\n",
    "print(\"üî• Focal Loss vs Standard Loss\\n\")\n",
    "\n",
    "focal_loss = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Easy example: model is very confident and correct\n",
    "easy_pred = torch.tensor([[10.0, -5.0]])  # Very confident about class 0\n",
    "easy_target = torch.tensor([0])\n",
    "\n",
    "print(\"Easy example (model confident & correct):\")\n",
    "print(f\"  Standard loss: {ce_loss(easy_pred, easy_target):.4f}\")\n",
    "print(f\"  Focal loss: {focal_loss(easy_pred, easy_target):.4f}\")\n",
    "print(\"  ‚Üí Focal loss is MUCH lower (focuses less on easy examples)\\n\")\n",
    "\n",
    "# Hard example: model is uncertain\n",
    "hard_pred = torch.tensor([[0.5, 0.4]])  # Uncertain between classes\n",
    "hard_target = torch.tensor([0])\n",
    "\n",
    "print(\"Hard example (model uncertain):\")\n",
    "print(f\"  Standard loss: {ce_loss(hard_pred, hard_target):.4f}\")\n",
    "print(f\"  Focal loss: {focal_loss(hard_pred, hard_target):.4f}\")\n",
    "print(\"  ‚Üí Focal loss is higher (focuses more on hard examples)\\n\")\n",
    "\n",
    "print(\"üí° This is how Focal Loss helps with imbalanced data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Custom Loss #3: Cost-Sensitive Loss\n",
    "\n",
    "**Problem:** Not all errors are equal!\n",
    "\n",
    "**Example:** Medical diagnosis\n",
    "- False Negative (missing cancer) = VERY BAD ($1M+ cost, could be fatal)\n",
    "- False Positive (extra test) = Not great but okay ($100 cost)\n",
    "\n",
    "**Solution:** Custom loss that penalizes expensive errors more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostSensitiveLoss(nn.Module):\n",
    "    \"\"\"Loss function that considers cost of different errors\"\"\"\n",
    "    \n",
    "    def __init__(self, cost_matrix):\n",
    "        \"\"\"\n",
    "        cost_matrix: [num_classes, num_classes]\n",
    "        cost_matrix[i][j] = cost of predicting j when true label is i\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_matrix = torch.tensor(cost_matrix, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        batch_size, num_classes = predictions.shape\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(predictions, dim=-1)\n",
    "        \n",
    "        # Move cost matrix to same device\n",
    "        cost_matrix = self.cost_matrix.to(predictions.device)\n",
    "        \n",
    "        # Calculate expected cost for each example\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            true_class = targets[i]\n",
    "            # Expected cost = sum of (probability of predicting j) * (cost of predicting j)\n",
    "            expected_cost = (probs[i] * cost_matrix[true_class]).sum()\n",
    "            losses.append(expected_cost)\n",
    "        \n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "# Medical example\n",
    "print(\"üíä Medical Diagnosis Example\\n\")\n",
    "\n",
    "# Cost matrix for binary classification (healthy vs sick)\n",
    "# Rows = true label, Columns = predicted label\n",
    "cost_matrix = [\n",
    "    [0,    100],   # True=Healthy, Pred=Healthy (0 cost) or Pred=Sick (100 cost - unnecessary treatment)\n",
    "    [10000, 0]     # True=Sick, Pred=Healthy (10000 cost - missed diagnosis!) or Pred=Sick (0 cost)\n",
    "]\n",
    "\n",
    "cost_loss = CostSensitiveLoss(cost_matrix)\n",
    "standard_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Case 1: Model predicts healthy for a sick patient (VERY BAD!)\n",
    "pred_healthy = torch.tensor([[2.0, -2.0]])  # Confident about healthy\n",
    "true_sick = torch.tensor([1])  # Actually sick\n",
    "\n",
    "print(\"Case 1: Predicting healthy for sick patient\")\n",
    "print(f\"  Standard loss: {standard_loss(pred_healthy, true_sick):.4f}\")\n",
    "print(f\"  Cost-sensitive loss: {cost_loss(pred_healthy, true_sick):.4f}\")\n",
    "print(\"  ‚Üí Cost-sensitive loss is MUCH higher (this error is expensive!)\\n\")\n",
    "\n",
    "# Case 2: Model predicts sick for a healthy patient (not great but okay)\n",
    "pred_sick = torch.tensor([[-2.0, 2.0]])  # Confident about sick\n",
    "true_healthy = torch.tensor([0])  # Actually healthy\n",
    "\n",
    "print(\"Case 2: Predicting sick for healthy patient\")\n",
    "print(f\"  Standard loss: {standard_loss(pred_sick, true_healthy):.4f}\")\n",
    "print(f\"  Cost-sensitive loss: {cost_loss(pred_sick, true_healthy):.4f}\")\n",
    "print(\"  ‚Üí Cost-sensitive loss is lower (this error is less expensive)\\n\")\n",
    "\n",
    "print(\"üí° Model learns to be more conservative - better safe than sorry!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé™ Custom Loss #4: Ranking Loss (Triplet Loss)\n",
    "\n",
    "**Use case:** You want to learn similarity, not just classify\n",
    "\n",
    "**Examples:**\n",
    "- Face recognition (same person = similar, different person = dissimilar)\n",
    "- Search engines (relevant docs = similar to query)\n",
    "- Recommendation systems\n",
    "\n",
    "**How it works:**\n",
    "- Anchor: your reference point\n",
    "- Positive: similar example\n",
    "- Negative: dissimilar example\n",
    "\n",
    "Goal: Make anchor closer to positive than to negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet Loss for learning embeddings\n",
    "    \n",
    "    Used in: Face recognition, semantic search, recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin  # How much closer should positive be?\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        anchor: embeddings of anchor examples [batch_size, embedding_dim]\n",
    "        positive: embeddings of similar examples [batch_size, embedding_dim]\n",
    "        negative: embeddings of dissimilar examples [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Distance between anchor and positive (should be small)\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        \n",
    "        # Distance between anchor and negative (should be large)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # Loss: we want pos_dist < neg_dist - margin\n",
    "        # If already satisfied, loss = 0\n",
    "        # If not, loss = how much we need to improve\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Example: Learning document similarity\n",
    "print(\"üìö Document Similarity Example\\n\")\n",
    "\n",
    "triplet_loss = TripletLoss(margin=1.0)\n",
    "\n",
    "# Simulate embeddings (in reality, these come from your model)\n",
    "anchor = torch.randn(4, 128)  # 4 anchor documents, 128-dim embeddings\n",
    "positive = anchor + torch.randn(4, 128) * 0.1  # Similar docs (close to anchor)\n",
    "negative = torch.randn(4, 128)  # Dissimilar docs (random)\n",
    "\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(f\"Triplet loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualize the concept\n",
    "print(\"\\nüìä Visual concept:\")\n",
    "print(\"\\n  Anchor    Positive    Negative\")\n",
    "print(\"    ‚Ä¢  ----  ‚Ä¢             \")\n",
    "print(\"          (close)          \")\n",
    "print(\"    ‚Ä¢  --------------------  ‚Ä¢\")\n",
    "print(\"              (far)          \")\n",
    "print(\"\\nüí° Goal: Keep positive close, push negative far!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Custom Loss #5: Multi-Task Loss\n",
    "\n",
    "**Use case:** Training on multiple tasks simultaneously\n",
    "\n",
    "**Challenge:** Different tasks have different loss scales!\n",
    "\n",
    "**Solution:** Combine losses with learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"Automatically balance multiple task losses\n",
    "    \n",
    "    Based on: \"Multi-Task Learning Using Uncertainty to Weigh Losses\"\n",
    "    Paper: https://arxiv.org/abs/1705.07115\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_tasks):\n",
    "        super().__init__()\n",
    "        # Learnable log variance for each task\n",
    "        self.log_vars = nn.Parameter(torch.zeros(num_tasks))\n",
    "    \n",
    "    def forward(self, losses):\n",
    "        \"\"\"\n",
    "        losses: list of losses for each task\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, loss in enumerate(losses):\n",
    "            # Automatic weighting based on learned uncertainty\n",
    "            precision = torch.exp(-self.log_vars[i])\n",
    "            total_loss += precision * loss + self.log_vars[i]\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "# Example with 3 tasks\n",
    "print(\"üéØ Multi-Task Loss Example\\n\")\n",
    "\n",
    "mtl_loss = MultiTaskLoss(num_tasks=3)\n",
    "\n",
    "# Simulate losses from 3 different tasks\n",
    "task1_loss = torch.tensor(0.5)  # Sentiment analysis\n",
    "task2_loss = torch.tensor(2.0)  # Topic classification\n",
    "task3_loss = torch.tensor(0.1)  # Length prediction\n",
    "\n",
    "combined_loss = mtl_loss([task1_loss, task2_loss, task3_loss])\n",
    "\n",
    "print(f\"Task 1 loss: {task1_loss:.4f}\")\n",
    "print(f\"Task 2 loss: {task2_loss:.4f}\")\n",
    "print(f\"Task 3 loss: {task3_loss:.4f}\")\n",
    "print(f\"\\nCombined loss: {combined_loss:.4f}\")\n",
    "print(f\"\\nLearned weights: {torch.exp(-mtl_loss.log_vars).detach()}\")\n",
    "print(\"\\nüí° Model automatically learns how to balance tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è How to Use Custom Loss in Training\n",
    "\n",
    "Super easy with HuggingFace Trainer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class CustomLossTrainer(Trainer):\n",
    "    \"\"\"Trainer with custom loss function\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, custom_loss_fn=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_loss_fn = custom_loss_fn\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get model outputs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Use custom loss if provided\n",
    "        if self.custom_loss_fn:\n",
    "            loss = self.custom_loss_fn(logits, labels)\n",
    "        else:\n",
    "            # Use default loss\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nüìù How to use in practice:\\n\")\n",
    "print(\"\"\"\n",
    "# Create your custom loss\n",
    "focal_loss = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "\n",
    "# Use it in training\n",
    "trainer = CustomLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    custom_loss_fn=focal_loss  # ‚Üê Your custom loss!\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ That's it! Model now trains with your custom loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì When to Use Which Loss?\n",
    "\n",
    "| Problem | Loss Function | Why? |\n",
    "|---------|--------------|------|\n",
    "| **Imbalanced classes** | Weighted CE or Focal Loss | Prevents model from ignoring minority class |\n",
    "| **Hard negative mining** | Focal Loss | Focuses on difficult examples |\n",
    "| **Cost-sensitive errors** | Custom Cost Matrix | Different errors have different impacts |\n",
    "| **Learning similarities** | Triplet Loss | For embeddings/search/recommendations |\n",
    "| **Multiple tasks** | Multi-Task Loss | Automatically balances task importance |\n",
    "| **Ranking problems** | Ranking Loss | When order matters more than class |\n",
    "| **Outlier robustness** | Huber Loss | Combines MSE + MAE benefits |\n",
    "\n",
    "**Default choice:** CrossEntropyLoss (it works 90% of the time!)\n",
    "\n",
    "**When to customize:**\n",
    "- You have a specific business need (e.g., false negatives are very expensive)\n",
    "- Standard loss isn't working well\n",
    "- You're doing something unique (similarity learning, multi-task, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Pro Tips\n",
    "\n",
    "### 1. **Start Simple**\n",
    "Always try standard loss first. Only customize if you have a good reason.\n",
    "\n",
    "### 2. **Monitor Multiple Metrics**\n",
    "```python\n",
    "# Don't just look at loss!\n",
    "metrics = {\n",
    "    'loss': loss.item(),\n",
    "    'accuracy': accuracy,\n",
    "    'f1_score': f1,\n",
    "    'per_class_accuracy': per_class_acc\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Validate on Business Metrics**\n",
    "```python\n",
    "# Example: Medical diagnosis\n",
    "false_negatives = count_fn(predictions, labels)\n",
    "if false_negatives > threshold:\n",
    "    # Increase weight on that class\n",
    "```\n",
    "\n",
    "### 4. **Experiment with Hyperparameters**\n",
    "```python\n",
    "# Try different values\n",
    "for gamma in [0.5, 1.0, 2.0, 5.0]:\n",
    "    loss_fn = FocalLoss(gamma=gamma)\n",
    "    # Train and compare\n",
    "```\n",
    "\n",
    "### 5. **Combine Losses**\n",
    "```python\n",
    "# Sometimes you want multiple objectives\n",
    "total_loss = 0.7 * classification_loss + 0.3 * regularization_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ You're Now a Loss Function Expert!\n",
    "\n",
    "You learned:\n",
    "- ‚úÖ How loss functions work\n",
    "- ‚úÖ 5 powerful custom losses (Weighted, Focal, Cost-Sensitive, Triplet, Multi-Task)\n",
    "- ‚úÖ When to use each one\n",
    "- ‚úÖ How to implement them in PyTorch\n",
    "- ‚úÖ How to use them with HuggingFace Trainer\n",
    "\n",
    "**Real-world applications:**\n",
    "- Medical diagnosis (cost-sensitive)\n",
    "- Fraud detection (focal loss for rare events)\n",
    "- Face recognition (triplet loss)\n",
    "- Multi-task NLP (multi-task loss)\n",
    "\n",
    "**Interview question you can now answer:**\n",
    "\n",
    "Q: \"Have you ever used custom loss functions?\"\n",
    "\n",
    "A: \"Yes! In my customer support bot project, I used weighted cross-entropy because we had imbalanced categories - 80% billing questions vs 20% technical. By weighting the technical class higher (weight=4.0), I improved F1 score on technical questions from 0.65 to 0.82 while maintaining overall accuracy.\"\n",
    "\n",
    "**That's the kind of answer that gets you hired!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Next up:** Real-world projects where you'll use these techniques! üíº"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Further Reading\n",
    "\n",
    "**Papers:**\n",
    "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "- [Multi-Task Learning Using Uncertainty to Weigh Losses](https://arxiv.org/abs/1705.07115)\n",
    "- [FaceNet: A Unified Embedding for Face Recognition](https://arxiv.org/abs/1503.03832) (Triplet Loss)\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- [Papers With Code - Loss Functions](https://paperswithcode.com/methods/category/loss-functions)\n",
    "\n",
    "Now go optimize those losses! üí™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
