{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module3_advanced/02_multitask_learning.ipynb)\n",
    "\n",
    "# ðŸŽ¨ Multi-Task Learning: One Model, Many Jobs\n",
    "\n",
    "**Duration:** 1.5 hours  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Module 2 complete\n",
    "\n",
    "---\n",
    "\n",
    "## Hey! Want Your Model to Do Multiple Things? ðŸ¤¹\n",
    "\n",
    "Imagine this:\n",
    "\n",
    "**Right now, you probably think:**\n",
    "- One model = One task\n",
    "- Sentiment analysis model\n",
    "- Summarization model\n",
    "- Q&A model\n",
    "\n",
    "**But what if ONE model could:**\n",
    "- Analyze sentiment âœ…\n",
    "- AND summarize text âœ…\n",
    "- AND answer questions âœ…\n",
    "- AND classify topics âœ…\n",
    "\n",
    "**All at the same time?**\n",
    "\n",
    "That's **Multi-Task Learning (MTL)**!\n",
    "\n",
    "Think of it like a Swiss Army knife ðŸ”ªðŸ”§âœ‚ï¸ - one tool, many functions!\n",
    "\n",
    "Let me show you how to build one... ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” Why Multi-Task Learning?\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "### 1. **Efficiency** âš¡\n",
    "- Train ONE model instead of 5 separate models\n",
    "- Save training time\n",
    "- Save GPU costs\n",
    "\n",
    "### 2. **Better Performance** ðŸ“ˆ\n",
    "- Tasks help each other learn!\n",
    "- Sentiment analysis helps with topic classification\n",
    "- Q&A helps with summarization\n",
    "- **Shared knowledge = Better results**\n",
    "\n",
    "### 3. **Less Data Needed** ðŸ’¾\n",
    "- Each task's data helps the others\n",
    "- Got lots of sentiment data but little summarization data?\n",
    "- Perfect! The sentiment task helps the summarization task learn!\n",
    "\n",
    "### 4. **Deployment** ðŸš€\n",
    "- Deploy ONE model instead of multiple\n",
    "- Less memory in production\n",
    "- Simpler architecture\n",
    "\n",
    "**Real Example:**\n",
    "Google's BERT was trained on multiple tasks:\n",
    "- Masked language modeling\n",
    "- Next sentence prediction\n",
    "\n",
    "That's why it works so well for everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ How Multi-Task Learning Works\n",
    "\n",
    "Simple concept:\n",
    "\n",
    "**Regular Training:**\n",
    "```\n",
    "Input â†’ Shared Encoder â†’ Task A Head â†’ Output A\n",
    "```\n",
    "\n",
    "**Multi-Task Training:**\n",
    "```\n",
    "                    â”Œâ†’ Task A Head â†’ Output A\n",
    "                    â”‚\n",
    "Input â†’ Shared Encoder â”œâ†’ Task B Head â†’ Output B\n",
    "                    â”‚\n",
    "                    â””â†’ Task C Head â†’ Output C\n",
    "```\n",
    "\n",
    "**The magic:**\n",
    "- **Shared Encoder** learns features that help ALL tasks\n",
    "- **Task-specific Heads** specialize for each task\n",
    "- During training, we alternate between tasks\n",
    "\n",
    "**Like learning multiple languages:**\n",
    "- Grammar rules are shared (encoder)\n",
    "- Vocabulary is specific (heads)\n",
    "- Learning Spanish helps you learn Italian!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Let's Build a Multi-Task Model!\n",
    "\n",
    "We'll create a model that does:\n",
    "1. **Sentiment Analysis** (positive/negative)\n",
    "2. **Topic Classification** (tech/sports/politics/entertainment)\n",
    "3. **Text Length Prediction** (short/medium/long)\n",
    "\n",
    "Three tasks, one model! Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch accelerate evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Our Multi-Task Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\"One model, multiple tasks!\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_tasks_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared encoder - this learns features for ALL tasks\n",
    "        print(\"ðŸ§  Creating shared encoder...\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific heads\n",
    "        print(\"ðŸŽ¯ Creating task-specific heads...\")\n",
    "        self.task_heads = nn.ModuleDict()\n",
    "        \n",
    "        for task_name, num_labels in num_tasks_config.items():\n",
    "            # Each task gets its own classification head\n",
    "            self.task_heads[task_name] = nn.Sequential(\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(self.hidden_size, num_labels)\n",
    "            )\n",
    "            print(f\"   âœ… {task_name}: {num_labels} classes\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task_name):\n",
    "        # Get shared representations\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Route to appropriate task head\n",
    "        logits = self.task_heads[task_name](pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Define our tasks\n",
    "tasks_config = {\n",
    "    'sentiment': 2,      # positive/negative\n",
    "    'topic': 4,          # tech/sports/politics/entertainment\n",
    "    'length': 3,         # short/medium/long\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ—ï¸ Building multi-task model...\\n\")\n",
    "model = MultiTaskModel('distilbert-base-uncased', tasks_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "print(\"\\nâœ… Model ready!\")\n",
    "print(f\"\\nðŸ“Š Model size: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Multi-Task Dataset\n",
    "\n",
    "We need data for all three tasks. Let's create synthetic data for demo purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Create synthetic dataset\n",
    "def create_multitask_dataset(num_samples=1000):\n",
    "    \"\"\"Generate synthetic data for our three tasks\"\"\"\n",
    "    \n",
    "    topics = ['tech', 'sports', 'politics', 'entertainment']\n",
    "    \n",
    "    # Templates for different topics and sentiments\n",
    "    templates = {\n",
    "        'tech': {\n",
    "            'positive': [\n",
    "                \"This new smartphone is absolutely amazing! The features are incredible.\",\n",
    "                \"I love this software update. It makes everything so much faster!\",\n",
    "                \"Best laptop I've ever owned. The performance is outstanding.\"\n",
    "            ],\n",
    "            'negative': [\n",
    "                \"This app crashes constantly. Very frustrating experience.\",\n",
    "                \"Terrible battery life on this device. Completely disappointing.\",\n",
    "                \"The new update broke everything. This is unacceptable.\"\n",
    "            ]\n",
    "        },\n",
    "        'sports': {\n",
    "            'positive': [\n",
    "                \"What an incredible game! The team played brilliantly.\",\n",
    "                \"Amazing performance by the athletes. They deserved this win!\",\n",
    "                \"Best match of the season. Absolutely thrilling to watch.\"\n",
    "            ],\n",
    "            'negative': [\n",
    "                \"Terrible performance today. The team looked completely lost.\",\n",
    "                \"Very disappointing game. They need to improve significantly.\",\n",
    "                \"Worst match I've seen. Completely unorganized play.\"\n",
    "            ]\n",
    "        },\n",
    "        'politics': {\n",
    "            'positive': [\n",
    "                \"Great policy announcement today. This will help many people.\",\n",
    "                \"Excellent speech by the leader. Very inspiring and hopeful.\",\n",
    "                \"Important reforms being implemented. Positive changes ahead.\"\n",
    "            ],\n",
    "            'negative': [\n",
    "                \"Terrible policy decision. This will hurt the economy.\",\n",
    "                \"Very disappointing statement. No real solutions proposed.\",\n",
    "                \"Poor leadership on this issue. We need better direction.\"\n",
    "            ]\n",
    "        },\n",
    "        'entertainment': {\n",
    "            'positive': [\n",
    "                \"Fantastic movie! The plot was engaging and well-executed.\",\n",
    "                \"Loved this new album. Every song is a masterpiece.\",\n",
    "                \"Brilliant TV show. Can't wait for the next episode!\"\n",
    "            ],\n",
    "            'negative': [\n",
    "                \"Boring movie. Predictable plot and bad acting.\",\n",
    "                \"Disappointing album. Nothing memorable or original.\",\n",
    "                \"Terrible show. Waste of time watching this.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Random topic\n",
    "        topic = random.choice(topics)\n",
    "        topic_id = topics.index(topic)\n",
    "        \n",
    "        # Random sentiment\n",
    "        sentiment = random.choice(['positive', 'negative'])\n",
    "        sentiment_id = 1 if sentiment == 'positive' else 0\n",
    "        \n",
    "        # Random text\n",
    "        text = random.choice(templates[topic][sentiment])\n",
    "        \n",
    "        # Add some variation\n",
    "        if random.random() > 0.5:\n",
    "            text = text + \" \" + random.choice(templates[topic][sentiment])\n",
    "        \n",
    "        # Determine length category\n",
    "        length = len(text.split())\n",
    "        if length < 10:\n",
    "            length_id = 0  # short\n",
    "        elif length < 20:\n",
    "            length_id = 1  # medium\n",
    "        else:\n",
    "            length_id = 2  # long\n",
    "        \n",
    "        data.append({\n",
    "            'text': text,\n",
    "            'sentiment_label': sentiment_id,\n",
    "            'topic_label': topic_id,\n",
    "            'length_label': length_id,\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# Create datasets\n",
    "print(\"ðŸ“¦ Creating multi-task dataset...\\n\")\n",
    "train_dataset = create_multitask_dataset(1000)\n",
    "test_dataset = create_multitask_dataset(200)\n",
    "\n",
    "print(\"âœ… Dataset created!\")\n",
    "print(f\"\\nðŸ“Š Training samples: {len(train_dataset)}\")\n",
    "print(f\"ðŸ“Š Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nðŸ” Example data point:\")\n",
    "example = train_dataset[0]\n",
    "print(f\"\\nText: {example['text']}\")\n",
    "print(f\"Sentiment: {example['sentiment_label']} (0=negative, 1=positive)\")\n",
    "print(f\"Topic: {example['topic_label']} (0=tech, 1=sports, 2=politics, 3=entertainment)\")\n",
    "print(f\"Length: {example['length_label']} (0=short, 1=medium, 2=long)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Multi-Task Trainer\n",
    "\n",
    "This is where the magic happens - we train on all tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class MultiTaskTrainer:\n",
    "    \"\"\"Custom trainer for multi-task learning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, train_dataset, test_dataset):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        # Task names\n",
    "        self.tasks = ['sentiment', 'topic', 'length']\n",
    "        \n",
    "        # Move model to device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(f\"ðŸ–¥ï¸ Using device: {self.device}\")\n",
    "    \n",
    "    def train(self, num_epochs=3, batch_size=16, learning_rate=3e-5):\n",
    "        \"\"\"Train on all tasks simultaneously\"\"\"\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(f\"\\nðŸš€ Starting multi-task training...\")\n",
    "        print(f\"   Epochs: {num_epochs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\\n\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            task_losses = {task: 0 for task in self.tasks}\n",
    "            \n",
    "            # Create batches\n",
    "            num_batches = len(self.train_dataset) // batch_size\n",
    "            \n",
    "            progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx in progress_bar:\n",
    "                # Get batch\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch = self.train_dataset[start_idx:end_idx]\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer(\n",
    "                    batch['text'],\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Train on each task\n",
    "                batch_loss = 0\n",
    "                \n",
    "                for task in self.tasks:\n",
    "                    # Get predictions for this task\n",
    "                    logits = self.model(\n",
    "                        inputs['input_ids'],\n",
    "                        inputs['attention_mask'],\n",
    "                        task_name=task\n",
    "                    )\n",
    "                    \n",
    "                    # Get labels for this task\n",
    "                    labels = torch.tensor(batch[f'{task}_label']).to(self.device)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(logits, labels)\n",
    "                    batch_loss += loss\n",
    "                    task_losses[task] += loss.item()\n",
    "                \n",
    "                # Average loss across tasks\n",
    "                batch_loss = batch_loss / len(self.tasks)\n",
    "                \n",
    "                # Backprop\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += batch_loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'loss': f'{batch_loss.item():.4f}'})\n",
    "            \n",
    "            # Epoch summary\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"\\n   ðŸ“Š Epoch {epoch+1} completed!\")\n",
    "            print(f\"   ðŸ“‰ Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            for task in self.tasks:\n",
    "                task_avg = task_losses[task] / num_batches\n",
    "                print(f\"   ðŸ“Œ {task} loss: {task_avg:.4f}\")\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            print()\n",
    "            self.evaluate()\n",
    "            print()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate on all tasks\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        results = {task: {'correct': 0, 'total': 0} for task in self.tasks}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for example in self.test_dataset:\n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer(\n",
    "                    example['text'],\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Evaluate each task\n",
    "                for task in self.tasks:\n",
    "                    logits = self.model(\n",
    "                        inputs['input_ids'],\n",
    "                        inputs['attention_mask'],\n",
    "                        task_name=task\n",
    "                    )\n",
    "                    \n",
    "                    pred = logits.argmax(dim=-1).item()\n",
    "                    true_label = example[f'{task}_label']\n",
    "                    \n",
    "                    if pred == true_label:\n",
    "                        results[task]['correct'] += 1\n",
    "                    results[task]['total'] += 1\n",
    "        \n",
    "        # Print results\n",
    "        print(\"   ðŸŽ¯ Evaluation Results:\")\n",
    "        for task in self.tasks:\n",
    "            accuracy = results[task]['correct'] / results[task]['total']\n",
    "            print(f\"   {task.capitalize()}: {accuracy:.3f} ({results[task]['correct']}/{results[task]['total']})\")\n",
    "        \n",
    "        # Overall average\n",
    "        avg_accuracy = np.mean([results[task]['correct']/results[task]['total'] for task in self.tasks])\n",
    "        print(f\"   â­ Average: {avg_accuracy:.3f}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = MultiTaskTrainer(model, tokenizer, train_dataset, test_dataset)\n",
    "\n",
    "print(\"âœ… Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Multi-Task Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train(num_epochs=3, batch_size=16, learning_rate=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Look at That!\n",
    "\n",
    "You just trained ONE model to do THREE different tasks!\n",
    "\n",
    "**Notice:**\n",
    "- The model learned all three tasks simultaneously\n",
    "- Tasks helped each other (shared encoder)\n",
    "- Single training loop, multiple outputs\n",
    "- More efficient than training 3 separate models!\n",
    "\n",
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Our Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_tasks(text):\n",
    "    \"\"\"Get predictions from all tasks for a given text\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    ).to(trainer.device)\n",
    "    \n",
    "    # Get predictions from all tasks\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task in trainer.tasks:\n",
    "            logits = model(\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask'],\n",
    "                task_name=task\n",
    "            )\n",
    "            pred_id = logits.argmax(dim=-1).item()\n",
    "            predictions[task] = pred_id\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This new smartphone is absolutely amazing! Best tech purchase ever!\",\n",
    "    \"Terrible game today. The team played horribly.\",\n",
    "    \"The new policy is excellent. This will help many people.\",\n",
    "    \"Loved this movie! Fantastic plot and great acting.\",\n",
    "]\n",
    "\n",
    "label_names = {\n",
    "    'sentiment': ['Negative', 'Positive'],\n",
    "    'topic': ['Tech', 'Sports', 'Politics', 'Entertainment'],\n",
    "    'length': ['Short', 'Medium', 'Long']\n",
    "}\n",
    "\n",
    "print(\"ðŸ§ª Testing our multi-task model!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\nðŸ“ Example {i}:\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\\n\")\n",
    "    \n",
    "    preds = predict_all_tasks(text)\n",
    "    \n",
    "    print(\"   Predictions:\")\n",
    "    for task, pred_id in preds.items():\n",
    "        label = label_names[task][pred_id]\n",
    "        print(f\"   â€¢ {task.capitalize()}: {label}\")\n",
    "    \n",
    "    print(\"   \" + \"-\"*60)\n",
    "\n",
    "print(\"\\nðŸŽ‰ One model, three predictions! Amazing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Advanced Multi-Task Strategies\n",
    "\n",
    "### 1. **Task Weighting**\n",
    "\n",
    "Not all tasks are equally important. Weight them differently:\n",
    "\n",
    "```python\n",
    "task_weights = {\n",
    "    'sentiment': 2.0,  # Most important\n",
    "    'topic': 1.0,      # Medium\n",
    "    'length': 0.5,     # Less important\n",
    "}\n",
    "\n",
    "# In training loop:\n",
    "weighted_loss = sum(task_weights[task] * loss for task, loss in losses.items())\n",
    "```\n",
    "\n",
    "### 2. **Task Sampling**\n",
    "\n",
    "Instead of training on ALL tasks each batch, sample tasks:\n",
    "\n",
    "```python\n",
    "# Each batch, randomly pick 1-2 tasks to train\n",
    "active_tasks = random.sample(all_tasks, k=2)\n",
    "loss = sum(losses[task] for task in active_tasks)\n",
    "```\n",
    "\n",
    "### 3. **Gradual Task Introduction**\n",
    "\n",
    "Start with one task, gradually add more:\n",
    "\n",
    "```python\n",
    "if epoch < 2:\n",
    "    tasks = ['sentiment']  # Start with main task\n",
    "elif epoch < 4:\n",
    "    tasks = ['sentiment', 'topic']  # Add second task\n",
    "else:\n",
    "    tasks = ['sentiment', 'topic', 'length']  # All tasks\n",
    "```\n",
    "\n",
    "### 4. **Task-Specific Learning Rates**\n",
    "\n",
    "Different tasks might need different learning rates:\n",
    "\n",
    "```python\n",
    "optimizer = AdamW([\n",
    "    {'params': model.encoder.parameters(), 'lr': 2e-5},\n",
    "    {'params': model.task_heads['sentiment'].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.task_heads['topic'].parameters(), 'lr': 3e-5},\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ When to Use Multi-Task Learning\n",
    "\n",
    "**âœ… GOOD Use Cases:**\n",
    "\n",
    "1. **Related Tasks**\n",
    "   - Sentiment + Topic classification âœ…\n",
    "   - NER + POS tagging âœ…\n",
    "   - Q&A + Summarization âœ…\n",
    "\n",
    "2. **Limited Data for Some Tasks**\n",
    "   - Lots of data for Task A\n",
    "   - Little data for Task B\n",
    "   - MTL helps Task B learn from Task A!\n",
    "\n",
    "3. **Production Efficiency**\n",
    "   - Deploy one model instead of many\n",
    "   - Save memory and costs\n",
    "\n",
    "**âŒ BAD Use Cases:**\n",
    "\n",
    "1. **Completely Unrelated Tasks**\n",
    "   - Image classification + Text summarization âŒ\n",
    "   - Tasks that don't share knowledge\n",
    "\n",
    "2. **Conflicting Objectives**\n",
    "   - Tasks that require opposite features\n",
    "   - May hurt each other's performance\n",
    "\n",
    "3. **One Task Has Plenty of Data**\n",
    "   - If Task A has 1M examples and Task B has 100\n",
    "   - Task A might dominate training\n",
    "   - Use task weighting to fix this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Practical Tips\n",
    "\n",
    "### 1. **Monitor Each Task Separately**\n",
    "Don't just look at average loss - track each task's performance!\n",
    "\n",
    "### 2. **Balance Your Batches**\n",
    "Make sure each batch has examples from all tasks (or sample fairly)\n",
    "\n",
    "### 3. **Shared vs Task-Specific**\n",
    "- Encoder = shared (learns general features)\n",
    "- Heads = task-specific (learns task details)\n",
    "- You can also add task-specific middle layers!\n",
    "\n",
    "### 4. **Learning Rate**\n",
    "MTL often works better with slightly LOWER learning rates\n",
    "- Try 2e-5 instead of 3e-5\n",
    "\n",
    "### 5. **Evaluation**\n",
    "Evaluate EACH task on its own test set\n",
    "- Don't just look at combined metrics\n",
    "\n",
    "### 6. **Debugging**\n",
    "If one task is hurting another:\n",
    "- Try task weighting\n",
    "- Try task sampling\n",
    "- Maybe train separately after all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ You're Now a Multi-Task Learning Expert!\n",
    "\n",
    "You just learned:\n",
    "- âœ… How to build multi-task architectures\n",
    "- âœ… How to train on multiple tasks simultaneously\n",
    "- âœ… When to use MTL (and when not to)\n",
    "- âœ… Advanced strategies (weighting, sampling, etc.)\n",
    "- âœ… How tasks can help each other learn\n",
    "\n",
    "**Real-World Impact:**\n",
    "- Google's BERT: Multi-task training\n",
    "- T5: Trained on 20+ tasks!\n",
    "- GPT models: Multi-task learners\n",
    "\n",
    "Multi-task learning is how modern AI gets so powerful!\n",
    "\n",
    "Pretty cool, right? ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**Next up:** Custom Loss Functions - Get really creative! ðŸŽ²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Further Reading\n",
    "\n",
    "**Papers:**\n",
    "- [An Overview of Multi-Task Learning in Deep Neural Networks](https://arxiv.org/abs/1706.05098)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "- [T5: Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "**Resources:**\n",
    "- [HuggingFace Multi-Task Learning Examples](https://huggingface.co/docs/transformers/tasks_explained#multi-task-learning)\n",
    "- [Fast.ai MTL Tutorial](https://www.fast.ai)\n",
    "\n",
    "Now go train some multi-talented models! ðŸ’ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
