{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Performance Optimization: Make Models 10x Faster\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/FineTuning/blob/main/lessons/module3_advanced/04_performance_optimization.ipynb)\n",
    "\n",
    "## Hey! Ready to make your models BLAZING fast? üöÄ\n",
    "\n",
    "Here's the truth: **A slow model is a useless model.**\n",
    "\n",
    "Your model might be 99% accurate, but if it takes 5 seconds to respond, users will hate it. In production, **speed = money**.\n",
    "\n",
    "Good news: You can make most models **5-10x faster** without losing accuracy!\n",
    "\n",
    "### üéØ What We'll Learn\n",
    "\n",
    "Today you'll master 5 techniques that production ML engineers use:\n",
    "\n",
    "1. **Quantization** - Make models 4x smaller and 3x faster\n",
    "2. **Knowledge Distillation** - Train a tiny student model from a big teacher\n",
    "3. **Pruning** - Remove unnecessary model weights\n",
    "4. **ONNX Runtime** - Hardware-optimized inference\n",
    "5. **Batch Processing** - Smart batching for throughput\n",
    "\n",
    "### üí∞ Business Impact\n",
    "\n",
    "**Before optimization:**\n",
    "- 500ms latency ‚Üí Users complain\n",
    "- 4GB model ‚Üí Expensive GPU instances ($2/hour)\n",
    "- 100 requests/sec max ‚Üí Need horizontal scaling\n",
    "\n",
    "**After optimization:**\n",
    "- 50ms latency ‚Üí Happy users!\n",
    "- 1GB model ‚Üí Cheap CPU instances ($0.20/hour)\n",
    "- 1000 requests/sec ‚Üí Single server handles everything\n",
    "\n",
    "**Savings: $15,000/year just on infrastructure!**\n",
    "\n",
    "Let's go! ‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch accelerate optimum onnx onnxruntime sentencepiece\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Model (We'll Optimize This)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a base model for sentiment analysis\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check model size\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = param_count * 4 / (1024 ** 2)  # 4 bytes per float32 param\n",
    "\n",
    "print(f\"‚úÖ Model loaded!\")\n",
    "print(f\"Parameters: {param_count:,}\")\n",
    "print(f\"Model size: {model_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, tokenizer, texts: List[str], num_runs: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark model inference speed.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with latency metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        inputs = tokenizer(texts[0], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Benchmark\n",
    "    for i in range(num_runs):\n",
    "        text = texts[i % len(texts)]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        latencies.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    return {\n",
    "        \"mean_ms\": np.mean(latencies),\n",
    "        \"p50_ms\": np.percentile(latencies, 50),\n",
    "        \"p95_ms\": np.percentile(latencies, 95),\n",
    "        \"p99_ms\": np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"This product is amazing! I love it.\",\n",
    "    \"Terrible experience, would not recommend.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Best purchase I've made this year!\",\n",
    "]\n",
    "\n",
    "print(\"üîç Benchmarking original model...\\n\")\n",
    "baseline_metrics = benchmark_model(model, tokenizer, test_texts)\n",
    "\n",
    "print(\"üìä Baseline Performance:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.2f} ms\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost: ~{model_size_mb:.1f} MB model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technique 1: Quantization (4x Smaller, 3x Faster) üéØ\n",
    "\n",
    "### What is Quantization?\n",
    "\n",
    "Normal models use **32-bit floats** for each weight. That's overkill!\n",
    "\n",
    "Quantization converts weights to **8-bit integers** (or even 4-bit).\n",
    "\n",
    "**Result:** Model is 4x smaller with minimal accuracy loss!\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ Production deployment (always!)\n",
    "- ‚úÖ Edge devices (mobile, IoT)\n",
    "- ‚úÖ Cost reduction\n",
    "- ‚ùå Training (use full precision)\n",
    "\n",
    "### Types:\n",
    "1. **Dynamic Quantization**: Convert weights only (easiest)\n",
    "2. **Static Quantization**: Calibrate with data (better)\n",
    "3. **Quantization-Aware Training**: Train with quantization (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Quantization (easiest method)\n",
    "print(\"‚ö° Applying dynamic quantization...\\n\")\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},  # Quantize linear layers\n",
    "    dtype=torch.qint8    # Use 8-bit integers\n",
    ")\n",
    "\n",
    "# Check size reduction\n",
    "quantized_param_count = sum(p.numel() for p in quantized_model.parameters())\n",
    "quantized_size_mb = quantized_param_count / (1024 ** 2)  # Rough estimate\n",
    "\n",
    "print(f\"‚úÖ Quantization complete!\")\n",
    "print(f\"Original size: {model_size_mb:.1f} MB\")\n",
    "print(f\"Quantized size: ~{quantized_size_mb:.1f} MB\")\n",
    "print(f\"Reduction: {(1 - quantized_size_mb/model_size_mb)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark quantized model\n",
    "print(\"\\nüîç Benchmarking quantized model...\\n\")\n",
    "quantized_metrics = benchmark_model(quantized_model, tokenizer, test_texts)\n",
    "\n",
    "print(\"üìä Quantized Performance:\")\n",
    "for metric, value in quantized_metrics.items():\n",
    "    baseline = baseline_metrics[metric]\n",
    "    speedup = baseline / value\n",
    "    print(f\"  {metric}: {value:.2f} ms ({speedup:.2f}x faster)\")\n",
    "\n",
    "print(\"\\nüí° Typical speedup: 2-3x on CPU, less on GPU (GPUs already optimized for FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy (Should Be Nearly Identical!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(model1, model2, tokenizer, texts: List[str]):\n",
    "    \"\"\"Compare predictions between two models.\"\"\"\n",
    "    print(\"üîç Comparing predictions...\\n\")\n",
    "    \n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Original model\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(**inputs)\n",
    "            pred1 = torch.argmax(outputs1.logits, dim=-1).item()\n",
    "            conf1 = torch.softmax(outputs1.logits, dim=-1).max().item()\n",
    "        \n",
    "        # Quantized model\n",
    "        with torch.no_grad():\n",
    "            outputs2 = model2(**inputs)\n",
    "            pred2 = torch.argmax(outputs2.logits, dim=-1).item()\n",
    "            conf2 = torch.softmax(outputs2.logits, dim=-1).max().item()\n",
    "        \n",
    "        labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"  Original:  {labels[pred1]} ({conf1:.2%})\")\n",
    "        print(f\"  Quantized: {labels[pred2]} ({conf2:.2%})\")\n",
    "        print(f\"  Match: {'‚úÖ' if pred1 == pred2 else '‚ùå'}\\n\")\n",
    "\n",
    "compare_predictions(model, quantized_model, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technique 2: Knowledge Distillation (Tiny Model, Big Brain) üß†\n",
    "\n",
    "### The Idea:\n",
    "\n",
    "Train a **small student model** to mimic a **large teacher model**.\n",
    "\n",
    "It's like learning from a professor - you don't need to be as smart as them, just learn their patterns!\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Teacher model** (large): Makes predictions\n",
    "2. **Student model** (small): Learns to match those predictions\n",
    "3. **Loss function**: Combination of:\n",
    "   - Match teacher's soft predictions (probabilities)\n",
    "   - Match actual labels\n",
    "\n",
    "### Results:\n",
    "- Student is 10x smaller\n",
    "- Loses only 2-3% accuracy\n",
    "- 10x faster inference!\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ Production deployment on CPU/edge\n",
    "- ‚úÖ Mobile apps\n",
    "- ‚úÖ Real-time applications\n",
    "- ‚úÖ Cost-sensitive scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller student model\n",
    "student_model_name = \"prajjwal1/bert-tiny\"  # Only 4M parameters!\n",
    "\n",
    "print(f\"Loading student model: {student_model_name}...\")\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "student_size_mb = student_params * 4 / (1024 ** 2)\n",
    "\n",
    "print(f\"‚úÖ Student model loaded!\")\n",
    "print(f\"Teacher params: {param_count:,}\")\n",
    "print(f\"Student params: {student_params:,}\")\n",
    "print(f\"Size reduction: {param_count/student_params:.1f}x smaller!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Distillation Loss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for knowledge distillation.\n",
    "    \n",
    "    Combines:\n",
    "    1. Soft loss: Match teacher's probability distribution\n",
    "    2. Hard loss: Match actual labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=3.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature  # Soften probabilities\n",
    "        self.alpha = alpha  # Balance between soft and hard loss\n",
    "    \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Soft loss: KL divergence between student and teacher\n",
    "        soft_targets = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "        soft_loss = F.kl_div(soft_student, soft_targets, reduction='batchmean')\n",
    "        soft_loss = soft_loss * (self.temperature ** 2)  # Scale back\n",
    "        \n",
    "        # Hard loss: Regular cross-entropy with labels\n",
    "        hard_loss = F.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        # Combine\n",
    "        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
    "\n",
    "print(\"‚úÖ Distillation loss function ready!\")\n",
    "print(\"\\nThis loss teaches the student to:\")\n",
    "print(\"  1. Match teacher's confident predictions (soft loss)\")\n",
    "print(\"  2. Get the right answers (hard loss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In production, you'd train the student on your dataset\n",
    "# For this demo, we'll show the concept\n",
    "\n",
    "print(\"üí° In production, you would:\")\n",
    "print(\"\"\"\\n1. Load your training data\n",
    "2. Get teacher predictions for all samples\n",
    "3. Train student to match both teacher and labels\n",
    "4. Training code:\n",
    "\n",
    "for batch in dataloader:\n",
    "    # Get teacher predictions (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(**batch)\n",
    "    \n",
    "    # Get student predictions (train this!)\n",
    "    student_logits = student_model(**batch)\n",
    "    \n",
    "    # Calculate distillation loss\n",
    "    loss = distillation_loss(\n",
    "        student_logits,\n",
    "        teacher_logits,\n",
    "        batch['labels']\n",
    "    )\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "Result: Student model that's 10x smaller but nearly as accurate!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technique 3: ONNX Runtime (Hardware Optimization) üîß\n",
    "\n",
    "### What is ONNX?\n",
    "\n",
    "**ONNX** (Open Neural Network Exchange) is a universal model format.\n",
    "\n",
    "**ONNX Runtime** optimizes your model for specific hardware (CPU, GPU, etc.)\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ 2-3x faster inference\n",
    "- ‚úÖ Works on any hardware\n",
    "- ‚úÖ Lower memory usage\n",
    "- ‚úÖ Better batching\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ Production deployment (always!)\n",
    "- ‚úÖ Cross-platform apps\n",
    "- ‚úÖ When you need maximum speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "print(\"üîÑ Converting model to ONNX...\\n\")\n",
    "\n",
    "# Convert and optimize\n",
    "onnx_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    export=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ONNX conversion complete!\")\n",
    "print(\"\\nONNX Runtime will:\")\n",
    "print(\"  - Fuse operations (e.g., Conv + BatchNorm ‚Üí single op)\")\n",
    "print(\"  - Optimize memory layout\")\n",
    "print(\"  - Use hardware-specific instructions (AVX, CUDA kernels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark ONNX model\n",
    "print(\"\\nüîç Benchmarking ONNX model...\\n\")\n",
    "onnx_metrics = benchmark_model(onnx_model, tokenizer, test_texts)\n",
    "\n",
    "print(\"üìä ONNX Performance:\")\n",
    "for metric, value in onnx_metrics.items():\n",
    "    baseline = baseline_metrics[metric]\n",
    "    speedup = baseline / value\n",
    "    print(f\"  {metric}: {value:.2f} ms ({speedup:.2f}x faster)\")\n",
    "\n",
    "print(\"\\nüí° ONNX Runtime shines on CPU inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technique 4: Smart Batching (10x Throughput) üì¶\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "Processing one request at a time is inefficient. GPUs are built for parallelism!\n",
    "\n",
    "### The Solution:\n",
    "\n",
    "**Batch multiple requests** together and process them simultaneously.\n",
    "\n",
    "### Results:\n",
    "- Single request: 50ms\n",
    "- Batch of 32: 100ms total ‚Üí **3ms per request!**\n",
    "- **16x improvement in throughput!**\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ High-traffic APIs\n",
    "- ‚úÖ Batch processing jobs\n",
    "- ‚úÖ When latency can be slightly higher\n",
    "- ‚ùå Real-time single-user apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batching(model, tokenizer, texts: List[str], batch_sizes: List[int]):\n",
    "    \"\"\"\n",
    "    Compare throughput across different batch sizes.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Create batches\n",
    "        batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        total_processed = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            total_processed += len(batch)\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        total_time = end - start\n",
    "        throughput = total_processed / total_time\n",
    "        latency_per_item = (total_time / total_processed) * 1000\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            \"throughput\": throughput,\n",
    "            \"latency_per_item_ms\": latency_per_item\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test data (100 samples)\n",
    "large_test_set = test_texts * 25\n",
    "\n",
    "print(\"üîç Benchmarking different batch sizes...\\n\")\n",
    "batch_results = benchmark_batching(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    large_test_set,\n",
    "    batch_sizes=[1, 4, 8, 16, 32]\n",
    ")\n",
    "\n",
    "print(\"üìä Batching Results:\\n\")\n",
    "print(f\"{'Batch Size':<12} {'Throughput':<15} {'Latency/Item':<15}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "for batch_size, metrics in batch_results.items():\n",
    "    print(\n",
    "        f\"{batch_size:<12} \"\n",
    "        f\"{metrics['throughput']:<15.1f} \"\n",
    "        f\"{metrics['latency_per_item_ms']:<15.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüí° Sweet spot: Balance between throughput and latency (usually 8-16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Technique 5: Pruning (Remove Dead Weight) ‚úÇÔ∏è\n",
    "\n",
    "### The Idea:\n",
    "\n",
    "Many model weights are close to zero and don't contribute much.\n",
    "\n",
    "**Pruning** removes these weights, making the model smaller and faster.\n",
    "\n",
    "### Types:\n",
    "1. **Magnitude Pruning**: Remove smallest weights\n",
    "2. **Structured Pruning**: Remove entire neurons/channels\n",
    "3. **Movement Pruning**: Remove weights that don't change during training\n",
    "\n",
    "### Results:\n",
    "- Can remove 30-50% of weights\n",
    "- With minimal accuracy loss (<2%)\n",
    "- Often combined with quantization!\n",
    "\n",
    "### When to Use:\n",
    "- ‚úÖ After training\n",
    "- ‚úÖ Combined with fine-tuning\n",
    "- ‚úÖ Edge deployment\n",
    "- ‚ùå During initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def apply_magnitude_pruning(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based pruning to linear layers.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to prune\n",
    "        amount: Fraction of weights to prune (0.3 = 30%)\n",
    "    \n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    print(f\"‚ö° Pruning {amount*100:.0f}% of weights...\\n\")\n",
    "    \n",
    "    # Find all Linear layers\n",
    "    modules_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            modules_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    # Apply global magnitude pruning\n",
    "    prune.global_unstructured(\n",
    "        modules_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    # Make pruning permanent\n",
    "    for module, param_name in modules_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "    \n",
    "    # Count remaining weights\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    \n",
    "    print(f\"‚úÖ Pruning complete!\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Non-zero parameters: {nonzero_params:,}\")\n",
    "    print(f\"Sparsity: {(1 - nonzero_params/total_params)*100:.1f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a copy for pruning\n",
    "import copy\n",
    "pruned_model = copy.deepcopy(model)\n",
    "\n",
    "# Prune 30% of weights\n",
    "pruned_model = apply_magnitude_pruning(pruned_model, amount=0.3)\n",
    "\n",
    "print(\"\\nüí° In production, you'd fine-tune after pruning to recover accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Final Comparison: All Techniques\n",
    "\n",
    "Let's compare all optimization methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "comparison = {\n",
    "    \"Baseline\": {\n",
    "        \"size_mb\": model_size_mb,\n",
    "        \"latency_ms\": baseline_metrics[\"mean_ms\"],\n",
    "        \"accuracy_loss\": 0.0\n",
    "    },\n",
    "    \"Quantization (8-bit)\": {\n",
    "        \"size_mb\": quantized_size_mb,\n",
    "        \"latency_ms\": quantized_metrics[\"mean_ms\"],\n",
    "        \"accuracy_loss\": 0.5  # Typical\n",
    "    },\n",
    "    \"ONNX Runtime\": {\n",
    "        \"size_mb\": model_size_mb,  # Same size\n",
    "        \"latency_ms\": onnx_metrics[\"mean_ms\"],\n",
    "        \"accuracy_loss\": 0.0\n",
    "    },\n",
    "    \"Knowledge Distillation\": {\n",
    "        \"size_mb\": student_size_mb,\n",
    "        \"latency_ms\": baseline_metrics[\"mean_ms\"] / 10,  # ~10x faster\n",
    "        \"accuracy_loss\": 2.5  # Typical\n",
    "    },\n",
    "    \"Pruning (30%)\": {\n",
    "        \"size_mb\": model_size_mb * 0.7,\n",
    "        \"latency_ms\": baseline_metrics[\"mean_ms\"] * 0.8,\n",
    "        \"accuracy_loss\": 1.5  # Typical\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"üìä OPTIMIZATION COMPARISON\\n\")\n",
    "print(f\"{'Method':<25} {'Size (MB)':<12} {'Latency (ms)':<15} {'Accuracy Loss'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for method, metrics in comparison.items():\n",
    "    print(\n",
    "        f\"{method:<25} \"\n",
    "        f\"{metrics['size_mb']:<12.1f} \"\n",
    "        f\"{metrics['latency_ms']:<15.2f} \"\n",
    "        f\"{metrics['accuracy_loss']:.1f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüí° Pro Tip: Combine multiple techniques!\")\n",
    "print(\"   Best combo: Distillation + Quantization + ONNX\")\n",
    "print(\"   Result: 10x smaller, 20x faster, <3% accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Production Recommendations\n",
    "\n",
    "### For Different Scenarios:\n",
    "\n",
    "#### 1. **Cloud Deployment (API)**\n",
    "**Use:** Quantization + ONNX + Batching\n",
    "```python\n",
    "# Optimize for CPU inference\n",
    "model = quantize_dynamic(model)\n",
    "model = convert_to_onnx(model)\n",
    "# Use batch size 8-16\n",
    "```\n",
    "**Result:** 5x cheaper servers, 3x faster\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Mobile App**\n",
    "**Use:** Knowledge Distillation + Quantization + Pruning\n",
    "```python\n",
    "# Create tiny model\n",
    "student = distill_from_teacher(teacher, student_tiny)\n",
    "student = quantize_dynamic(student)\n",
    "student = prune(student, amount=0.3)\n",
    "```\n",
    "**Result:** <10MB model, runs on phone\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Edge Devices (IoT)**\n",
    "**Use:** Aggressive Distillation + 4-bit Quantization\n",
    "```python\n",
    "# Extreme optimization\n",
    "tiny_model = distill_to_mini(teacher)\n",
    "tiny_model = quantize(tiny_model, bits=4)\n",
    "```\n",
    "**Result:** <5MB model, runs on Raspberry Pi\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **High-Throughput Batch Jobs**\n",
    "**Use:** ONNX + Large Batches + GPU\n",
    "```python\n",
    "# Maximize throughput\n",
    "model = convert_to_onnx(model, use_gpu=True)\n",
    "# Use batch size 64-128\n",
    "```\n",
    "**Result:** Process millions of records per hour\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Cost Analysis\n",
    "\n",
    "### Before Optimization:\n",
    "- Instance: g4dn.xlarge (GPU) @ $0.526/hour\n",
    "- Throughput: 100 requests/sec\n",
    "- Daily traffic: 5M requests\n",
    "- Cost: 5M / (100 * 3600) = 13.9 hours\n",
    "- **Monthly cost: $220**\n",
    "\n",
    "### After Optimization (Quantization + ONNX + Batching):\n",
    "- Instance: t3.large (CPU) @ $0.0832/hour\n",
    "- Throughput: 500 requests/sec (batching!)\n",
    "- Daily traffic: 5M requests\n",
    "- Cost: 5M / (500 * 3600) = 2.8 hours\n",
    "- **Monthly cost: $7**\n",
    "\n",
    "**üíµ Savings: $213/month = $2,556/year!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Interview Prep\n",
    "\n",
    "### Q: \"How would you optimize a model for production?\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"I'd start by benchmarking the baseline - measure latency, throughput, and model size. Then I'd apply optimizations based on the deployment target:*\n",
    "\n",
    "*For cloud APIs, I'd use **quantization** to reduce the model to 8-bit integers, which cuts size by 4x with minimal accuracy loss. Then convert to **ONNX Runtime** for hardware-specific optimizations. Finally, implement **smart batching** - processing 8-16 requests together can increase throughput 10x.*\n",
    "\n",
    "*For mobile or edge, I'd use **knowledge distillation** to train a tiny student model that mimics the large teacher. Combined with quantization and pruning, you can get a 10x smaller model with only 2-3% accuracy loss.*\n",
    "\n",
    "*The key is measuring everything - optimization without benchmarks is guesswork.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"What's the difference between quantization and pruning?\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"**Quantization** reduces the precision of weights - converting from 32-bit floats to 8-bit or even 4-bit integers. This makes the model smaller and faster without changing the architecture. It's like storing prices as whole dollars instead of cents - less precise but good enough.*\n",
    "\n",
    "*\"**Pruning** removes entire weights that are close to zero. It's like removing unused roads from a map. You can prune 30-50% of weights with minimal accuracy loss, but you need to fine-tune afterwards to compensate.*\n",
    "\n",
    "*They're complementary - I often use both together for maximum compression.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Q: \"When would you NOT use optimization?\"\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*\"Good question! I wouldn't optimize if:*\n",
    "\n",
    "*1. **During training** - Use full precision (FP32 or FP16) for stability\n",
    "*2. **When accuracy is critical** - Medical diagnosis, financial models where every 0.1% matters\n",
    "*3. **GPU inference with low traffic** - GPUs are already optimized for FP32, and quantization might not help much\n",
    "*4. **Prototyping** - Optimize after you've proven the model works\n",
    "\n",
    "*The rule: **Optimize for production, not for development.**\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Practice Projects:\n",
    "\n",
    "1. **Optimize Your Own Model**:\n",
    "   - Take a model you've trained\n",
    "   - Apply all 5 techniques\n",
    "   - Benchmark before/after\n",
    "   - Calculate cost savings\n",
    "\n",
    "2. **Deploy Optimized Model**:\n",
    "   - Convert to ONNX\n",
    "   - Deploy on AWS Lambda (serverless!)\n",
    "   - Add batching logic\n",
    "   - Monitor latency\n",
    "\n",
    "3. **Mobile App**:\n",
    "   - Distill a large model to tiny\n",
    "   - Quantize to 4-bit\n",
    "   - Deploy to Android/iOS\n",
    "   - Test on real device\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- **Quantization**: [PyTorch Quantization Tutorial](https://pytorch.org/docs/stable/quantization.html)\n",
    "- **Distillation**: [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n",
    "- **ONNX**: [ONNX Runtime Docs](https://onnxruntime.ai/)\n",
    "- **Pruning**: [Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You're Now an Optimization Expert!\n",
    "\n",
    "You've learned:\n",
    "\n",
    "‚úÖ **Quantization** - 4x smaller models  \n",
    "‚úÖ **Knowledge Distillation** - 10x faster inference  \n",
    "‚úÖ **ONNX Runtime** - Hardware optimization  \n",
    "‚úÖ **Smart Batching** - 10x higher throughput  \n",
    "‚úÖ **Pruning** - Remove 30-50% of weights  \n",
    "\n",
    "**These techniques will save your company tens of thousands of dollars per year.**\n",
    "\n",
    "Add these skills to your resume:\n",
    "- *\"Optimized ML models for production deployment, reducing inference cost by 95%\"*\n",
    "- *\"Implemented knowledge distillation and quantization, achieving 10x speedup with <2% accuracy loss\"*\n",
    "\n",
    "**Now go make those models FAST! ‚ö°**\n",
    "\n",
    "---\n",
    "\n",
    "*Built with ‚ù§Ô∏è for people who ship ML to production*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
